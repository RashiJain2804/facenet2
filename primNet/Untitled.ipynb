{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nandwani_vaibhav/anaconda3/envs/fastai/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nandwani_vaibhav/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from eval_metrics import evaluate\n",
    "from logger import Logger\n",
    "# from TripletFaceDataset import TripletFaceDataset\n",
    "# from LFWDataset import LFWDataset\n",
    "from PIL import Image\n",
    "#from utils import PairwiseDistance,display_triplet_distance,display_triplet_distance_test\n",
    "import collections\n",
    "from torchvision.datasets import ImageFolder\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/home/nandwani_vaibhav/wildlife/data/train/'\n",
    "testpath = '/home/nandwani_vaibhav/wildlife/data/test/'\n",
    "log_dir = '/home/nandwani_vaibhav/wildlife/primNet/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#resume\n",
    "start_epoch = 0\n",
    "epochs = 5\n",
    "# Training options\n",
    "embedding_size = 256\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "lr = 0.1\n",
    "lr_decay = 1e-4\n",
    "wd = 0.0\n",
    "optimizer ='adagrad'\n",
    "# Device options\n",
    "no_cuda = False\n",
    "gpu_id ='0'\n",
    "seed = 0\n",
    "log_interval = 10\n",
    "\n",
    "\n",
    "# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n",
    "# order to prevent any memory allocation on unused GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(seed)\n",
    "\n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.makedirs(log_dir)\n",
    "\n",
    "if cuda:\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "LOG_DIR = log_dir \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger\n",
    "logger = Logger(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale(object):\n",
    "    \"\"\"Rescales the input PIL.Image to the given 'size'.\n",
    "    If 'size' is a 2-element tuple or list in the order of (width, height), it will be the exactly size to scale.\n",
    "    If 'size' is a number, it will indicate the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the exactly size or the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation_iterator(dataLoader):\n",
    "    for data, target in dataLoader:\n",
    "        yield data, target        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 2, 'pin_memory': True} if cuda else {}\n",
    "transform = transforms.Compose([\n",
    "                         Scale((112,112)),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ],\n",
    "                                               std = [ 0.5, 0.5, 0.5 ])\n",
    "                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageFolder(testpath, transform=transform)\n",
    "train_dataset = ImageFolder(dataroot, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, **kwargs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **kwargs)\n",
    "val_iterator = validation_iterator(test_loader)\n",
    "train_iterator = validation_iterator(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "5873\n",
      "5873\n",
      "1289\n",
      "1289\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset.classes))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL.py ##\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "\n",
    "    channels_per_group = num_channels // groups\n",
    "\n",
    "    # reshape\n",
    "    x = x.view(batchsize, groups, channels_per_group, height, width)\n",
    "\n",
    "    # transpose\n",
    "    # - contiguous() required if transpose() is used before view().\n",
    "    #   See https://github.com/pytorch/pytorch/issues/764\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "\n",
    "    # flatten\n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "    return x\n",
    "    \n",
    "\n",
    "class PrimNet(nn.Module):\n",
    "    def __init__(self,embedding_size,num_classes,pretrained=False):\n",
    "        super(PrimNet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=2, padding=1, groups = 32),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, groups = 32),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, groups = 32),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Linear(7*7*1024, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, num_classes)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = channel_shuffle(out, 32)\n",
    "        out = self.layer3(out)\n",
    "        out = channel_shuffle(out, 32)\n",
    "        out = self.layer4(out)\n",
    "        out = channel_shuffle(out, 32)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        self.features = self.l2_norm(out)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        alpha=10\n",
    "        self.features = self.features*alpha\n",
    "        return self.features\n",
    "    \n",
    "    def forward_classifier(self, x):\n",
    "        features = self.forward(x)\n",
    "        res = self.fc2(features)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PrimNet(embedding_size=embedding_size,\n",
    "                      num_classes=len(train_dataset.classes),\n",
    "                      pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.resume:\n",
    "#         if os.path.isfile(args.resume):\n",
    "#             print('=> loading checkpoint {}'.format(args.resume))\n",
    "#             checkpoint = torch.load(args.resume)\n",
    "#             args.start_epoch = checkpoint['epoch']\n",
    "#             checkpoint = torch.load(args.resume)\n",
    "#             model.load_state_dict(checkpoint['state_dict'])\n",
    "#         else:\n",
    "#             print('=> no checkpoint found at {}'.format(args.resume))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/5873], Loss: 3.3189\n",
      "Epoch [1/5], Step [2/5873], Loss: 4.7692\n",
      "Epoch [1/5], Step [3/5873], Loss: 4.6633\n",
      "Epoch [1/5], Step [4/5873], Loss: 4.3682\n",
      "Epoch [1/5], Step [5/5873], Loss: 4.8028\n",
      "Epoch [1/5], Step [6/5873], Loss: 4.7255\n",
      "Epoch [1/5], Step [7/5873], Loss: 4.7638\n",
      "Epoch [1/5], Step [8/5873], Loss: 3.9401\n",
      "Epoch [1/5], Step [9/5873], Loss: 4.2348\n",
      "Epoch [1/5], Step [10/5873], Loss: 4.9853\n",
      "Epoch [1/5], Step [11/5873], Loss: 4.8254\n",
      "Epoch [1/5], Step [12/5873], Loss: 4.6027\n",
      "Epoch [1/5], Step [13/5873], Loss: 5.3315\n",
      "Epoch [1/5], Step [14/5873], Loss: 4.2478\n",
      "Epoch [1/5], Step [15/5873], Loss: 4.9121\n",
      "Epoch [1/5], Step [16/5873], Loss: 4.9252\n",
      "Epoch [1/5], Step [17/5873], Loss: 4.5858\n",
      "Epoch [1/5], Step [18/5873], Loss: 4.4825\n",
      "Epoch [1/5], Step [19/5873], Loss: 4.9400\n",
      "Epoch [1/5], Step [20/5873], Loss: 5.3266\n",
      "Epoch [1/5], Step [21/5873], Loss: 4.4422\n",
      "Epoch [1/5], Step [22/5873], Loss: 4.7196\n",
      "Epoch [1/5], Step [23/5873], Loss: 3.8188\n",
      "Epoch [1/5], Step [24/5873], Loss: 5.1380\n",
      "Epoch [1/5], Step [25/5873], Loss: 3.2092\n",
      "Epoch [1/5], Step [26/5873], Loss: 5.3118\n",
      "Epoch [1/5], Step [27/5873], Loss: 3.9340\n",
      "Epoch [1/5], Step [28/5873], Loss: 4.4416\n",
      "Epoch [1/5], Step [29/5873], Loss: 3.0348\n",
      "Epoch [1/5], Step [30/5873], Loss: 3.4902\n",
      "Epoch [1/5], Step [31/5873], Loss: 5.4196\n",
      "Epoch [1/5], Step [32/5873], Loss: 4.5078\n",
      "Epoch [1/5], Step [33/5873], Loss: 3.3946\n",
      "Epoch [1/5], Step [34/5873], Loss: 4.5497\n",
      "Epoch [1/5], Step [35/5873], Loss: 5.2477\n",
      "Epoch [1/5], Step [36/5873], Loss: 5.8539\n",
      "Epoch [1/5], Step [37/5873], Loss: 6.0987\n",
      "Epoch [1/5], Step [38/5873], Loss: 3.8377\n",
      "Epoch [1/5], Step [39/5873], Loss: 4.1856\n",
      "Epoch [1/5], Step [40/5873], Loss: 4.6437\n",
      "Epoch [1/5], Step [41/5873], Loss: 4.0478\n",
      "Epoch [1/5], Step [42/5873], Loss: 5.3587\n",
      "Epoch [1/5], Step [43/5873], Loss: 3.8527\n",
      "Epoch [1/5], Step [44/5873], Loss: 5.2264\n",
      "Epoch [1/5], Step [45/5873], Loss: 5.2191\n",
      "Epoch [1/5], Step [46/5873], Loss: 5.6747\n",
      "Epoch [1/5], Step [47/5873], Loss: 4.3101\n",
      "Epoch [1/5], Step [48/5873], Loss: 4.2434\n",
      "Epoch [1/5], Step [49/5873], Loss: 4.3711\n",
      "Epoch [1/5], Step [50/5873], Loss: 5.4034\n",
      "Epoch [1/5], Step [51/5873], Loss: 3.8496\n",
      "Epoch [1/5], Step [52/5873], Loss: 4.7176\n",
      "Epoch [1/5], Step [53/5873], Loss: 3.8715\n",
      "Epoch [1/5], Step [54/5873], Loss: 5.8327\n",
      "Epoch [1/5], Step [55/5873], Loss: 4.6609\n",
      "Epoch [1/5], Step [56/5873], Loss: 4.8098\n",
      "Epoch [1/5], Step [57/5873], Loss: 4.3093\n",
      "Epoch [1/5], Step [58/5873], Loss: 4.0091\n",
      "Epoch [1/5], Step [59/5873], Loss: 3.8848\n",
      "Epoch [1/5], Step [60/5873], Loss: 6.4036\n",
      "Epoch [1/5], Step [61/5873], Loss: 2.9249\n",
      "Epoch [1/5], Step [62/5873], Loss: 4.9435\n",
      "Epoch [1/5], Step [63/5873], Loss: 3.8117\n",
      "Epoch [1/5], Step [64/5873], Loss: 5.7718\n",
      "Epoch [1/5], Step [65/5873], Loss: 5.1659\n",
      "Epoch [1/5], Step [66/5873], Loss: 3.5794\n",
      "Epoch [1/5], Step [67/5873], Loss: 3.4948\n",
      "Epoch [1/5], Step [68/5873], Loss: 5.8512\n",
      "Epoch [1/5], Step [69/5873], Loss: 4.5268\n",
      "Epoch [1/5], Step [70/5873], Loss: 3.9143\n",
      "Epoch [1/5], Step [71/5873], Loss: 5.5706\n",
      "Epoch [1/5], Step [72/5873], Loss: 3.8202\n",
      "Epoch [1/5], Step [73/5873], Loss: 5.6814\n",
      "Epoch [1/5], Step [74/5873], Loss: 5.9168\n",
      "Epoch [1/5], Step [75/5873], Loss: 5.4628\n",
      "Epoch [1/5], Step [76/5873], Loss: 4.1575\n",
      "Epoch [1/5], Step [77/5873], Loss: 4.9242\n",
      "Epoch [1/5], Step [78/5873], Loss: 4.9322\n",
      "Epoch [1/5], Step [79/5873], Loss: 4.1834\n",
      "Epoch [1/5], Step [80/5873], Loss: 4.2716\n",
      "Epoch [1/5], Step [81/5873], Loss: 3.8931\n",
      "Epoch [1/5], Step [82/5873], Loss: 4.4528\n",
      "Epoch [1/5], Step [83/5873], Loss: 5.2440\n",
      "Epoch [1/5], Step [84/5873], Loss: 4.3233\n",
      "Epoch [1/5], Step [85/5873], Loss: 5.0496\n",
      "Epoch [1/5], Step [86/5873], Loss: 4.1817\n",
      "Epoch [1/5], Step [87/5873], Loss: 3.0173\n",
      "Epoch [1/5], Step [88/5873], Loss: 2.8359\n",
      "Epoch [1/5], Step [89/5873], Loss: 5.6217\n",
      "Epoch [1/5], Step [90/5873], Loss: 4.9438\n",
      "Epoch [1/5], Step [91/5873], Loss: 5.0903\n",
      "Epoch [1/5], Step [92/5873], Loss: 5.8282\n",
      "Epoch [1/5], Step [93/5873], Loss: 5.4879\n",
      "Epoch [1/5], Step [94/5873], Loss: 3.8374\n",
      "Epoch [1/5], Step [95/5873], Loss: 5.6901\n",
      "Epoch [1/5], Step [96/5873], Loss: 5.8284\n",
      "Epoch [1/5], Step [97/5873], Loss: 4.4862\n",
      "Epoch [1/5], Step [98/5873], Loss: 3.9555\n",
      "Epoch [1/5], Step [99/5873], Loss: 3.5474\n",
      "Epoch [1/5], Step [100/5873], Loss: 4.1568\n",
      "Epoch [1/5], Step [101/5873], Loss: 4.4642\n",
      "Epoch [1/5], Step [102/5873], Loss: 5.0755\n",
      "Epoch [1/5], Step [103/5873], Loss: 5.4193\n",
      "Epoch [1/5], Step [104/5873], Loss: 4.3886\n",
      "Epoch [1/5], Step [105/5873], Loss: 6.1467\n",
      "Epoch [1/5], Step [106/5873], Loss: 4.4874\n",
      "Epoch [1/5], Step [107/5873], Loss: 4.5587\n",
      "Epoch [1/5], Step [108/5873], Loss: 4.4186\n",
      "Epoch [1/5], Step [109/5873], Loss: 4.7886\n",
      "Epoch [1/5], Step [110/5873], Loss: 5.2309\n",
      "Epoch [1/5], Step [111/5873], Loss: 4.4305\n",
      "Epoch [1/5], Step [112/5873], Loss: 4.4052\n",
      "Epoch [1/5], Step [113/5873], Loss: 4.2790\n",
      "Epoch [1/5], Step [114/5873], Loss: 3.8643\n",
      "Epoch [1/5], Step [115/5873], Loss: 4.9563\n",
      "Epoch [1/5], Step [116/5873], Loss: 4.4815\n",
      "Epoch [1/5], Step [117/5873], Loss: 4.1724\n",
      "Epoch [1/5], Step [118/5873], Loss: 4.8065\n",
      "Epoch [1/5], Step [119/5873], Loss: 2.6566\n",
      "Epoch [1/5], Step [120/5873], Loss: 6.0013\n",
      "Epoch [1/5], Step [121/5873], Loss: 4.4830\n",
      "Epoch [1/5], Step [122/5873], Loss: 4.2354\n",
      "Epoch [1/5], Step [123/5873], Loss: 5.7191\n",
      "Epoch [1/5], Step [124/5873], Loss: 3.9067\n",
      "Epoch [1/5], Step [125/5873], Loss: 5.7230\n",
      "Epoch [1/5], Step [126/5873], Loss: 3.7451\n",
      "Epoch [1/5], Step [127/5873], Loss: 3.0019\n",
      "Epoch [1/5], Step [128/5873], Loss: 3.9379\n",
      "Epoch [1/5], Step [129/5873], Loss: 6.1636\n",
      "Epoch [1/5], Step [130/5873], Loss: 3.7992\n",
      "Epoch [1/5], Step [131/5873], Loss: 3.4654\n",
      "Epoch [1/5], Step [132/5873], Loss: 3.6809\n",
      "Epoch [1/5], Step [133/5873], Loss: 5.7796\n",
      "Epoch [1/5], Step [134/5873], Loss: 6.0129\n",
      "Epoch [1/5], Step [135/5873], Loss: 3.6308\n",
      "Epoch [1/5], Step [136/5873], Loss: 5.3363\n",
      "Epoch [1/5], Step [137/5873], Loss: 3.0061\n",
      "Epoch [1/5], Step [138/5873], Loss: 4.5728\n",
      "Epoch [1/5], Step [139/5873], Loss: 5.6544\n",
      "Epoch [1/5], Step [140/5873], Loss: 5.1392\n",
      "Epoch [1/5], Step [141/5873], Loss: 4.5134\n",
      "Epoch [1/5], Step [142/5873], Loss: 4.0990\n",
      "Epoch [1/5], Step [143/5873], Loss: 4.1984\n",
      "Epoch [1/5], Step [144/5873], Loss: 3.7925\n",
      "Epoch [1/5], Step [145/5873], Loss: 4.1602\n",
      "Epoch [1/5], Step [146/5873], Loss: 5.4951\n",
      "Epoch [1/5], Step [147/5873], Loss: 3.1194\n",
      "Epoch [1/5], Step [148/5873], Loss: 5.5418\n",
      "Epoch [1/5], Step [149/5873], Loss: 5.1586\n",
      "Epoch [1/5], Step [150/5873], Loss: 4.5363\n",
      "Epoch [1/5], Step [151/5873], Loss: 3.6595\n",
      "Epoch [1/5], Step [152/5873], Loss: 6.0534\n",
      "Epoch [1/5], Step [153/5873], Loss: 2.8803\n",
      "Epoch [1/5], Step [154/5873], Loss: 3.9120\n",
      "Epoch [1/5], Step [155/5873], Loss: 4.9676\n",
      "Epoch [1/5], Step [156/5873], Loss: 5.0038\n",
      "Epoch [1/5], Step [157/5873], Loss: 4.5558\n",
      "Epoch [1/5], Step [158/5873], Loss: 5.6100\n",
      "Epoch [1/5], Step [159/5873], Loss: 5.7741\n",
      "Epoch [1/5], Step [160/5873], Loss: 4.4434\n",
      "Epoch [1/5], Step [161/5873], Loss: 4.2092\n",
      "Epoch [1/5], Step [162/5873], Loss: 6.0424\n",
      "Epoch [1/5], Step [163/5873], Loss: 5.0352\n",
      "Epoch [1/5], Step [164/5873], Loss: 4.0740\n",
      "Epoch [1/5], Step [165/5873], Loss: 3.9705\n",
      "Epoch [1/5], Step [166/5873], Loss: 3.9913\n",
      "Epoch [1/5], Step [167/5873], Loss: 4.6953\n",
      "Epoch [1/5], Step [168/5873], Loss: 5.7978\n",
      "Epoch [1/5], Step [169/5873], Loss: 4.7941\n",
      "Epoch [1/5], Step [170/5873], Loss: 5.4918\n",
      "Epoch [1/5], Step [171/5873], Loss: 4.7738\n",
      "Epoch [1/5], Step [172/5873], Loss: 5.4814\n",
      "Epoch [1/5], Step [173/5873], Loss: 4.7782\n",
      "Epoch [1/5], Step [174/5873], Loss: 4.7908\n",
      "Epoch [1/5], Step [175/5873], Loss: 5.1686\n",
      "Epoch [1/5], Step [176/5873], Loss: 5.1696\n",
      "Epoch [1/5], Step [177/5873], Loss: 3.6616\n",
      "Epoch [1/5], Step [178/5873], Loss: 3.9742\n",
      "Epoch [1/5], Step [179/5873], Loss: 3.6362\n",
      "Epoch [1/5], Step [180/5873], Loss: 4.3161\n",
      "Epoch [1/5], Step [181/5873], Loss: 5.4564\n",
      "Epoch [1/5], Step [182/5873], Loss: 5.2254\n",
      "Epoch [1/5], Step [183/5873], Loss: 4.5787\n",
      "Epoch [1/5], Step [184/5873], Loss: 3.6247\n",
      "Epoch [1/5], Step [185/5873], Loss: 5.1705\n",
      "Epoch [1/5], Step [186/5873], Loss: 5.4327\n",
      "Epoch [1/5], Step [187/5873], Loss: 3.7988\n",
      "Epoch [1/5], Step [188/5873], Loss: 3.9587\n",
      "Epoch [1/5], Step [189/5873], Loss: 4.5769\n",
      "Epoch [1/5], Step [190/5873], Loss: 4.7858\n",
      "Epoch [1/5], Step [191/5873], Loss: 5.2570\n",
      "Epoch [1/5], Step [192/5873], Loss: 6.0611\n",
      "Epoch [1/5], Step [193/5873], Loss: 3.3747\n",
      "Epoch [1/5], Step [194/5873], Loss: 4.7857\n",
      "Epoch [1/5], Step [195/5873], Loss: 4.9012\n",
      "Epoch [1/5], Step [196/5873], Loss: 5.5215\n",
      "Epoch [1/5], Step [197/5873], Loss: 4.6938\n",
      "Epoch [1/5], Step [198/5873], Loss: 4.5914\n",
      "Epoch [1/5], Step [199/5873], Loss: 4.9902\n",
      "Epoch [1/5], Step [200/5873], Loss: 4.9077\n",
      "Epoch [1/5], Step [201/5873], Loss: 3.9626\n",
      "Epoch [1/5], Step [202/5873], Loss: 5.2990\n",
      "Epoch [1/5], Step [203/5873], Loss: 5.9146\n",
      "Epoch [1/5], Step [204/5873], Loss: 3.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [205/5873], Loss: 4.3659\n",
      "Epoch [1/5], Step [206/5873], Loss: 3.9992\n",
      "Epoch [1/5], Step [207/5873], Loss: 4.0589\n",
      "Epoch [1/5], Step [208/5873], Loss: 5.9578\n",
      "Epoch [1/5], Step [209/5873], Loss: 3.9134\n",
      "Epoch [1/5], Step [210/5873], Loss: 5.0040\n",
      "Epoch [1/5], Step [211/5873], Loss: 3.5954\n",
      "Epoch [1/5], Step [212/5873], Loss: 4.8574\n",
      "Epoch [1/5], Step [213/5873], Loss: 4.4317\n",
      "Epoch [1/5], Step [214/5873], Loss: 4.9772\n",
      "Epoch [1/5], Step [215/5873], Loss: 4.2038\n",
      "Epoch [1/5], Step [216/5873], Loss: 4.0788\n",
      "Epoch [1/5], Step [217/5873], Loss: 4.7345\n",
      "Epoch [1/5], Step [218/5873], Loss: 4.3962\n",
      "Epoch [1/5], Step [219/5873], Loss: 3.8645\n",
      "Epoch [1/5], Step [220/5873], Loss: 5.0162\n",
      "Epoch [1/5], Step [221/5873], Loss: 4.8326\n",
      "Epoch [1/5], Step [222/5873], Loss: 4.2788\n",
      "Epoch [1/5], Step [223/5873], Loss: 4.4451\n",
      "Epoch [1/5], Step [224/5873], Loss: 3.8472\n",
      "Epoch [1/5], Step [225/5873], Loss: 2.6126\n",
      "Epoch [1/5], Step [226/5873], Loss: 3.7385\n",
      "Epoch [1/5], Step [227/5873], Loss: 4.5662\n",
      "Epoch [1/5], Step [228/5873], Loss: 3.1790\n",
      "Epoch [1/5], Step [229/5873], Loss: 4.6716\n",
      "Epoch [1/5], Step [230/5873], Loss: 4.6669\n",
      "Epoch [1/5], Step [231/5873], Loss: 4.2376\n",
      "Epoch [1/5], Step [232/5873], Loss: 4.8862\n",
      "Epoch [1/5], Step [233/5873], Loss: 4.2569\n",
      "Epoch [1/5], Step [234/5873], Loss: 5.4999\n",
      "Epoch [1/5], Step [235/5873], Loss: 4.2476\n",
      "Epoch [1/5], Step [236/5873], Loss: 5.0775\n",
      "Epoch [1/5], Step [237/5873], Loss: 4.2687\n",
      "Epoch [1/5], Step [238/5873], Loss: 3.3514\n",
      "Epoch [1/5], Step [239/5873], Loss: 4.6026\n",
      "Epoch [1/5], Step [240/5873], Loss: 3.8990\n",
      "Epoch [1/5], Step [241/5873], Loss: 5.4627\n",
      "Epoch [1/5], Step [242/5873], Loss: 5.7078\n",
      "Epoch [1/5], Step [243/5873], Loss: 5.1047\n",
      "Epoch [1/5], Step [244/5873], Loss: 5.6290\n",
      "Epoch [1/5], Step [245/5873], Loss: 5.1299\n",
      "Epoch [1/5], Step [246/5873], Loss: 4.0832\n",
      "Epoch [1/5], Step [247/5873], Loss: 4.3640\n",
      "Epoch [1/5], Step [248/5873], Loss: 4.5440\n",
      "Epoch [1/5], Step [249/5873], Loss: 3.1476\n",
      "Epoch [1/5], Step [250/5873], Loss: 3.5276\n",
      "Epoch [1/5], Step [251/5873], Loss: 4.2071\n",
      "Epoch [1/5], Step [252/5873], Loss: 4.0178\n",
      "Epoch [1/5], Step [253/5873], Loss: 3.4236\n",
      "Epoch [1/5], Step [254/5873], Loss: 3.8735\n",
      "Epoch [1/5], Step [255/5873], Loss: 4.4227\n",
      "Epoch [1/5], Step [256/5873], Loss: 4.5933\n",
      "Epoch [1/5], Step [257/5873], Loss: 3.3174\n",
      "Epoch [1/5], Step [258/5873], Loss: 4.9468\n",
      "Epoch [1/5], Step [259/5873], Loss: 4.4679\n",
      "Epoch [1/5], Step [260/5873], Loss: 3.3630\n",
      "Epoch [1/5], Step [261/5873], Loss: 2.6240\n",
      "Epoch [1/5], Step [262/5873], Loss: 4.6503\n",
      "Epoch [1/5], Step [263/5873], Loss: 3.6543\n",
      "Epoch [1/5], Step [264/5873], Loss: 5.7578\n",
      "Epoch [1/5], Step [265/5873], Loss: 3.4842\n",
      "Epoch [1/5], Step [266/5873], Loss: 5.5987\n",
      "Epoch [1/5], Step [267/5873], Loss: 2.9219\n",
      "Epoch [1/5], Step [268/5873], Loss: 5.6824\n",
      "Epoch [1/5], Step [269/5873], Loss: 3.3339\n",
      "Epoch [1/5], Step [270/5873], Loss: 5.4081\n",
      "Epoch [1/5], Step [271/5873], Loss: 3.7507\n",
      "Epoch [1/5], Step [272/5873], Loss: 4.6131\n",
      "Epoch [1/5], Step [273/5873], Loss: 4.6445\n",
      "Epoch [1/5], Step [274/5873], Loss: 4.4177\n",
      "Epoch [1/5], Step [275/5873], Loss: 5.6044\n",
      "Epoch [1/5], Step [276/5873], Loss: 5.3692\n",
      "Epoch [1/5], Step [277/5873], Loss: 2.9550\n",
      "Epoch [1/5], Step [278/5873], Loss: 5.2746\n",
      "Epoch [1/5], Step [279/5873], Loss: 5.2274\n",
      "Epoch [1/5], Step [280/5873], Loss: 3.9355\n",
      "Epoch [1/5], Step [281/5873], Loss: 4.6122\n",
      "Epoch [1/5], Step [282/5873], Loss: 4.8862\n",
      "Epoch [1/5], Step [283/5873], Loss: 3.5341\n",
      "Epoch [1/5], Step [284/5873], Loss: 6.0009\n",
      "Epoch [1/5], Step [285/5873], Loss: 5.7514\n",
      "Epoch [1/5], Step [286/5873], Loss: 2.7368\n",
      "Epoch [1/5], Step [287/5873], Loss: 5.7028\n",
      "Epoch [1/5], Step [288/5873], Loss: 3.0227\n",
      "Epoch [1/5], Step [289/5873], Loss: 4.8936\n",
      "Epoch [1/5], Step [290/5873], Loss: 4.6474\n",
      "Epoch [1/5], Step [291/5873], Loss: 5.5087\n",
      "Epoch [1/5], Step [292/5873], Loss: 2.5939\n",
      "Epoch [1/5], Step [293/5873], Loss: 4.7855\n",
      "Epoch [1/5], Step [294/5873], Loss: 5.9773\n",
      "Epoch [1/5], Step [295/5873], Loss: 6.0221\n",
      "Epoch [1/5], Step [296/5873], Loss: 5.5840\n",
      "Epoch [1/5], Step [297/5873], Loss: 5.7578\n",
      "Epoch [1/5], Step [298/5873], Loss: 5.0673\n",
      "Epoch [1/5], Step [299/5873], Loss: 5.6762\n",
      "Epoch [1/5], Step [300/5873], Loss: 4.3954\n",
      "Epoch [1/5], Step [301/5873], Loss: 5.4420\n",
      "Epoch [1/5], Step [302/5873], Loss: 3.3361\n",
      "Epoch [1/5], Step [303/5873], Loss: 4.0617\n",
      "Epoch [1/5], Step [304/5873], Loss: 4.2299\n",
      "Epoch [1/5], Step [305/5873], Loss: 5.3516\n",
      "Epoch [1/5], Step [306/5873], Loss: 3.4491\n",
      "Epoch [1/5], Step [307/5873], Loss: 4.4068\n",
      "Epoch [1/5], Step [308/5873], Loss: 5.4966\n",
      "Epoch [1/5], Step [309/5873], Loss: 3.5714\n",
      "Epoch [1/5], Step [310/5873], Loss: 4.8543\n",
      "Epoch [1/5], Step [311/5873], Loss: 5.8503\n",
      "Epoch [1/5], Step [312/5873], Loss: 5.3559\n",
      "Epoch [1/5], Step [313/5873], Loss: 4.6030\n",
      "Epoch [1/5], Step [314/5873], Loss: 4.3597\n",
      "Epoch [1/5], Step [315/5873], Loss: 3.2433\n",
      "Epoch [1/5], Step [316/5873], Loss: 5.0103\n",
      "Epoch [1/5], Step [317/5873], Loss: 2.9503\n",
      "Epoch [1/5], Step [318/5873], Loss: 3.3983\n",
      "Epoch [1/5], Step [319/5873], Loss: 4.2872\n",
      "Epoch [1/5], Step [320/5873], Loss: 3.9273\n",
      "Epoch [1/5], Step [321/5873], Loss: 5.1965\n",
      "Epoch [1/5], Step [322/5873], Loss: 3.5416\n",
      "Epoch [1/5], Step [323/5873], Loss: 6.3359\n",
      "Epoch [1/5], Step [324/5873], Loss: 5.0454\n",
      "Epoch [1/5], Step [325/5873], Loss: 3.7866\n",
      "Epoch [1/5], Step [326/5873], Loss: 3.9102\n",
      "Epoch [1/5], Step [327/5873], Loss: 3.7191\n",
      "Epoch [1/5], Step [328/5873], Loss: 4.3098\n",
      "Epoch [1/5], Step [329/5873], Loss: 5.0980\n",
      "Epoch [1/5], Step [330/5873], Loss: 3.9046\n",
      "Epoch [1/5], Step [331/5873], Loss: 2.5457\n",
      "Epoch [1/5], Step [332/5873], Loss: 3.1836\n",
      "Epoch [1/5], Step [333/5873], Loss: 5.2538\n",
      "Epoch [1/5], Step [334/5873], Loss: 5.3823\n",
      "Epoch [1/5], Step [335/5873], Loss: 2.6854\n",
      "Epoch [1/5], Step [336/5873], Loss: 4.4591\n",
      "Epoch [1/5], Step [337/5873], Loss: 4.6670\n",
      "Epoch [1/5], Step [338/5873], Loss: 5.4672\n",
      "Epoch [1/5], Step [339/5873], Loss: 4.1730\n",
      "Epoch [1/5], Step [340/5873], Loss: 4.8831\n",
      "Epoch [1/5], Step [341/5873], Loss: 5.2591\n",
      "Epoch [1/5], Step [342/5873], Loss: 3.3719\n",
      "Epoch [1/5], Step [343/5873], Loss: 3.6584\n",
      "Epoch [1/5], Step [344/5873], Loss: 2.7213\n",
      "Epoch [1/5], Step [345/5873], Loss: 4.4459\n",
      "Epoch [1/5], Step [346/5873], Loss: 4.5226\n",
      "Epoch [1/5], Step [347/5873], Loss: 5.1802\n",
      "Epoch [1/5], Step [348/5873], Loss: 5.1395\n",
      "Epoch [1/5], Step [349/5873], Loss: 5.0815\n",
      "Epoch [1/5], Step [350/5873], Loss: 6.4043\n",
      "Epoch [1/5], Step [351/5873], Loss: 5.4790\n",
      "Epoch [1/5], Step [352/5873], Loss: 4.4792\n",
      "Epoch [1/5], Step [353/5873], Loss: 2.7754\n",
      "Epoch [1/5], Step [354/5873], Loss: 5.5165\n",
      "Epoch [1/5], Step [355/5873], Loss: 3.8963\n",
      "Epoch [1/5], Step [356/5873], Loss: 3.5557\n",
      "Epoch [1/5], Step [357/5873], Loss: 4.5800\n",
      "Epoch [1/5], Step [358/5873], Loss: 5.3204\n",
      "Epoch [1/5], Step [359/5873], Loss: 2.9053\n",
      "Epoch [1/5], Step [360/5873], Loss: 3.6119\n",
      "Epoch [1/5], Step [361/5873], Loss: 3.6778\n",
      "Epoch [1/5], Step [362/5873], Loss: 2.5030\n",
      "Epoch [1/5], Step [363/5873], Loss: 5.2054\n",
      "Epoch [1/5], Step [364/5873], Loss: 4.4340\n",
      "Epoch [1/5], Step [365/5873], Loss: 4.8520\n",
      "Epoch [1/5], Step [366/5873], Loss: 2.7161\n",
      "Epoch [1/5], Step [367/5873], Loss: 4.0361\n",
      "Epoch [1/5], Step [368/5873], Loss: 4.3072\n",
      "Epoch [1/5], Step [369/5873], Loss: 5.5802\n",
      "Epoch [1/5], Step [370/5873], Loss: 4.5310\n",
      "Epoch [1/5], Step [371/5873], Loss: 5.6313\n",
      "Epoch [1/5], Step [372/5873], Loss: 4.2588\n",
      "Epoch [1/5], Step [373/5873], Loss: 5.3855\n",
      "Epoch [1/5], Step [374/5873], Loss: 3.2686\n",
      "Epoch [1/5], Step [375/5873], Loss: 4.0512\n",
      "Epoch [1/5], Step [376/5873], Loss: 4.5566\n",
      "Epoch [1/5], Step [377/5873], Loss: 3.6648\n",
      "Epoch [1/5], Step [378/5873], Loss: 4.8953\n",
      "Epoch [1/5], Step [379/5873], Loss: 4.6499\n",
      "Epoch [1/5], Step [380/5873], Loss: 6.3658\n",
      "Epoch [1/5], Step [381/5873], Loss: 3.1328\n",
      "Epoch [1/5], Step [382/5873], Loss: 5.5302\n",
      "Epoch [1/5], Step [383/5873], Loss: 5.2235\n",
      "Epoch [1/5], Step [384/5873], Loss: 5.0283\n",
      "Epoch [1/5], Step [385/5873], Loss: 2.3614\n",
      "Epoch [1/5], Step [386/5873], Loss: 4.4793\n",
      "Epoch [1/5], Step [387/5873], Loss: 2.8838\n",
      "Epoch [1/5], Step [388/5873], Loss: 5.0934\n",
      "Epoch [1/5], Step [389/5873], Loss: 4.4402\n",
      "Epoch [1/5], Step [390/5873], Loss: 5.1841\n",
      "Epoch [1/5], Step [391/5873], Loss: 2.6912\n",
      "Epoch [1/5], Step [392/5873], Loss: 5.3078\n",
      "Epoch [1/5], Step [393/5873], Loss: 2.8530\n",
      "Epoch [1/5], Step [394/5873], Loss: 5.4211\n",
      "Epoch [1/5], Step [395/5873], Loss: 4.7035\n",
      "Epoch [1/5], Step [396/5873], Loss: 5.4469\n",
      "Epoch [1/5], Step [397/5873], Loss: 5.9117\n",
      "Epoch [1/5], Step [398/5873], Loss: 5.8229\n",
      "Epoch [1/5], Step [399/5873], Loss: 4.3054\n",
      "Epoch [1/5], Step [400/5873], Loss: 5.1454\n",
      "Epoch [1/5], Step [401/5873], Loss: 2.6775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [402/5873], Loss: 4.5869\n",
      "Epoch [1/5], Step [403/5873], Loss: 5.2177\n",
      "Epoch [1/5], Step [404/5873], Loss: 4.3431\n",
      "Epoch [1/5], Step [405/5873], Loss: 4.8761\n",
      "Epoch [1/5], Step [406/5873], Loss: 5.0402\n",
      "Epoch [1/5], Step [407/5873], Loss: 5.8858\n",
      "Epoch [1/5], Step [408/5873], Loss: 4.7356\n",
      "Epoch [1/5], Step [409/5873], Loss: 4.7754\n",
      "Epoch [1/5], Step [410/5873], Loss: 2.7832\n",
      "Epoch [1/5], Step [411/5873], Loss: 4.1740\n",
      "Epoch [1/5], Step [412/5873], Loss: 2.7878\n",
      "Epoch [1/5], Step [413/5873], Loss: 3.4709\n",
      "Epoch [1/5], Step [414/5873], Loss: 4.9337\n",
      "Epoch [1/5], Step [415/5873], Loss: 4.7412\n",
      "Epoch [1/5], Step [416/5873], Loss: 4.8966\n",
      "Epoch [1/5], Step [417/5873], Loss: 5.7508\n",
      "Epoch [1/5], Step [418/5873], Loss: 4.9414\n",
      "Epoch [1/5], Step [419/5873], Loss: 4.5715\n",
      "Epoch [1/5], Step [420/5873], Loss: 4.3554\n",
      "Epoch [1/5], Step [421/5873], Loss: 4.8064\n",
      "Epoch [1/5], Step [422/5873], Loss: 3.3533\n",
      "Epoch [1/5], Step [423/5873], Loss: 4.7458\n",
      "Epoch [1/5], Step [424/5873], Loss: 4.6322\n",
      "Epoch [1/5], Step [425/5873], Loss: 4.4573\n",
      "Epoch [1/5], Step [426/5873], Loss: 4.6201\n",
      "Epoch [1/5], Step [427/5873], Loss: 4.1722\n",
      "Epoch [1/5], Step [428/5873], Loss: 4.7477\n",
      "Epoch [1/5], Step [429/5873], Loss: 3.9975\n",
      "Epoch [1/5], Step [430/5873], Loss: 5.3785\n",
      "Epoch [1/5], Step [431/5873], Loss: 3.7472\n",
      "Epoch [1/5], Step [432/5873], Loss: 4.3154\n",
      "Epoch [1/5], Step [433/5873], Loss: 3.9278\n",
      "Epoch [1/5], Step [434/5873], Loss: 3.3285\n",
      "Epoch [1/5], Step [435/5873], Loss: 4.5363\n",
      "Epoch [1/5], Step [436/5873], Loss: 4.1595\n",
      "Epoch [1/5], Step [437/5873], Loss: 4.4411\n",
      "Epoch [1/5], Step [438/5873], Loss: 2.7657\n",
      "Epoch [1/5], Step [439/5873], Loss: 2.8887\n",
      "Epoch [1/5], Step [440/5873], Loss: 4.4354\n",
      "Epoch [1/5], Step [441/5873], Loss: 5.4295\n",
      "Epoch [1/5], Step [442/5873], Loss: 3.0475\n",
      "Epoch [1/5], Step [443/5873], Loss: 3.8169\n",
      "Epoch [1/5], Step [444/5873], Loss: 2.7389\n",
      "Epoch [1/5], Step [445/5873], Loss: 5.6714\n",
      "Epoch [1/5], Step [446/5873], Loss: 5.6507\n",
      "Epoch [1/5], Step [447/5873], Loss: 3.3014\n",
      "Epoch [1/5], Step [448/5873], Loss: 2.6126\n",
      "Epoch [1/5], Step [449/5873], Loss: 4.8453\n",
      "Epoch [1/5], Step [450/5873], Loss: 5.2315\n",
      "Epoch [1/5], Step [451/5873], Loss: 5.7755\n",
      "Epoch [1/5], Step [452/5873], Loss: 4.0172\n",
      "Epoch [1/5], Step [453/5873], Loss: 4.8952\n",
      "Epoch [1/5], Step [454/5873], Loss: 3.2696\n",
      "Epoch [1/5], Step [455/5873], Loss: 4.3472\n",
      "Epoch [1/5], Step [456/5873], Loss: 2.8550\n",
      "Epoch [1/5], Step [457/5873], Loss: 5.5392\n",
      "Epoch [1/5], Step [458/5873], Loss: 4.5556\n",
      "Epoch [1/5], Step [459/5873], Loss: 4.3031\n",
      "Epoch [1/5], Step [460/5873], Loss: 4.4455\n",
      "Epoch [1/5], Step [461/5873], Loss: 5.2054\n",
      "Epoch [1/5], Step [462/5873], Loss: 5.2905\n",
      "Epoch [1/5], Step [463/5873], Loss: 3.9361\n",
      "Epoch [1/5], Step [464/5873], Loss: 3.8202\n",
      "Epoch [1/5], Step [465/5873], Loss: 3.8737\n",
      "Epoch [1/5], Step [466/5873], Loss: 5.6314\n",
      "Epoch [1/5], Step [467/5873], Loss: 4.7702\n",
      "Epoch [1/5], Step [468/5873], Loss: 2.9901\n",
      "Epoch [1/5], Step [469/5873], Loss: 3.1082\n",
      "Epoch [1/5], Step [470/5873], Loss: 4.0186\n",
      "Epoch [1/5], Step [471/5873], Loss: 4.5895\n",
      "Epoch [1/5], Step [472/5873], Loss: 4.8032\n",
      "Epoch [1/5], Step [473/5873], Loss: 5.0120\n",
      "Epoch [1/5], Step [474/5873], Loss: 3.8053\n",
      "Epoch [1/5], Step [475/5873], Loss: 4.6083\n",
      "Epoch [1/5], Step [476/5873], Loss: 4.2093\n",
      "Epoch [1/5], Step [477/5873], Loss: 4.8242\n",
      "Epoch [1/5], Step [478/5873], Loss: 2.4367\n",
      "Epoch [1/5], Step [479/5873], Loss: 5.3793\n",
      "Epoch [1/5], Step [480/5873], Loss: 4.4761\n",
      "Epoch [1/5], Step [481/5873], Loss: 2.7253\n",
      "Epoch [1/5], Step [482/5873], Loss: 3.0505\n",
      "Epoch [1/5], Step [483/5873], Loss: 4.5782\n",
      "Epoch [1/5], Step [484/5873], Loss: 4.8111\n",
      "Epoch [1/5], Step [485/5873], Loss: 5.8818\n",
      "Epoch [1/5], Step [486/5873], Loss: 2.4903\n",
      "Epoch [1/5], Step [487/5873], Loss: 4.7848\n",
      "Epoch [1/5], Step [488/5873], Loss: 4.5872\n",
      "Epoch [1/5], Step [489/5873], Loss: 4.2410\n",
      "Epoch [1/5], Step [490/5873], Loss: 1.6007\n",
      "Epoch [1/5], Step [491/5873], Loss: 4.1339\n",
      "Epoch [1/5], Step [492/5873], Loss: 4.9783\n",
      "Epoch [1/5], Step [493/5873], Loss: 3.6463\n",
      "Epoch [1/5], Step [494/5873], Loss: 5.7095\n",
      "Epoch [1/5], Step [495/5873], Loss: 5.0185\n",
      "Epoch [1/5], Step [496/5873], Loss: 4.9709\n",
      "Epoch [1/5], Step [497/5873], Loss: 5.3581\n",
      "Epoch [1/5], Step [498/5873], Loss: 3.7669\n",
      "Epoch [1/5], Step [499/5873], Loss: 4.4314\n",
      "Epoch [1/5], Step [500/5873], Loss: 3.9542\n",
      "Epoch [1/5], Step [501/5873], Loss: 4.5327\n",
      "Epoch [1/5], Step [502/5873], Loss: 5.6110\n",
      "Epoch [1/5], Step [503/5873], Loss: 3.2596\n",
      "Epoch [1/5], Step [504/5873], Loss: 4.1507\n",
      "Epoch [1/5], Step [505/5873], Loss: 2.0449\n",
      "Epoch [1/5], Step [506/5873], Loss: 4.1016\n",
      "Epoch [1/5], Step [507/5873], Loss: 3.7471\n",
      "Epoch [1/5], Step [508/5873], Loss: 3.1558\n",
      "Epoch [1/5], Step [509/5873], Loss: 4.8898\n",
      "Epoch [1/5], Step [510/5873], Loss: 5.8083\n",
      "Epoch [1/5], Step [511/5873], Loss: 4.4787\n",
      "Epoch [1/5], Step [512/5873], Loss: 3.8907\n",
      "Epoch [1/5], Step [513/5873], Loss: 3.7127\n",
      "Epoch [1/5], Step [514/5873], Loss: 5.5896\n",
      "Epoch [1/5], Step [515/5873], Loss: 3.9343\n",
      "Epoch [1/5], Step [516/5873], Loss: 1.9502\n",
      "Epoch [1/5], Step [517/5873], Loss: 4.2983\n",
      "Epoch [1/5], Step [518/5873], Loss: 2.4078\n",
      "Epoch [1/5], Step [519/5873], Loss: 3.1835\n",
      "Epoch [1/5], Step [520/5873], Loss: 4.4793\n",
      "Epoch [1/5], Step [521/5873], Loss: 1.7989\n",
      "Epoch [1/5], Step [522/5873], Loss: 4.3760\n",
      "Epoch [1/5], Step [523/5873], Loss: 5.0600\n",
      "Epoch [1/5], Step [524/5873], Loss: 4.0564\n",
      "Epoch [1/5], Step [525/5873], Loss: 4.8779\n",
      "Epoch [1/5], Step [526/5873], Loss: 3.8541\n",
      "Epoch [1/5], Step [527/5873], Loss: 6.1289\n",
      "Epoch [1/5], Step [528/5873], Loss: 5.5441\n",
      "Epoch [1/5], Step [529/5873], Loss: 5.1488\n",
      "Epoch [1/5], Step [530/5873], Loss: 5.9250\n",
      "Epoch [1/5], Step [531/5873], Loss: 4.3777\n",
      "Epoch [1/5], Step [532/5873], Loss: 4.8000\n",
      "Epoch [1/5], Step [533/5873], Loss: 4.6979\n",
      "Epoch [1/5], Step [534/5873], Loss: 4.7137\n",
      "Epoch [1/5], Step [535/5873], Loss: 5.4179\n",
      "Epoch [1/5], Step [536/5873], Loss: 4.9793\n",
      "Epoch [1/5], Step [537/5873], Loss: 5.4512\n",
      "Epoch [1/5], Step [538/5873], Loss: 4.4971\n",
      "Epoch [1/5], Step [539/5873], Loss: 4.1996\n",
      "Epoch [1/5], Step [540/5873], Loss: 4.4136\n",
      "Epoch [1/5], Step [541/5873], Loss: 5.8170\n",
      "Epoch [1/5], Step [542/5873], Loss: 5.6974\n",
      "Epoch [1/5], Step [543/5873], Loss: 4.2450\n",
      "Epoch [1/5], Step [544/5873], Loss: 6.1128\n",
      "Epoch [1/5], Step [545/5873], Loss: 4.9271\n",
      "Epoch [1/5], Step [546/5873], Loss: 5.3087\n",
      "Epoch [1/5], Step [547/5873], Loss: 4.8579\n",
      "Epoch [1/5], Step [548/5873], Loss: 4.1571\n",
      "Epoch [1/5], Step [549/5873], Loss: 5.2745\n",
      "Epoch [1/5], Step [550/5873], Loss: 3.1822\n",
      "Epoch [1/5], Step [551/5873], Loss: 3.9649\n",
      "Epoch [1/5], Step [552/5873], Loss: 4.4287\n",
      "Epoch [1/5], Step [553/5873], Loss: 5.3213\n",
      "Epoch [1/5], Step [554/5873], Loss: 5.8635\n",
      "Epoch [1/5], Step [555/5873], Loss: 5.7262\n",
      "Epoch [1/5], Step [556/5873], Loss: 3.9884\n",
      "Epoch [1/5], Step [557/5873], Loss: 5.4933\n",
      "Epoch [1/5], Step [558/5873], Loss: 3.6060\n",
      "Epoch [1/5], Step [559/5873], Loss: 3.6460\n",
      "Epoch [1/5], Step [560/5873], Loss: 4.0061\n",
      "Epoch [1/5], Step [561/5873], Loss: 4.9289\n",
      "Epoch [1/5], Step [562/5873], Loss: 4.7550\n",
      "Epoch [1/5], Step [563/5873], Loss: 4.2657\n",
      "Epoch [1/5], Step [564/5873], Loss: 3.8946\n",
      "Epoch [1/5], Step [565/5873], Loss: 2.0900\n",
      "Epoch [1/5], Step [566/5873], Loss: 5.7272\n",
      "Epoch [1/5], Step [567/5873], Loss: 3.9611\n",
      "Epoch [1/5], Step [568/5873], Loss: 2.5988\n",
      "Epoch [1/5], Step [569/5873], Loss: 4.1606\n",
      "Epoch [1/5], Step [570/5873], Loss: 5.2100\n",
      "Epoch [1/5], Step [571/5873], Loss: 4.4353\n",
      "Epoch [1/5], Step [572/5873], Loss: 4.3778\n",
      "Epoch [1/5], Step [573/5873], Loss: 3.6985\n",
      "Epoch [1/5], Step [574/5873], Loss: 3.8109\n",
      "Epoch [1/5], Step [575/5873], Loss: 5.1562\n",
      "Epoch [1/5], Step [576/5873], Loss: 4.7901\n",
      "Epoch [1/5], Step [577/5873], Loss: 4.7021\n",
      "Epoch [1/5], Step [578/5873], Loss: 5.7171\n",
      "Epoch [1/5], Step [579/5873], Loss: 4.8586\n",
      "Epoch [1/5], Step [580/5873], Loss: 2.6893\n",
      "Epoch [1/5], Step [581/5873], Loss: 3.5108\n",
      "Epoch [1/5], Step [582/5873], Loss: 4.4200\n",
      "Epoch [1/5], Step [583/5873], Loss: 4.4660\n",
      "Epoch [1/5], Step [584/5873], Loss: 5.0765\n",
      "Epoch [1/5], Step [585/5873], Loss: 4.4774\n",
      "Epoch [1/5], Step [586/5873], Loss: 4.0930\n",
      "Epoch [1/5], Step [587/5873], Loss: 4.5573\n",
      "Epoch [1/5], Step [588/5873], Loss: 4.9397\n",
      "Epoch [1/5], Step [589/5873], Loss: 4.2218\n",
      "Epoch [1/5], Step [590/5873], Loss: 5.5622\n",
      "Epoch [1/5], Step [591/5873], Loss: 2.3875\n",
      "Epoch [1/5], Step [592/5873], Loss: 2.4956\n",
      "Epoch [1/5], Step [593/5873], Loss: 5.0754\n",
      "Epoch [1/5], Step [594/5873], Loss: 2.3488\n",
      "Epoch [1/5], Step [595/5873], Loss: 2.7272\n",
      "Epoch [1/5], Step [596/5873], Loss: 4.2614\n",
      "Epoch [1/5], Step [597/5873], Loss: 4.7698\n",
      "Epoch [1/5], Step [598/5873], Loss: 4.8886\n",
      "Epoch [1/5], Step [599/5873], Loss: 3.4162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [600/5873], Loss: 6.2089\n",
      "Epoch [1/5], Step [601/5873], Loss: 3.7181\n",
      "Epoch [1/5], Step [602/5873], Loss: 5.4958\n",
      "Epoch [1/5], Step [603/5873], Loss: 5.6453\n",
      "Epoch [1/5], Step [604/5873], Loss: 5.1738\n",
      "Epoch [1/5], Step [605/5873], Loss: 4.5670\n",
      "Epoch [1/5], Step [606/5873], Loss: 3.7497\n",
      "Epoch [1/5], Step [607/5873], Loss: 3.3086\n",
      "Epoch [1/5], Step [608/5873], Loss: 4.3189\n",
      "Epoch [1/5], Step [609/5873], Loss: 3.8634\n",
      "Epoch [1/5], Step [610/5873], Loss: 5.3895\n",
      "Epoch [1/5], Step [611/5873], Loss: 5.1450\n",
      "Epoch [1/5], Step [612/5873], Loss: 5.1879\n",
      "Epoch [1/5], Step [613/5873], Loss: 4.1889\n",
      "Epoch [1/5], Step [614/5873], Loss: 4.2896\n",
      "Epoch [1/5], Step [615/5873], Loss: 4.1794\n",
      "Epoch [1/5], Step [616/5873], Loss: 3.7118\n",
      "Epoch [1/5], Step [617/5873], Loss: 3.2044\n",
      "Epoch [1/5], Step [618/5873], Loss: 4.9006\n",
      "Epoch [1/5], Step [619/5873], Loss: 3.5021\n",
      "Epoch [1/5], Step [620/5873], Loss: 4.8355\n",
      "Epoch [1/5], Step [621/5873], Loss: 4.7506\n",
      "Epoch [1/5], Step [622/5873], Loss: 4.0225\n",
      "Epoch [1/5], Step [623/5873], Loss: 2.5259\n",
      "Epoch [1/5], Step [624/5873], Loss: 5.7465\n",
      "Epoch [1/5], Step [625/5873], Loss: 5.1336\n",
      "Epoch [1/5], Step [626/5873], Loss: 3.6983\n",
      "Epoch [1/5], Step [627/5873], Loss: 5.8205\n",
      "Epoch [1/5], Step [628/5873], Loss: 3.8247\n",
      "Epoch [1/5], Step [629/5873], Loss: 3.7228\n",
      "Epoch [1/5], Step [630/5873], Loss: 3.3532\n",
      "Epoch [1/5], Step [631/5873], Loss: 4.1138\n",
      "Epoch [1/5], Step [632/5873], Loss: 3.8569\n",
      "Epoch [1/5], Step [633/5873], Loss: 4.2041\n",
      "Epoch [1/5], Step [634/5873], Loss: 2.0729\n",
      "Epoch [1/5], Step [635/5873], Loss: 5.4899\n",
      "Epoch [1/5], Step [636/5873], Loss: 6.0482\n",
      "Epoch [1/5], Step [637/5873], Loss: 5.3623\n",
      "Epoch [1/5], Step [638/5873], Loss: 5.1556\n",
      "Epoch [1/5], Step [639/5873], Loss: 4.1185\n",
      "Epoch [1/5], Step [640/5873], Loss: 4.1081\n",
      "Epoch [1/5], Step [641/5873], Loss: 4.3533\n",
      "Epoch [1/5], Step [642/5873], Loss: 3.7773\n",
      "Epoch [1/5], Step [643/5873], Loss: 4.3837\n",
      "Epoch [1/5], Step [644/5873], Loss: 4.5702\n",
      "Epoch [1/5], Step [645/5873], Loss: 4.1245\n",
      "Epoch [1/5], Step [646/5873], Loss: 5.4284\n",
      "Epoch [1/5], Step [647/5873], Loss: 5.1233\n",
      "Epoch [1/5], Step [648/5873], Loss: 4.1139\n",
      "Epoch [1/5], Step [649/5873], Loss: 5.2050\n",
      "Epoch [1/5], Step [650/5873], Loss: 5.1717\n",
      "Epoch [1/5], Step [651/5873], Loss: 5.4873\n",
      "Epoch [1/5], Step [652/5873], Loss: 4.3420\n",
      "Epoch [1/5], Step [653/5873], Loss: 4.5556\n",
      "Epoch [1/5], Step [654/5873], Loss: 5.5826\n",
      "Epoch [1/5], Step [655/5873], Loss: 5.7117\n",
      "Epoch [1/5], Step [656/5873], Loss: 3.8662\n",
      "Epoch [1/5], Step [657/5873], Loss: 4.1301\n",
      "Epoch [1/5], Step [658/5873], Loss: 3.8059\n",
      "Epoch [1/5], Step [659/5873], Loss: 4.8487\n",
      "Epoch [1/5], Step [660/5873], Loss: 4.4099\n",
      "Epoch [1/5], Step [661/5873], Loss: 5.3020\n",
      "Epoch [1/5], Step [662/5873], Loss: 3.2633\n",
      "Epoch [1/5], Step [663/5873], Loss: 3.4856\n",
      "Epoch [1/5], Step [664/5873], Loss: 3.9793\n",
      "Epoch [1/5], Step [665/5873], Loss: 3.7396\n",
      "Epoch [1/5], Step [666/5873], Loss: 5.1062\n",
      "Epoch [1/5], Step [667/5873], Loss: 5.1770\n",
      "Epoch [1/5], Step [668/5873], Loss: 4.2851\n",
      "Epoch [1/5], Step [669/5873], Loss: 3.3407\n",
      "Epoch [1/5], Step [670/5873], Loss: 5.2612\n",
      "Epoch [1/5], Step [671/5873], Loss: 3.7856\n",
      "Epoch [1/5], Step [672/5873], Loss: 2.6021\n",
      "Epoch [1/5], Step [673/5873], Loss: 4.7639\n",
      "Epoch [1/5], Step [674/5873], Loss: 3.2626\n",
      "Epoch [1/5], Step [675/5873], Loss: 5.5339\n",
      "Epoch [1/5], Step [676/5873], Loss: 4.8740\n",
      "Epoch [1/5], Step [677/5873], Loss: 6.3482\n",
      "Epoch [1/5], Step [678/5873], Loss: 2.9100\n",
      "Epoch [1/5], Step [679/5873], Loss: 5.0306\n",
      "Epoch [1/5], Step [680/5873], Loss: 2.8433\n",
      "Epoch [1/5], Step [681/5873], Loss: 5.2915\n",
      "Epoch [1/5], Step [682/5873], Loss: 5.8153\n",
      "Epoch [1/5], Step [683/5873], Loss: 4.2004\n",
      "Epoch [1/5], Step [684/5873], Loss: 4.1140\n",
      "Epoch [1/5], Step [685/5873], Loss: 3.7366\n",
      "Epoch [1/5], Step [686/5873], Loss: 4.3267\n",
      "Epoch [1/5], Step [687/5873], Loss: 5.8219\n",
      "Epoch [1/5], Step [688/5873], Loss: 4.6525\n",
      "Epoch [1/5], Step [689/5873], Loss: 4.4355\n",
      "Epoch [1/5], Step [690/5873], Loss: 4.9291\n",
      "Epoch [1/5], Step [691/5873], Loss: 3.5452\n",
      "Epoch [1/5], Step [692/5873], Loss: 4.0155\n",
      "Epoch [1/5], Step [693/5873], Loss: 4.2599\n",
      "Epoch [1/5], Step [694/5873], Loss: 5.3366\n",
      "Epoch [1/5], Step [695/5873], Loss: 4.8649\n",
      "Epoch [1/5], Step [696/5873], Loss: 4.5256\n",
      "Epoch [1/5], Step [697/5873], Loss: 5.1930\n",
      "Epoch [1/5], Step [698/5873], Loss: 5.2892\n",
      "Epoch [1/5], Step [699/5873], Loss: 2.9542\n",
      "Epoch [1/5], Step [700/5873], Loss: 4.6738\n",
      "Epoch [1/5], Step [701/5873], Loss: 5.3866\n",
      "Epoch [1/5], Step [702/5873], Loss: 3.3386\n",
      "Epoch [1/5], Step [703/5873], Loss: 5.6043\n",
      "Epoch [1/5], Step [704/5873], Loss: 4.5918\n",
      "Epoch [1/5], Step [705/5873], Loss: 3.7564\n",
      "Epoch [1/5], Step [706/5873], Loss: 4.9995\n",
      "Epoch [1/5], Step [707/5873], Loss: 3.2637\n",
      "Epoch [1/5], Step [708/5873], Loss: 5.0394\n",
      "Epoch [1/5], Step [709/5873], Loss: 3.8885\n",
      "Epoch [1/5], Step [710/5873], Loss: 3.8675\n",
      "Epoch [1/5], Step [711/5873], Loss: 3.9418\n",
      "Epoch [1/5], Step [712/5873], Loss: 4.7846\n",
      "Epoch [1/5], Step [713/5873], Loss: 5.3845\n",
      "Epoch [1/5], Step [714/5873], Loss: 3.5925\n",
      "Epoch [1/5], Step [715/5873], Loss: 5.5120\n",
      "Epoch [1/5], Step [716/5873], Loss: 3.8420\n",
      "Epoch [1/5], Step [717/5873], Loss: 5.1882\n",
      "Epoch [1/5], Step [718/5873], Loss: 4.8558\n",
      "Epoch [1/5], Step [719/5873], Loss: 4.6906\n",
      "Epoch [1/5], Step [720/5873], Loss: 4.6026\n",
      "Epoch [1/5], Step [721/5873], Loss: 4.9128\n",
      "Epoch [1/5], Step [722/5873], Loss: 4.1760\n",
      "Epoch [1/5], Step [723/5873], Loss: 4.6828\n",
      "Epoch [1/5], Step [724/5873], Loss: 3.7610\n",
      "Epoch [1/5], Step [725/5873], Loss: 4.3873\n",
      "Epoch [1/5], Step [726/5873], Loss: 4.1708\n",
      "Epoch [1/5], Step [727/5873], Loss: 3.0622\n",
      "Epoch [1/5], Step [728/5873], Loss: 5.7156\n",
      "Epoch [1/5], Step [729/5873], Loss: 2.6097\n",
      "Epoch [1/5], Step [730/5873], Loss: 3.7175\n",
      "Epoch [1/5], Step [731/5873], Loss: 6.0446\n",
      "Epoch [1/5], Step [732/5873], Loss: 3.7857\n",
      "Epoch [1/5], Step [733/5873], Loss: 4.4362\n",
      "Epoch [1/5], Step [734/5873], Loss: 4.0305\n",
      "Epoch [1/5], Step [735/5873], Loss: 4.6119\n",
      "Epoch [1/5], Step [736/5873], Loss: 1.9295\n",
      "Epoch [1/5], Step [737/5873], Loss: 3.2371\n",
      "Epoch [1/5], Step [738/5873], Loss: 2.2377\n",
      "Epoch [1/5], Step [739/5873], Loss: 5.1470\n",
      "Epoch [1/5], Step [740/5873], Loss: 4.1205\n",
      "Epoch [1/5], Step [741/5873], Loss: 5.8005\n",
      "Epoch [1/5], Step [742/5873], Loss: 3.2025\n",
      "Epoch [1/5], Step [743/5873], Loss: 4.1862\n",
      "Epoch [1/5], Step [744/5873], Loss: 4.7605\n",
      "Epoch [1/5], Step [745/5873], Loss: 3.7444\n",
      "Epoch [1/5], Step [746/5873], Loss: 5.3673\n",
      "Epoch [1/5], Step [747/5873], Loss: 5.5026\n",
      "Epoch [1/5], Step [748/5873], Loss: 3.1304\n",
      "Epoch [1/5], Step [749/5873], Loss: 4.9435\n",
      "Epoch [1/5], Step [750/5873], Loss: 4.8424\n",
      "Epoch [1/5], Step [751/5873], Loss: 3.6252\n",
      "Epoch [1/5], Step [752/5873], Loss: 5.3334\n",
      "Epoch [1/5], Step [753/5873], Loss: 5.4410\n",
      "Epoch [1/5], Step [754/5873], Loss: 3.7893\n",
      "Epoch [1/5], Step [755/5873], Loss: 5.2418\n",
      "Epoch [1/5], Step [756/5873], Loss: 4.6476\n",
      "Epoch [1/5], Step [757/5873], Loss: 4.0116\n",
      "Epoch [1/5], Step [758/5873], Loss: 5.4983\n",
      "Epoch [1/5], Step [759/5873], Loss: 4.9327\n",
      "Epoch [1/5], Step [760/5873], Loss: 3.3742\n",
      "Epoch [1/5], Step [761/5873], Loss: 3.9552\n",
      "Epoch [1/5], Step [762/5873], Loss: 5.1417\n",
      "Epoch [1/5], Step [763/5873], Loss: 3.8500\n",
      "Epoch [1/5], Step [764/5873], Loss: 4.5561\n",
      "Epoch [1/5], Step [765/5873], Loss: 4.2913\n",
      "Epoch [1/5], Step [766/5873], Loss: 5.0137\n",
      "Epoch [1/5], Step [767/5873], Loss: 5.0561\n",
      "Epoch [1/5], Step [768/5873], Loss: 3.3693\n",
      "Epoch [1/5], Step [769/5873], Loss: 5.2665\n",
      "Epoch [1/5], Step [770/5873], Loss: 4.4773\n",
      "Epoch [1/5], Step [771/5873], Loss: 4.4074\n",
      "Epoch [1/5], Step [772/5873], Loss: 5.2992\n",
      "Epoch [1/5], Step [773/5873], Loss: 5.3763\n",
      "Epoch [1/5], Step [774/5873], Loss: 4.9241\n",
      "Epoch [1/5], Step [775/5873], Loss: 3.9517\n",
      "Epoch [1/5], Step [776/5873], Loss: 4.9022\n",
      "Epoch [1/5], Step [777/5873], Loss: 4.7554\n",
      "Epoch [1/5], Step [778/5873], Loss: 4.6495\n",
      "Epoch [1/5], Step [779/5873], Loss: 3.4772\n",
      "Epoch [1/5], Step [780/5873], Loss: 3.9434\n",
      "Epoch [1/5], Step [781/5873], Loss: 4.9673\n",
      "Epoch [1/5], Step [782/5873], Loss: 4.9633\n",
      "Epoch [1/5], Step [783/5873], Loss: 3.9810\n",
      "Epoch [1/5], Step [784/5873], Loss: 5.1316\n",
      "Epoch [1/5], Step [785/5873], Loss: 4.9977\n",
      "Epoch [1/5], Step [786/5873], Loss: 5.2039\n",
      "Epoch [1/5], Step [787/5873], Loss: 3.9384\n",
      "Epoch [1/5], Step [788/5873], Loss: 3.7995\n",
      "Epoch [1/5], Step [789/5873], Loss: 3.7745\n",
      "Epoch [1/5], Step [790/5873], Loss: 4.1800\n",
      "Epoch [1/5], Step [791/5873], Loss: 4.7093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [792/5873], Loss: 4.1370\n",
      "Epoch [1/5], Step [793/5873], Loss: 3.1510\n",
      "Epoch [1/5], Step [794/5873], Loss: 3.9247\n",
      "Epoch [1/5], Step [795/5873], Loss: 5.0302\n",
      "Epoch [1/5], Step [796/5873], Loss: 3.8588\n",
      "Epoch [1/5], Step [797/5873], Loss: 3.8763\n",
      "Epoch [1/5], Step [798/5873], Loss: 4.8156\n",
      "Epoch [1/5], Step [799/5873], Loss: 3.3318\n",
      "Epoch [1/5], Step [800/5873], Loss: 5.6403\n",
      "Epoch [1/5], Step [801/5873], Loss: 3.1241\n",
      "Epoch [1/5], Step [802/5873], Loss: 5.5389\n",
      "Epoch [1/5], Step [803/5873], Loss: 3.6294\n",
      "Epoch [1/5], Step [804/5873], Loss: 4.5215\n",
      "Epoch [1/5], Step [805/5873], Loss: 3.7976\n",
      "Epoch [1/5], Step [806/5873], Loss: 4.7670\n",
      "Epoch [1/5], Step [807/5873], Loss: 3.9790\n",
      "Epoch [1/5], Step [808/5873], Loss: 3.0657\n",
      "Epoch [1/5], Step [809/5873], Loss: 4.3207\n",
      "Epoch [1/5], Step [810/5873], Loss: 4.4873\n",
      "Epoch [1/5], Step [811/5873], Loss: 5.3551\n",
      "Epoch [1/5], Step [812/5873], Loss: 3.6091\n",
      "Epoch [1/5], Step [813/5873], Loss: 5.5416\n",
      "Epoch [1/5], Step [814/5873], Loss: 3.9114\n",
      "Epoch [1/5], Step [815/5873], Loss: 5.6116\n",
      "Epoch [1/5], Step [816/5873], Loss: 3.7557\n",
      "Epoch [1/5], Step [817/5873], Loss: 3.5118\n",
      "Epoch [1/5], Step [818/5873], Loss: 4.6126\n",
      "Epoch [1/5], Step [819/5873], Loss: 4.5725\n",
      "Epoch [1/5], Step [820/5873], Loss: 4.1587\n",
      "Epoch [1/5], Step [821/5873], Loss: 3.4144\n",
      "Epoch [1/5], Step [822/5873], Loss: 4.8467\n",
      "Epoch [1/5], Step [823/5873], Loss: 4.8531\n",
      "Epoch [1/5], Step [824/5873], Loss: 5.7473\n",
      "Epoch [1/5], Step [825/5873], Loss: 4.0304\n",
      "Epoch [1/5], Step [826/5873], Loss: 5.5147\n",
      "Epoch [1/5], Step [827/5873], Loss: 4.6865\n",
      "Epoch [1/5], Step [828/5873], Loss: 5.1625\n",
      "Epoch [1/5], Step [829/5873], Loss: 4.0217\n",
      "Epoch [1/5], Step [830/5873], Loss: 3.8006\n",
      "Epoch [1/5], Step [831/5873], Loss: 3.3595\n",
      "Epoch [1/5], Step [832/5873], Loss: 3.9258\n",
      "Epoch [1/5], Step [833/5873], Loss: 4.9123\n",
      "Epoch [1/5], Step [834/5873], Loss: 2.2658\n",
      "Epoch [1/5], Step [835/5873], Loss: 4.1294\n",
      "Epoch [1/5], Step [836/5873], Loss: 5.2712\n",
      "Epoch [1/5], Step [837/5873], Loss: 3.9941\n",
      "Epoch [1/5], Step [838/5873], Loss: 3.3068\n",
      "Epoch [1/5], Step [839/5873], Loss: 2.9717\n",
      "Epoch [1/5], Step [840/5873], Loss: 5.1220\n",
      "Epoch [1/5], Step [841/5873], Loss: 3.8137\n",
      "Epoch [1/5], Step [842/5873], Loss: 3.1111\n",
      "Epoch [1/5], Step [843/5873], Loss: 3.0753\n",
      "Epoch [1/5], Step [844/5873], Loss: 2.8075\n",
      "Epoch [1/5], Step [845/5873], Loss: 5.3669\n",
      "Epoch [1/5], Step [846/5873], Loss: 4.5027\n",
      "Epoch [1/5], Step [847/5873], Loss: 3.4865\n",
      "Epoch [1/5], Step [848/5873], Loss: 5.3374\n",
      "Epoch [1/5], Step [849/5873], Loss: 6.0453\n",
      "Epoch [1/5], Step [850/5873], Loss: 4.6728\n",
      "Epoch [1/5], Step [851/5873], Loss: 5.4058\n",
      "Epoch [1/5], Step [852/5873], Loss: 4.8149\n",
      "Epoch [1/5], Step [853/5873], Loss: 4.3850\n",
      "Epoch [1/5], Step [854/5873], Loss: 4.7128\n",
      "Epoch [1/5], Step [855/5873], Loss: 4.0902\n",
      "Epoch [1/5], Step [856/5873], Loss: 5.1874\n",
      "Epoch [1/5], Step [857/5873], Loss: 4.9525\n",
      "Epoch [1/5], Step [858/5873], Loss: 4.1971\n",
      "Epoch [1/5], Step [859/5873], Loss: 4.6459\n",
      "Epoch [1/5], Step [860/5873], Loss: 3.2601\n",
      "Epoch [1/5], Step [861/5873], Loss: 4.1238\n",
      "Epoch [1/5], Step [862/5873], Loss: 4.4341\n",
      "Epoch [1/5], Step [863/5873], Loss: 2.2669\n",
      "Epoch [1/5], Step [864/5873], Loss: 3.7111\n",
      "Epoch [1/5], Step [865/5873], Loss: 4.6413\n",
      "Epoch [1/5], Step [866/5873], Loss: 3.4191\n",
      "Epoch [1/5], Step [867/5873], Loss: 3.8960\n",
      "Epoch [1/5], Step [868/5873], Loss: 4.7131\n",
      "Epoch [1/5], Step [869/5873], Loss: 4.1585\n",
      "Epoch [1/5], Step [870/5873], Loss: 3.6435\n",
      "Epoch [1/5], Step [871/5873], Loss: 2.0579\n",
      "Epoch [1/5], Step [872/5873], Loss: 3.2308\n",
      "Epoch [1/5], Step [873/5873], Loss: 2.4034\n",
      "Epoch [1/5], Step [874/5873], Loss: 3.8493\n",
      "Epoch [1/5], Step [875/5873], Loss: 3.1346\n",
      "Epoch [1/5], Step [876/5873], Loss: 3.4296\n",
      "Epoch [1/5], Step [877/5873], Loss: 4.9728\n",
      "Epoch [1/5], Step [878/5873], Loss: 2.8228\n",
      "Epoch [1/5], Step [879/5873], Loss: 4.5988\n",
      "Epoch [1/5], Step [880/5873], Loss: 3.5040\n",
      "Epoch [1/5], Step [881/5873], Loss: 2.2052\n",
      "Epoch [1/5], Step [882/5873], Loss: 4.2417\n",
      "Epoch [1/5], Step [883/5873], Loss: 2.7372\n",
      "Epoch [1/5], Step [884/5873], Loss: 2.7401\n",
      "Epoch [1/5], Step [885/5873], Loss: 4.9702\n",
      "Epoch [1/5], Step [886/5873], Loss: 3.1851\n",
      "Epoch [1/5], Step [887/5873], Loss: 5.6550\n",
      "Epoch [1/5], Step [888/5873], Loss: 5.1278\n",
      "Epoch [1/5], Step [889/5873], Loss: 5.2722\n",
      "Epoch [1/5], Step [890/5873], Loss: 2.7680\n",
      "Epoch [1/5], Step [891/5873], Loss: 5.9367\n",
      "Epoch [1/5], Step [892/5873], Loss: 3.0730\n",
      "Epoch [1/5], Step [893/5873], Loss: 2.5766\n",
      "Epoch [1/5], Step [894/5873], Loss: 4.1436\n",
      "Epoch [1/5], Step [895/5873], Loss: 5.5199\n",
      "Epoch [1/5], Step [896/5873], Loss: 4.3639\n",
      "Epoch [1/5], Step [897/5873], Loss: 3.6257\n",
      "Epoch [1/5], Step [898/5873], Loss: 4.2987\n",
      "Epoch [1/5], Step [899/5873], Loss: 4.4095\n",
      "Epoch [1/5], Step [900/5873], Loss: 4.7738\n",
      "Epoch [1/5], Step [901/5873], Loss: 4.8487\n",
      "Epoch [1/5], Step [902/5873], Loss: 4.2079\n",
      "Epoch [1/5], Step [903/5873], Loss: 5.1828\n",
      "Epoch [1/5], Step [904/5873], Loss: 4.5013\n",
      "Epoch [1/5], Step [905/5873], Loss: 3.5264\n",
      "Epoch [1/5], Step [906/5873], Loss: 4.9848\n",
      "Epoch [1/5], Step [907/5873], Loss: 4.1341\n",
      "Epoch [1/5], Step [908/5873], Loss: 4.6319\n",
      "Epoch [1/5], Step [909/5873], Loss: 3.1715\n",
      "Epoch [1/5], Step [910/5873], Loss: 3.5377\n",
      "Epoch [1/5], Step [911/5873], Loss: 3.9953\n",
      "Epoch [1/5], Step [912/5873], Loss: 4.2758\n",
      "Epoch [1/5], Step [913/5873], Loss: 3.8819\n",
      "Epoch [1/5], Step [914/5873], Loss: 2.8179\n",
      "Epoch [1/5], Step [915/5873], Loss: 2.7368\n",
      "Epoch [1/5], Step [916/5873], Loss: 5.1252\n",
      "Epoch [1/5], Step [917/5873], Loss: 2.3933\n",
      "Epoch [1/5], Step [918/5873], Loss: 1.2455\n",
      "Epoch [1/5], Step [919/5873], Loss: 3.5841\n",
      "Epoch [1/5], Step [920/5873], Loss: 3.6408\n",
      "Epoch [1/5], Step [921/5873], Loss: 2.5660\n",
      "Epoch [1/5], Step [922/5873], Loss: 4.3286\n",
      "Epoch [1/5], Step [923/5873], Loss: 5.4882\n",
      "Epoch [1/5], Step [924/5873], Loss: 5.5170\n",
      "Epoch [1/5], Step [925/5873], Loss: 6.0879\n",
      "Epoch [1/5], Step [926/5873], Loss: 5.1565\n",
      "Epoch [1/5], Step [927/5873], Loss: 4.9995\n",
      "Epoch [1/5], Step [928/5873], Loss: 3.9516\n",
      "Epoch [1/5], Step [929/5873], Loss: 3.0129\n",
      "Epoch [1/5], Step [930/5873], Loss: 3.2314\n",
      "Epoch [1/5], Step [931/5873], Loss: 3.0495\n",
      "Epoch [1/5], Step [932/5873], Loss: 6.1198\n",
      "Epoch [1/5], Step [933/5873], Loss: 3.3509\n",
      "Epoch [1/5], Step [934/5873], Loss: 4.6205\n",
      "Epoch [1/5], Step [935/5873], Loss: 3.5462\n",
      "Epoch [1/5], Step [936/5873], Loss: 5.0129\n",
      "Epoch [1/5], Step [937/5873], Loss: 4.8532\n",
      "Epoch [1/5], Step [938/5873], Loss: 4.6447\n",
      "Epoch [1/5], Step [939/5873], Loss: 4.2239\n",
      "Epoch [1/5], Step [940/5873], Loss: 4.4690\n",
      "Epoch [1/5], Step [941/5873], Loss: 4.7753\n",
      "Epoch [1/5], Step [942/5873], Loss: 5.2065\n",
      "Epoch [1/5], Step [943/5873], Loss: 5.4646\n",
      "Epoch [1/5], Step [944/5873], Loss: 3.5741\n",
      "Epoch [1/5], Step [945/5873], Loss: 4.5777\n",
      "Epoch [1/5], Step [946/5873], Loss: 4.3504\n",
      "Epoch [1/5], Step [947/5873], Loss: 5.0145\n",
      "Epoch [1/5], Step [948/5873], Loss: 5.0526\n",
      "Epoch [1/5], Step [949/5873], Loss: 4.0714\n",
      "Epoch [1/5], Step [950/5873], Loss: 4.1313\n",
      "Epoch [1/5], Step [951/5873], Loss: 3.3711\n",
      "Epoch [1/5], Step [952/5873], Loss: 4.9691\n",
      "Epoch [1/5], Step [953/5873], Loss: 3.9801\n",
      "Epoch [1/5], Step [954/5873], Loss: 4.3876\n",
      "Epoch [1/5], Step [955/5873], Loss: 4.2045\n",
      "Epoch [1/5], Step [956/5873], Loss: 3.0321\n",
      "Epoch [1/5], Step [957/5873], Loss: 5.7771\n",
      "Epoch [1/5], Step [958/5873], Loss: 5.9031\n",
      "Epoch [1/5], Step [959/5873], Loss: 3.2519\n",
      "Epoch [1/5], Step [960/5873], Loss: 3.9596\n",
      "Epoch [1/5], Step [961/5873], Loss: 4.9990\n",
      "Epoch [1/5], Step [962/5873], Loss: 4.8354\n",
      "Epoch [1/5], Step [963/5873], Loss: 3.6627\n",
      "Epoch [1/5], Step [964/5873], Loss: 3.9614\n",
      "Epoch [1/5], Step [965/5873], Loss: 4.8142\n",
      "Epoch [1/5], Step [966/5873], Loss: 5.0770\n",
      "Epoch [1/5], Step [967/5873], Loss: 4.1801\n",
      "Epoch [1/5], Step [968/5873], Loss: 4.2822\n",
      "Epoch [1/5], Step [969/5873], Loss: 3.9030\n",
      "Epoch [1/5], Step [970/5873], Loss: 3.6678\n",
      "Epoch [1/5], Step [971/5873], Loss: 3.5307\n",
      "Epoch [1/5], Step [972/5873], Loss: 5.0071\n",
      "Epoch [1/5], Step [973/5873], Loss: 3.9163\n",
      "Epoch [1/5], Step [974/5873], Loss: 5.6782\n",
      "Epoch [1/5], Step [975/5873], Loss: 4.6268\n",
      "Epoch [1/5], Step [976/5873], Loss: 5.0499\n",
      "Epoch [1/5], Step [977/5873], Loss: 4.8272\n",
      "Epoch [1/5], Step [978/5873], Loss: 3.4290\n",
      "Epoch [1/5], Step [979/5873], Loss: 4.0452\n",
      "Epoch [1/5], Step [980/5873], Loss: 4.6195\n",
      "Epoch [1/5], Step [981/5873], Loss: 4.9055\n",
      "Epoch [1/5], Step [982/5873], Loss: 5.2043\n",
      "Epoch [1/5], Step [983/5873], Loss: 4.8864\n",
      "Epoch [1/5], Step [984/5873], Loss: 3.6072\n",
      "Epoch [1/5], Step [985/5873], Loss: 4.6268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [986/5873], Loss: 4.5368\n",
      "Epoch [1/5], Step [987/5873], Loss: 2.4499\n",
      "Epoch [1/5], Step [988/5873], Loss: 5.7866\n",
      "Epoch [1/5], Step [989/5873], Loss: 4.5727\n",
      "Epoch [1/5], Step [990/5873], Loss: 4.4125\n",
      "Epoch [1/5], Step [991/5873], Loss: 5.6593\n",
      "Epoch [1/5], Step [992/5873], Loss: 4.2175\n",
      "Epoch [1/5], Step [993/5873], Loss: 3.7702\n",
      "Epoch [1/5], Step [994/5873], Loss: 4.5352\n",
      "Epoch [1/5], Step [995/5873], Loss: 4.0696\n",
      "Epoch [1/5], Step [996/5873], Loss: 2.4004\n",
      "Epoch [1/5], Step [997/5873], Loss: 5.5307\n",
      "Epoch [1/5], Step [998/5873], Loss: 4.1382\n",
      "Epoch [1/5], Step [999/5873], Loss: 3.2235\n",
      "Epoch [1/5], Step [1000/5873], Loss: 4.3209\n",
      "Epoch [1/5], Step [1001/5873], Loss: 5.0177\n",
      "Epoch [1/5], Step [1002/5873], Loss: 4.3164\n",
      "Epoch [1/5], Step [1003/5873], Loss: 5.2924\n",
      "Epoch [1/5], Step [1004/5873], Loss: 4.3331\n",
      "Epoch [1/5], Step [1005/5873], Loss: 5.0953\n",
      "Epoch [1/5], Step [1006/5873], Loss: 5.0259\n",
      "Epoch [1/5], Step [1007/5873], Loss: 3.4085\n",
      "Epoch [1/5], Step [1008/5873], Loss: 4.8905\n",
      "Epoch [1/5], Step [1009/5873], Loss: 3.8190\n",
      "Epoch [1/5], Step [1010/5873], Loss: 5.7650\n",
      "Epoch [1/5], Step [1011/5873], Loss: 4.0237\n",
      "Epoch [1/5], Step [1012/5873], Loss: 4.4363\n",
      "Epoch [1/5], Step [1013/5873], Loss: 2.8234\n",
      "Epoch [1/5], Step [1014/5873], Loss: 5.0003\n",
      "Epoch [1/5], Step [1015/5873], Loss: 5.7879\n",
      "Epoch [1/5], Step [1016/5873], Loss: 4.9203\n",
      "Epoch [1/5], Step [1017/5873], Loss: 3.7482\n",
      "Epoch [1/5], Step [1018/5873], Loss: 3.7769\n",
      "Epoch [1/5], Step [1019/5873], Loss: 4.3686\n",
      "Epoch [1/5], Step [1020/5873], Loss: 3.8113\n",
      "Epoch [1/5], Step [1021/5873], Loss: 5.5775\n",
      "Epoch [1/5], Step [1022/5873], Loss: 5.0013\n",
      "Epoch [1/5], Step [1023/5873], Loss: 4.2017\n",
      "Epoch [1/5], Step [1024/5873], Loss: 5.9921\n",
      "Epoch [1/5], Step [1025/5873], Loss: 4.4812\n",
      "Epoch [1/5], Step [1026/5873], Loss: 3.8147\n",
      "Epoch [1/5], Step [1027/5873], Loss: 3.5892\n",
      "Epoch [1/5], Step [1028/5873], Loss: 5.1459\n",
      "Epoch [1/5], Step [1029/5873], Loss: 3.8801\n",
      "Epoch [1/5], Step [1030/5873], Loss: 4.3040\n",
      "Epoch [1/5], Step [1031/5873], Loss: 3.9798\n",
      "Epoch [1/5], Step [1032/5873], Loss: 4.0101\n",
      "Epoch [1/5], Step [1033/5873], Loss: 3.9103\n",
      "Epoch [1/5], Step [1034/5873], Loss: 2.8507\n",
      "Epoch [1/5], Step [1035/5873], Loss: 4.1241\n",
      "Epoch [1/5], Step [1036/5873], Loss: 2.9425\n",
      "Epoch [1/5], Step [1037/5873], Loss: 5.1852\n",
      "Epoch [1/5], Step [1038/5873], Loss: 4.3200\n",
      "Epoch [1/5], Step [1039/5873], Loss: 3.8999\n",
      "Epoch [1/5], Step [1040/5873], Loss: 6.2960\n",
      "Epoch [1/5], Step [1041/5873], Loss: 4.8266\n",
      "Epoch [1/5], Step [1042/5873], Loss: 3.7880\n",
      "Epoch [1/5], Step [1043/5873], Loss: 2.0995\n",
      "Epoch [1/5], Step [1044/5873], Loss: 4.1553\n",
      "Epoch [1/5], Step [1045/5873], Loss: 3.1477\n",
      "Epoch [1/5], Step [1046/5873], Loss: 2.4724\n",
      "Epoch [1/5], Step [1047/5873], Loss: 3.9499\n",
      "Epoch [1/5], Step [1048/5873], Loss: 3.6187\n",
      "Epoch [1/5], Step [1049/5873], Loss: 6.0947\n",
      "Epoch [1/5], Step [1050/5873], Loss: 3.0803\n",
      "Epoch [1/5], Step [1051/5873], Loss: 5.3421\n",
      "Epoch [1/5], Step [1052/5873], Loss: 3.6868\n",
      "Epoch [1/5], Step [1053/5873], Loss: 4.8693\n",
      "Epoch [1/5], Step [1054/5873], Loss: 3.7638\n",
      "Epoch [1/5], Step [1055/5873], Loss: 4.7798\n",
      "Epoch [1/5], Step [1056/5873], Loss: 3.3134\n",
      "Epoch [1/5], Step [1057/5873], Loss: 4.4470\n",
      "Epoch [1/5], Step [1058/5873], Loss: 5.9560\n",
      "Epoch [1/5], Step [1059/5873], Loss: 4.2291\n",
      "Epoch [1/5], Step [1060/5873], Loss: 3.3959\n",
      "Epoch [1/5], Step [1061/5873], Loss: 5.9442\n",
      "Epoch [1/5], Step [1062/5873], Loss: 4.4552\n",
      "Epoch [1/5], Step [1063/5873], Loss: 5.1832\n",
      "Epoch [1/5], Step [1064/5873], Loss: 3.5766\n",
      "Epoch [1/5], Step [1065/5873], Loss: 4.2398\n",
      "Epoch [1/5], Step [1066/5873], Loss: 2.7153\n",
      "Epoch [1/5], Step [1067/5873], Loss: 2.4975\n",
      "Epoch [1/5], Step [1068/5873], Loss: 5.0496\n",
      "Epoch [1/5], Step [1069/5873], Loss: 4.4383\n",
      "Epoch [1/5], Step [1070/5873], Loss: 3.8496\n",
      "Epoch [1/5], Step [1071/5873], Loss: 2.1024\n",
      "Epoch [1/5], Step [1072/5873], Loss: 4.5579\n",
      "Epoch [1/5], Step [1073/5873], Loss: 4.8305\n",
      "Epoch [1/5], Step [1074/5873], Loss: 4.8170\n",
      "Epoch [1/5], Step [1075/5873], Loss: 5.7900\n",
      "Epoch [1/5], Step [1076/5873], Loss: 4.2838\n",
      "Epoch [1/5], Step [1077/5873], Loss: 3.5640\n",
      "Epoch [1/5], Step [1078/5873], Loss: 3.6866\n",
      "Epoch [1/5], Step [1079/5873], Loss: 4.5717\n",
      "Epoch [1/5], Step [1080/5873], Loss: 6.0505\n",
      "Epoch [1/5], Step [1081/5873], Loss: 5.7632\n",
      "Epoch [1/5], Step [1082/5873], Loss: 2.9303\n",
      "Epoch [1/5], Step [1083/5873], Loss: 3.5319\n",
      "Epoch [1/5], Step [1084/5873], Loss: 4.0929\n",
      "Epoch [1/5], Step [1085/5873], Loss: 4.4408\n",
      "Epoch [1/5], Step [1086/5873], Loss: 2.7696\n",
      "Epoch [1/5], Step [1087/5873], Loss: 3.2118\n",
      "Epoch [1/5], Step [1088/5873], Loss: 4.0273\n",
      "Epoch [1/5], Step [1089/5873], Loss: 3.0466\n",
      "Epoch [1/5], Step [1090/5873], Loss: 3.4705\n",
      "Epoch [1/5], Step [1091/5873], Loss: 4.8314\n",
      "Epoch [1/5], Step [1092/5873], Loss: 5.8230\n",
      "Epoch [1/5], Step [1093/5873], Loss: 3.1641\n",
      "Epoch [1/5], Step [1094/5873], Loss: 3.7513\n",
      "Epoch [1/5], Step [1095/5873], Loss: 4.0838\n",
      "Epoch [1/5], Step [1096/5873], Loss: 5.3272\n",
      "Epoch [1/5], Step [1097/5873], Loss: 4.2152\n",
      "Epoch [1/5], Step [1098/5873], Loss: 2.8511\n",
      "Epoch [1/5], Step [1099/5873], Loss: 3.1745\n",
      "Epoch [1/5], Step [1100/5873], Loss: 2.7657\n",
      "Epoch [1/5], Step [1101/5873], Loss: 5.9640\n",
      "Epoch [1/5], Step [1102/5873], Loss: 1.9991\n",
      "Epoch [1/5], Step [1103/5873], Loss: 5.8554\n",
      "Epoch [1/5], Step [1104/5873], Loss: 5.1469\n",
      "Epoch [1/5], Step [1105/5873], Loss: 4.4289\n",
      "Epoch [1/5], Step [1106/5873], Loss: 6.0263\n",
      "Epoch [1/5], Step [1107/5873], Loss: 4.7632\n",
      "Epoch [1/5], Step [1108/5873], Loss: 5.2751\n",
      "Epoch [1/5], Step [1109/5873], Loss: 4.2559\n",
      "Epoch [1/5], Step [1110/5873], Loss: 4.5604\n",
      "Epoch [1/5], Step [1111/5873], Loss: 4.6224\n",
      "Epoch [1/5], Step [1112/5873], Loss: 4.6041\n",
      "Epoch [1/5], Step [1113/5873], Loss: 5.5776\n",
      "Epoch [1/5], Step [1114/5873], Loss: 4.5248\n",
      "Epoch [1/5], Step [1115/5873], Loss: 3.4380\n",
      "Epoch [1/5], Step [1116/5873], Loss: 4.1982\n",
      "Epoch [1/5], Step [1117/5873], Loss: 3.4425\n",
      "Epoch [1/5], Step [1118/5873], Loss: 4.0368\n",
      "Epoch [1/5], Step [1119/5873], Loss: 4.3070\n",
      "Epoch [1/5], Step [1120/5873], Loss: 4.0077\n",
      "Epoch [1/5], Step [1121/5873], Loss: 4.0492\n",
      "Epoch [1/5], Step [1122/5873], Loss: 4.0295\n",
      "Epoch [1/5], Step [1123/5873], Loss: 3.0984\n",
      "Epoch [1/5], Step [1124/5873], Loss: 4.3535\n",
      "Epoch [1/5], Step [1125/5873], Loss: 3.8338\n",
      "Epoch [1/5], Step [1126/5873], Loss: 4.9884\n",
      "Epoch [1/5], Step [1127/5873], Loss: 5.1549\n",
      "Epoch [1/5], Step [1128/5873], Loss: 2.0328\n",
      "Epoch [1/5], Step [1129/5873], Loss: 5.1964\n",
      "Epoch [1/5], Step [1130/5873], Loss: 5.9026\n",
      "Epoch [1/5], Step [1131/5873], Loss: 2.4076\n",
      "Epoch [1/5], Step [1132/5873], Loss: 3.4903\n",
      "Epoch [1/5], Step [1133/5873], Loss: 5.5591\n",
      "Epoch [1/5], Step [1134/5873], Loss: 4.0665\n",
      "Epoch [1/5], Step [1135/5873], Loss: 4.8323\n",
      "Epoch [1/5], Step [1136/5873], Loss: 4.7994\n",
      "Epoch [1/5], Step [1137/5873], Loss: 4.1693\n",
      "Epoch [1/5], Step [1138/5873], Loss: 5.1569\n",
      "Epoch [1/5], Step [1139/5873], Loss: 2.4350\n",
      "Epoch [1/5], Step [1140/5873], Loss: 4.3134\n",
      "Epoch [1/5], Step [1141/5873], Loss: 4.6463\n",
      "Epoch [1/5], Step [1142/5873], Loss: 4.0275\n",
      "Epoch [1/5], Step [1143/5873], Loss: 4.7848\n",
      "Epoch [1/5], Step [1144/5873], Loss: 3.2378\n",
      "Epoch [1/5], Step [1145/5873], Loss: 4.2893\n",
      "Epoch [1/5], Step [1146/5873], Loss: 4.7095\n",
      "Epoch [1/5], Step [1147/5873], Loss: 3.2985\n",
      "Epoch [1/5], Step [1148/5873], Loss: 4.3559\n",
      "Epoch [1/5], Step [1149/5873], Loss: 3.6545\n",
      "Epoch [1/5], Step [1150/5873], Loss: 3.6452\n",
      "Epoch [1/5], Step [1151/5873], Loss: 5.4333\n",
      "Epoch [1/5], Step [1152/5873], Loss: 6.3193\n",
      "Epoch [1/5], Step [1153/5873], Loss: 3.6709\n",
      "Epoch [1/5], Step [1154/5873], Loss: 5.1617\n",
      "Epoch [1/5], Step [1155/5873], Loss: 5.8921\n",
      "Epoch [1/5], Step [1156/5873], Loss: 4.8403\n",
      "Epoch [1/5], Step [1157/5873], Loss: 5.3085\n",
      "Epoch [1/5], Step [1158/5873], Loss: 5.7983\n",
      "Epoch [1/5], Step [1159/5873], Loss: 4.4086\n",
      "Epoch [1/5], Step [1160/5873], Loss: 4.8270\n",
      "Epoch [1/5], Step [1161/5873], Loss: 5.4267\n",
      "Epoch [1/5], Step [1162/5873], Loss: 4.2718\n",
      "Epoch [1/5], Step [1163/5873], Loss: 3.4979\n",
      "Epoch [1/5], Step [1164/5873], Loss: 5.5601\n",
      "Epoch [1/5], Step [1165/5873], Loss: 4.0550\n",
      "Epoch [1/5], Step [1166/5873], Loss: 4.5904\n",
      "Epoch [1/5], Step [1167/5873], Loss: 3.2954\n",
      "Epoch [1/5], Step [1168/5873], Loss: 3.7585\n",
      "Epoch [1/5], Step [1169/5873], Loss: 5.2391\n",
      "Epoch [1/5], Step [1170/5873], Loss: 2.4207\n",
      "Epoch [1/5], Step [1171/5873], Loss: 2.0818\n",
      "Epoch [1/5], Step [1172/5873], Loss: 4.5673\n",
      "Epoch [1/5], Step [1173/5873], Loss: 4.3496\n",
      "Epoch [1/5], Step [1174/5873], Loss: 5.1011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1175/5873], Loss: 5.1238\n",
      "Epoch [1/5], Step [1176/5873], Loss: 5.8245\n",
      "Epoch [1/5], Step [1177/5873], Loss: 3.9674\n",
      "Epoch [1/5], Step [1178/5873], Loss: 4.8975\n",
      "Epoch [1/5], Step [1179/5873], Loss: 2.4424\n",
      "Epoch [1/5], Step [1180/5873], Loss: 4.7826\n",
      "Epoch [1/5], Step [1181/5873], Loss: 4.5101\n",
      "Epoch [1/5], Step [1182/5873], Loss: 4.3273\n",
      "Epoch [1/5], Step [1183/5873], Loss: 1.9553\n",
      "Epoch [1/5], Step [1184/5873], Loss: 5.7344\n",
      "Epoch [1/5], Step [1185/5873], Loss: 2.7224\n",
      "Epoch [1/5], Step [1186/5873], Loss: 5.2991\n",
      "Epoch [1/5], Step [1187/5873], Loss: 1.3147\n",
      "Epoch [1/5], Step [1188/5873], Loss: 5.0381\n",
      "Epoch [1/5], Step [1189/5873], Loss: 3.2235\n",
      "Epoch [1/5], Step [1190/5873], Loss: 5.1576\n",
      "Epoch [1/5], Step [1191/5873], Loss: 4.4159\n",
      "Epoch [1/5], Step [1192/5873], Loss: 2.8413\n",
      "Epoch [1/5], Step [1193/5873], Loss: 3.9753\n",
      "Epoch [1/5], Step [1194/5873], Loss: 4.3892\n",
      "Epoch [1/5], Step [1195/5873], Loss: 4.6574\n",
      "Epoch [1/5], Step [1196/5873], Loss: 2.9940\n",
      "Epoch [1/5], Step [1197/5873], Loss: 4.7119\n",
      "Epoch [1/5], Step [1198/5873], Loss: 5.7211\n",
      "Epoch [1/5], Step [1199/5873], Loss: 4.1861\n",
      "Epoch [1/5], Step [1200/5873], Loss: 5.6619\n",
      "Epoch [1/5], Step [1201/5873], Loss: 5.1356\n",
      "Epoch [1/5], Step [1202/5873], Loss: 3.4928\n",
      "Epoch [1/5], Step [1203/5873], Loss: 2.6890\n",
      "Epoch [1/5], Step [1204/5873], Loss: 2.5135\n",
      "Epoch [1/5], Step [1205/5873], Loss: 5.2194\n",
      "Epoch [1/5], Step [1206/5873], Loss: 5.3000\n",
      "Epoch [1/5], Step [1207/5873], Loss: 6.8063\n",
      "Epoch [1/5], Step [1208/5873], Loss: 4.4948\n",
      "Epoch [1/5], Step [1209/5873], Loss: 5.7529\n",
      "Epoch [1/5], Step [1210/5873], Loss: 4.4251\n",
      "Epoch [1/5], Step [1211/5873], Loss: 5.1593\n",
      "Epoch [1/5], Step [1212/5873], Loss: 4.7506\n",
      "Epoch [1/5], Step [1213/5873], Loss: 4.4577\n",
      "Epoch [1/5], Step [1214/5873], Loss: 4.7709\n",
      "Epoch [1/5], Step [1215/5873], Loss: 5.4579\n",
      "Epoch [1/5], Step [1216/5873], Loss: 4.4346\n",
      "Epoch [1/5], Step [1217/5873], Loss: 4.1836\n",
      "Epoch [1/5], Step [1218/5873], Loss: 4.6495\n",
      "Epoch [1/5], Step [1219/5873], Loss: 4.0179\n",
      "Epoch [1/5], Step [1220/5873], Loss: 3.9669\n",
      "Epoch [1/5], Step [1221/5873], Loss: 4.8161\n",
      "Epoch [1/5], Step [1222/5873], Loss: 3.0704\n",
      "Epoch [1/5], Step [1223/5873], Loss: 4.8564\n",
      "Epoch [1/5], Step [1224/5873], Loss: 5.6034\n",
      "Epoch [1/5], Step [1225/5873], Loss: 3.0386\n",
      "Epoch [1/5], Step [1226/5873], Loss: 2.5658\n",
      "Epoch [1/5], Step [1227/5873], Loss: 5.4973\n",
      "Epoch [1/5], Step [1228/5873], Loss: 4.8755\n",
      "Epoch [1/5], Step [1229/5873], Loss: 4.0613\n",
      "Epoch [1/5], Step [1230/5873], Loss: 4.8584\n",
      "Epoch [1/5], Step [1231/5873], Loss: 4.5821\n",
      "Epoch [1/5], Step [1232/5873], Loss: 5.3664\n",
      "Epoch [1/5], Step [1233/5873], Loss: 5.0446\n",
      "Epoch [1/5], Step [1234/5873], Loss: 4.0797\n",
      "Epoch [1/5], Step [1235/5873], Loss: 4.2950\n",
      "Epoch [1/5], Step [1236/5873], Loss: 3.6220\n",
      "Epoch [1/5], Step [1237/5873], Loss: 2.5842\n",
      "Epoch [1/5], Step [1238/5873], Loss: 3.0931\n",
      "Epoch [1/5], Step [1239/5873], Loss: 5.0010\n",
      "Epoch [1/5], Step [1240/5873], Loss: 3.9512\n",
      "Epoch [1/5], Step [1241/5873], Loss: 2.3445\n",
      "Epoch [1/5], Step [1242/5873], Loss: 2.5990\n",
      "Epoch [1/5], Step [1243/5873], Loss: 4.3278\n",
      "Epoch [1/5], Step [1244/5873], Loss: 4.7159\n",
      "Epoch [1/5], Step [1245/5873], Loss: 2.4429\n",
      "Epoch [1/5], Step [1246/5873], Loss: 5.4974\n",
      "Epoch [1/5], Step [1247/5873], Loss: 3.6940\n",
      "Epoch [1/5], Step [1248/5873], Loss: 3.7503\n",
      "Epoch [1/5], Step [1249/5873], Loss: 5.0583\n",
      "Epoch [1/5], Step [1250/5873], Loss: 4.1752\n",
      "Epoch [1/5], Step [1251/5873], Loss: 4.2819\n",
      "Epoch [1/5], Step [1252/5873], Loss: 5.4039\n",
      "Epoch [1/5], Step [1253/5873], Loss: 5.3138\n",
      "Epoch [1/5], Step [1254/5873], Loss: 4.6343\n",
      "Epoch [1/5], Step [1255/5873], Loss: 2.4694\n",
      "Epoch [1/5], Step [1256/5873], Loss: 3.6259\n",
      "Epoch [1/5], Step [1257/5873], Loss: 3.4715\n",
      "Epoch [1/5], Step [1258/5873], Loss: 5.1681\n",
      "Epoch [1/5], Step [1259/5873], Loss: 4.8820\n",
      "Epoch [1/5], Step [1260/5873], Loss: 4.8733\n",
      "Epoch [1/5], Step [1261/5873], Loss: 4.4663\n",
      "Epoch [1/5], Step [1262/5873], Loss: 3.7466\n",
      "Epoch [1/5], Step [1263/5873], Loss: 5.0569\n",
      "Epoch [1/5], Step [1264/5873], Loss: 1.8432\n",
      "Epoch [1/5], Step [1265/5873], Loss: 4.7348\n",
      "Epoch [1/5], Step [1266/5873], Loss: 2.3135\n",
      "Epoch [1/5], Step [1267/5873], Loss: 3.9989\n",
      "Epoch [1/5], Step [1268/5873], Loss: 4.5888\n",
      "Epoch [1/5], Step [1269/5873], Loss: 3.7346\n",
      "Epoch [1/5], Step [1270/5873], Loss: 5.6359\n",
      "Epoch [1/5], Step [1271/5873], Loss: 3.7986\n",
      "Epoch [1/5], Step [1272/5873], Loss: 4.8320\n",
      "Epoch [1/5], Step [1273/5873], Loss: 4.4107\n",
      "Epoch [1/5], Step [1274/5873], Loss: 4.4426\n",
      "Epoch [1/5], Step [1275/5873], Loss: 1.7062\n",
      "Epoch [1/5], Step [1276/5873], Loss: 3.7032\n",
      "Epoch [1/5], Step [1277/5873], Loss: 5.2574\n",
      "Epoch [1/5], Step [1278/5873], Loss: 4.8460\n",
      "Epoch [1/5], Step [1279/5873], Loss: 5.3002\n",
      "Epoch [1/5], Step [1280/5873], Loss: 3.5249\n",
      "Epoch [1/5], Step [1281/5873], Loss: 4.1981\n",
      "Epoch [1/5], Step [1282/5873], Loss: 5.2635\n",
      "Epoch [1/5], Step [1283/5873], Loss: 5.2097\n",
      "Epoch [1/5], Step [1284/5873], Loss: 4.9160\n",
      "Epoch [1/5], Step [1285/5873], Loss: 4.6285\n",
      "Epoch [1/5], Step [1286/5873], Loss: 5.1917\n",
      "Epoch [1/5], Step [1287/5873], Loss: 4.4859\n",
      "Epoch [1/5], Step [1288/5873], Loss: 5.7496\n",
      "Epoch [1/5], Step [1289/5873], Loss: 4.1875\n",
      "Epoch [1/5], Step [1290/5873], Loss: 3.4424\n",
      "Epoch [1/5], Step [1291/5873], Loss: 5.0376\n",
      "Epoch [1/5], Step [1292/5873], Loss: 3.8476\n",
      "Epoch [1/5], Step [1293/5873], Loss: 3.8072\n",
      "Epoch [1/5], Step [1294/5873], Loss: 1.8207\n",
      "Epoch [1/5], Step [1295/5873], Loss: 2.8721\n",
      "Epoch [1/5], Step [1296/5873], Loss: 4.5319\n",
      "Epoch [1/5], Step [1297/5873], Loss: 5.1824\n",
      "Epoch [1/5], Step [1298/5873], Loss: 5.4917\n",
      "Epoch [1/5], Step [1299/5873], Loss: 4.7539\n",
      "Epoch [1/5], Step [1300/5873], Loss: 2.3888\n",
      "Epoch [1/5], Step [1301/5873], Loss: 6.0218\n",
      "Epoch [1/5], Step [1302/5873], Loss: 4.7414\n",
      "Epoch [1/5], Step [1303/5873], Loss: 3.2493\n",
      "Epoch [1/5], Step [1304/5873], Loss: 4.9428\n",
      "Epoch [1/5], Step [1305/5873], Loss: 5.2263\n",
      "Epoch [1/5], Step [1306/5873], Loss: 2.9617\n",
      "Epoch [1/5], Step [1307/5873], Loss: 5.8518\n",
      "Epoch [1/5], Step [1308/5873], Loss: 2.4250\n",
      "Epoch [1/5], Step [1309/5873], Loss: 4.5348\n",
      "Epoch [1/5], Step [1310/5873], Loss: 4.1652\n",
      "Epoch [1/5], Step [1311/5873], Loss: 4.5588\n",
      "Epoch [1/5], Step [1312/5873], Loss: 4.0329\n",
      "Epoch [1/5], Step [1313/5873], Loss: 5.7520\n",
      "Epoch [1/5], Step [1314/5873], Loss: 3.9986\n",
      "Epoch [1/5], Step [1315/5873], Loss: 4.5545\n",
      "Epoch [1/5], Step [1316/5873], Loss: 3.5512\n",
      "Epoch [1/5], Step [1317/5873], Loss: 3.8594\n",
      "Epoch [1/5], Step [1318/5873], Loss: 4.7204\n",
      "Epoch [1/5], Step [1319/5873], Loss: 5.4298\n",
      "Epoch [1/5], Step [1320/5873], Loss: 3.9154\n",
      "Epoch [1/5], Step [1321/5873], Loss: 1.5945\n",
      "Epoch [1/5], Step [1322/5873], Loss: 3.6729\n",
      "Epoch [1/5], Step [1323/5873], Loss: 4.4284\n",
      "Epoch [1/5], Step [1324/5873], Loss: 4.9598\n",
      "Epoch [1/5], Step [1325/5873], Loss: 2.6152\n",
      "Epoch [1/5], Step [1326/5873], Loss: 4.0161\n",
      "Epoch [1/5], Step [1327/5873], Loss: 3.5847\n",
      "Epoch [1/5], Step [1328/5873], Loss: 5.0802\n",
      "Epoch [1/5], Step [1329/5873], Loss: 4.7917\n",
      "Epoch [1/5], Step [1330/5873], Loss: 6.0081\n",
      "Epoch [1/5], Step [1331/5873], Loss: 2.3965\n",
      "Epoch [1/5], Step [1332/5873], Loss: 4.3571\n",
      "Epoch [1/5], Step [1333/5873], Loss: 4.4381\n",
      "Epoch [1/5], Step [1334/5873], Loss: 4.9046\n",
      "Epoch [1/5], Step [1335/5873], Loss: 5.3813\n",
      "Epoch [1/5], Step [1336/5873], Loss: 3.9225\n",
      "Epoch [1/5], Step [1337/5873], Loss: 4.3825\n",
      "Epoch [1/5], Step [1338/5873], Loss: 3.8180\n",
      "Epoch [1/5], Step [1339/5873], Loss: 3.3713\n",
      "Epoch [1/5], Step [1340/5873], Loss: 3.8743\n",
      "Epoch [1/5], Step [1341/5873], Loss: 5.3189\n",
      "Epoch [1/5], Step [1342/5873], Loss: 4.1131\n",
      "Epoch [1/5], Step [1343/5873], Loss: 2.5170\n",
      "Epoch [1/5], Step [1344/5873], Loss: 5.7689\n",
      "Epoch [1/5], Step [1345/5873], Loss: 4.1111\n",
      "Epoch [1/5], Step [1346/5873], Loss: 4.7611\n",
      "Epoch [1/5], Step [1347/5873], Loss: 3.9951\n",
      "Epoch [1/5], Step [1348/5873], Loss: 5.5493\n",
      "Epoch [1/5], Step [1349/5873], Loss: 4.0737\n",
      "Epoch [1/5], Step [1350/5873], Loss: 2.7187\n",
      "Epoch [1/5], Step [1351/5873], Loss: 2.9959\n",
      "Epoch [1/5], Step [1352/5873], Loss: 3.4350\n",
      "Epoch [1/5], Step [1353/5873], Loss: 4.5454\n",
      "Epoch [1/5], Step [1354/5873], Loss: 5.2228\n",
      "Epoch [1/5], Step [1355/5873], Loss: 3.9656\n",
      "Epoch [1/5], Step [1356/5873], Loss: 3.4405\n",
      "Epoch [1/5], Step [1357/5873], Loss: 2.8362\n",
      "Epoch [1/5], Step [1358/5873], Loss: 5.2450\n",
      "Epoch [1/5], Step [1359/5873], Loss: 4.6706\n",
      "Epoch [1/5], Step [1360/5873], Loss: 3.8227\n",
      "Epoch [1/5], Step [1361/5873], Loss: 4.9858\n",
      "Epoch [1/5], Step [1362/5873], Loss: 4.8534\n",
      "Epoch [1/5], Step [1363/5873], Loss: 2.0680\n",
      "Epoch [1/5], Step [1364/5873], Loss: 3.8622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1365/5873], Loss: 4.4992\n",
      "Epoch [1/5], Step [1366/5873], Loss: 3.5476\n",
      "Epoch [1/5], Step [1367/5873], Loss: 4.6407\n",
      "Epoch [1/5], Step [1368/5873], Loss: 3.6324\n",
      "Epoch [1/5], Step [1369/5873], Loss: 5.6340\n",
      "Epoch [1/5], Step [1370/5873], Loss: 4.7088\n",
      "Epoch [1/5], Step [1371/5873], Loss: 3.4861\n",
      "Epoch [1/5], Step [1372/5873], Loss: 4.3390\n",
      "Epoch [1/5], Step [1373/5873], Loss: 5.1029\n",
      "Epoch [1/5], Step [1374/5873], Loss: 4.3343\n",
      "Epoch [1/5], Step [1375/5873], Loss: 4.5213\n",
      "Epoch [1/5], Step [1376/5873], Loss: 5.4274\n",
      "Epoch [1/5], Step [1377/5873], Loss: 2.5861\n",
      "Epoch [1/5], Step [1378/5873], Loss: 4.5329\n",
      "Epoch [1/5], Step [1379/5873], Loss: 4.3729\n",
      "Epoch [1/5], Step [1380/5873], Loss: 3.0075\n",
      "Epoch [1/5], Step [1381/5873], Loss: 3.0568\n",
      "Epoch [1/5], Step [1382/5873], Loss: 3.8683\n",
      "Epoch [1/5], Step [1383/5873], Loss: 4.1651\n",
      "Epoch [1/5], Step [1384/5873], Loss: 3.8237\n",
      "Epoch [1/5], Step [1385/5873], Loss: 4.6664\n",
      "Epoch [1/5], Step [1386/5873], Loss: 4.7903\n",
      "Epoch [1/5], Step [1387/5873], Loss: 4.0626\n",
      "Epoch [1/5], Step [1388/5873], Loss: 5.1155\n",
      "Epoch [1/5], Step [1389/5873], Loss: 4.2106\n",
      "Epoch [1/5], Step [1390/5873], Loss: 1.5525\n",
      "Epoch [1/5], Step [1391/5873], Loss: 4.7839\n",
      "Epoch [1/5], Step [1392/5873], Loss: 4.7170\n",
      "Epoch [1/5], Step [1393/5873], Loss: 4.3970\n",
      "Epoch [1/5], Step [1394/5873], Loss: 3.7574\n",
      "Epoch [1/5], Step [1395/5873], Loss: 4.3232\n",
      "Epoch [1/5], Step [1396/5873], Loss: 1.8307\n",
      "Epoch [1/5], Step [1397/5873], Loss: 4.3646\n",
      "Epoch [1/5], Step [1398/5873], Loss: 4.4073\n",
      "Epoch [1/5], Step [1399/5873], Loss: 4.0668\n",
      "Epoch [1/5], Step [1400/5873], Loss: 1.9592\n",
      "Epoch [1/5], Step [1401/5873], Loss: 4.5345\n",
      "Epoch [1/5], Step [1402/5873], Loss: 4.7631\n",
      "Epoch [1/5], Step [1403/5873], Loss: 3.7141\n",
      "Epoch [1/5], Step [1404/5873], Loss: 4.3776\n",
      "Epoch [1/5], Step [1405/5873], Loss: 3.7251\n",
      "Epoch [1/5], Step [1406/5873], Loss: 3.8874\n",
      "Epoch [1/5], Step [1407/5873], Loss: 5.2658\n",
      "Epoch [1/5], Step [1408/5873], Loss: 5.1463\n",
      "Epoch [1/5], Step [1409/5873], Loss: 3.4162\n",
      "Epoch [1/5], Step [1410/5873], Loss: 3.7321\n",
      "Epoch [1/5], Step [1411/5873], Loss: 2.2815\n",
      "Epoch [1/5], Step [1412/5873], Loss: 5.3939\n",
      "Epoch [1/5], Step [1413/5873], Loss: 5.1390\n",
      "Epoch [1/5], Step [1414/5873], Loss: 4.0791\n",
      "Epoch [1/5], Step [1415/5873], Loss: 5.5832\n",
      "Epoch [1/5], Step [1416/5873], Loss: 5.2029\n",
      "Epoch [1/5], Step [1417/5873], Loss: 4.7907\n",
      "Epoch [1/5], Step [1418/5873], Loss: 4.8328\n",
      "Epoch [1/5], Step [1419/5873], Loss: 5.1182\n",
      "Epoch [1/5], Step [1420/5873], Loss: 4.7431\n",
      "Epoch [1/5], Step [1421/5873], Loss: 4.0089\n",
      "Epoch [1/5], Step [1422/5873], Loss: 4.8270\n",
      "Epoch [1/5], Step [1423/5873], Loss: 4.9486\n",
      "Epoch [1/5], Step [1424/5873], Loss: 4.5710\n",
      "Epoch [1/5], Step [1425/5873], Loss: 4.2289\n",
      "Epoch [1/5], Step [1426/5873], Loss: 3.7890\n",
      "Epoch [1/5], Step [1427/5873], Loss: 4.3262\n",
      "Epoch [1/5], Step [1428/5873], Loss: 5.4521\n",
      "Epoch [1/5], Step [1429/5873], Loss: 4.3012\n",
      "Epoch [1/5], Step [1430/5873], Loss: 5.7750\n",
      "Epoch [1/5], Step [1431/5873], Loss: 4.3362\n",
      "Epoch [1/5], Step [1432/5873], Loss: 4.2154\n",
      "Epoch [1/5], Step [1433/5873], Loss: 4.9254\n",
      "Epoch [1/5], Step [1434/5873], Loss: 4.8423\n",
      "Epoch [1/5], Step [1435/5873], Loss: 4.3193\n",
      "Epoch [1/5], Step [1436/5873], Loss: 4.9290\n",
      "Epoch [1/5], Step [1437/5873], Loss: 4.8967\n",
      "Epoch [1/5], Step [1438/5873], Loss: 3.9497\n",
      "Epoch [1/5], Step [1439/5873], Loss: 4.8977\n",
      "Epoch [1/5], Step [1440/5873], Loss: 5.1305\n",
      "Epoch [1/5], Step [1441/5873], Loss: 3.1828\n",
      "Epoch [1/5], Step [1442/5873], Loss: 4.2839\n",
      "Epoch [1/5], Step [1443/5873], Loss: 4.9995\n",
      "Epoch [1/5], Step [1444/5873], Loss: 5.1600\n",
      "Epoch [1/5], Step [1445/5873], Loss: 4.6232\n",
      "Epoch [1/5], Step [1446/5873], Loss: 3.8646\n",
      "Epoch [1/5], Step [1447/5873], Loss: 4.4833\n",
      "Epoch [1/5], Step [1448/5873], Loss: 4.6560\n",
      "Epoch [1/5], Step [1449/5873], Loss: 4.0806\n",
      "Epoch [1/5], Step [1450/5873], Loss: 3.9698\n",
      "Epoch [1/5], Step [1451/5873], Loss: 3.6805\n",
      "Epoch [1/5], Step [1452/5873], Loss: 4.3147\n",
      "Epoch [1/5], Step [1453/5873], Loss: 4.3725\n",
      "Epoch [1/5], Step [1454/5873], Loss: 3.1609\n",
      "Epoch [1/5], Step [1455/5873], Loss: 2.4219\n",
      "Epoch [1/5], Step [1456/5873], Loss: 4.4417\n",
      "Epoch [1/5], Step [1457/5873], Loss: 5.6608\n",
      "Epoch [1/5], Step [1458/5873], Loss: 4.3844\n",
      "Epoch [1/5], Step [1459/5873], Loss: 2.7088\n",
      "Epoch [1/5], Step [1460/5873], Loss: 3.7705\n",
      "Epoch [1/5], Step [1461/5873], Loss: 4.0682\n",
      "Epoch [1/5], Step [1462/5873], Loss: 3.2775\n",
      "Epoch [1/5], Step [1463/5873], Loss: 5.6586\n",
      "Epoch [1/5], Step [1464/5873], Loss: 5.6703\n",
      "Epoch [1/5], Step [1465/5873], Loss: 5.3219\n",
      "Epoch [1/5], Step [1466/5873], Loss: 5.0909\n",
      "Epoch [1/5], Step [1467/5873], Loss: 3.8977\n",
      "Epoch [1/5], Step [1468/5873], Loss: 4.6184\n",
      "Epoch [1/5], Step [1469/5873], Loss: 5.3371\n",
      "Epoch [1/5], Step [1470/5873], Loss: 5.8290\n",
      "Epoch [1/5], Step [1471/5873], Loss: 1.8457\n",
      "Epoch [1/5], Step [1472/5873], Loss: 4.7321\n",
      "Epoch [1/5], Step [1473/5873], Loss: 3.5906\n",
      "Epoch [1/5], Step [1474/5873], Loss: 5.6051\n",
      "Epoch [1/5], Step [1475/5873], Loss: 4.6038\n",
      "Epoch [1/5], Step [1476/5873], Loss: 2.5474\n",
      "Epoch [1/5], Step [1477/5873], Loss: 3.4777\n",
      "Epoch [1/5], Step [1478/5873], Loss: 3.9390\n",
      "Epoch [1/5], Step [1479/5873], Loss: 2.6500\n",
      "Epoch [1/5], Step [1480/5873], Loss: 4.4936\n",
      "Epoch [1/5], Step [1481/5873], Loss: 3.2554\n",
      "Epoch [1/5], Step [1482/5873], Loss: 2.6920\n",
      "Epoch [1/5], Step [1483/5873], Loss: 4.3348\n",
      "Epoch [1/5], Step [1484/5873], Loss: 3.7635\n",
      "Epoch [1/5], Step [1485/5873], Loss: 3.4252\n",
      "Epoch [1/5], Step [1486/5873], Loss: 3.6369\n",
      "Epoch [1/5], Step [1487/5873], Loss: 3.0096\n",
      "Epoch [1/5], Step [1488/5873], Loss: 3.5362\n",
      "Epoch [1/5], Step [1489/5873], Loss: 4.7726\n",
      "Epoch [1/5], Step [1490/5873], Loss: 3.3989\n",
      "Epoch [1/5], Step [1491/5873], Loss: 4.3852\n",
      "Epoch [1/5], Step [1492/5873], Loss: 5.8848\n",
      "Epoch [1/5], Step [1493/5873], Loss: 4.5157\n",
      "Epoch [1/5], Step [1494/5873], Loss: 5.1338\n",
      "Epoch [1/5], Step [1495/5873], Loss: 4.5659\n",
      "Epoch [1/5], Step [1496/5873], Loss: 4.3477\n",
      "Epoch [1/5], Step [1497/5873], Loss: 4.5760\n",
      "Epoch [1/5], Step [1498/5873], Loss: 5.5544\n",
      "Epoch [1/5], Step [1499/5873], Loss: 4.8732\n",
      "Epoch [1/5], Step [1500/5873], Loss: 5.0050\n",
      "Epoch [1/5], Step [1501/5873], Loss: 3.4919\n",
      "Epoch [1/5], Step [1502/5873], Loss: 4.5118\n",
      "Epoch [1/5], Step [1503/5873], Loss: 4.5554\n",
      "Epoch [1/5], Step [1504/5873], Loss: 2.3560\n",
      "Epoch [1/5], Step [1505/5873], Loss: 3.9781\n",
      "Epoch [1/5], Step [1506/5873], Loss: 4.7872\n",
      "Epoch [1/5], Step [1507/5873], Loss: 3.1746\n",
      "Epoch [1/5], Step [1508/5873], Loss: 4.8108\n",
      "Epoch [1/5], Step [1509/5873], Loss: 3.8559\n",
      "Epoch [1/5], Step [1510/5873], Loss: 3.3689\n",
      "Epoch [1/5], Step [1511/5873], Loss: 5.4378\n",
      "Epoch [1/5], Step [1512/5873], Loss: 5.0334\n",
      "Epoch [1/5], Step [1513/5873], Loss: 3.1707\n",
      "Epoch [1/5], Step [1514/5873], Loss: 3.8466\n",
      "Epoch [1/5], Step [1515/5873], Loss: 4.7787\n",
      "Epoch [1/5], Step [1516/5873], Loss: 3.7653\n",
      "Epoch [1/5], Step [1517/5873], Loss: 3.7432\n",
      "Epoch [1/5], Step [1518/5873], Loss: 5.4140\n",
      "Epoch [1/5], Step [1519/5873], Loss: 2.3583\n",
      "Epoch [1/5], Step [1520/5873], Loss: 4.3886\n",
      "Epoch [1/5], Step [1521/5873], Loss: 4.4421\n",
      "Epoch [1/5], Step [1522/5873], Loss: 2.7139\n",
      "Epoch [1/5], Step [1523/5873], Loss: 3.5417\n",
      "Epoch [1/5], Step [1524/5873], Loss: 3.9756\n",
      "Epoch [1/5], Step [1525/5873], Loss: 4.0344\n",
      "Epoch [1/5], Step [1526/5873], Loss: 4.7395\n",
      "Epoch [1/5], Step [1527/5873], Loss: 4.1697\n",
      "Epoch [1/5], Step [1528/5873], Loss: 5.6265\n",
      "Epoch [1/5], Step [1529/5873], Loss: 5.2045\n",
      "Epoch [1/5], Step [1530/5873], Loss: 4.9395\n",
      "Epoch [1/5], Step [1531/5873], Loss: 3.2852\n",
      "Epoch [1/5], Step [1532/5873], Loss: 4.3743\n",
      "Epoch [1/5], Step [1533/5873], Loss: 3.6819\n",
      "Epoch [1/5], Step [1534/5873], Loss: 4.4447\n",
      "Epoch [1/5], Step [1535/5873], Loss: 1.8784\n",
      "Epoch [1/5], Step [1536/5873], Loss: 3.0679\n",
      "Epoch [1/5], Step [1537/5873], Loss: 3.5129\n",
      "Epoch [1/5], Step [1538/5873], Loss: 2.2659\n",
      "Epoch [1/5], Step [1539/5873], Loss: 3.2846\n",
      "Epoch [1/5], Step [1540/5873], Loss: 3.9677\n",
      "Epoch [1/5], Step [1541/5873], Loss: 4.6925\n",
      "Epoch [1/5], Step [1542/5873], Loss: 5.7051\n",
      "Epoch [1/5], Step [1543/5873], Loss: 3.5200\n",
      "Epoch [1/5], Step [1544/5873], Loss: 4.1555\n",
      "Epoch [1/5], Step [1545/5873], Loss: 4.3932\n",
      "Epoch [1/5], Step [1546/5873], Loss: 1.6848\n",
      "Epoch [1/5], Step [1547/5873], Loss: 3.4280\n",
      "Epoch [1/5], Step [1548/5873], Loss: 1.9102\n",
      "Epoch [1/5], Step [1549/5873], Loss: 5.2759\n",
      "Epoch [1/5], Step [1550/5873], Loss: 5.0642\n",
      "Epoch [1/5], Step [1551/5873], Loss: 4.9729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1552/5873], Loss: 4.2636\n",
      "Epoch [1/5], Step [1553/5873], Loss: 3.1989\n",
      "Epoch [1/5], Step [1554/5873], Loss: 5.4935\n",
      "Epoch [1/5], Step [1555/5873], Loss: 4.6563\n",
      "Epoch [1/5], Step [1556/5873], Loss: 1.9916\n",
      "Epoch [1/5], Step [1557/5873], Loss: 2.5140\n",
      "Epoch [1/5], Step [1558/5873], Loss: 7.2655\n",
      "Epoch [1/5], Step [1559/5873], Loss: 2.0895\n",
      "Epoch [1/5], Step [1560/5873], Loss: 3.0715\n",
      "Epoch [1/5], Step [1561/5873], Loss: 4.2309\n",
      "Epoch [1/5], Step [1562/5873], Loss: 3.6503\n",
      "Epoch [1/5], Step [1563/5873], Loss: 4.1215\n",
      "Epoch [1/5], Step [1564/5873], Loss: 5.1781\n",
      "Epoch [1/5], Step [1565/5873], Loss: 4.3631\n",
      "Epoch [1/5], Step [1566/5873], Loss: 3.2753\n",
      "Epoch [1/5], Step [1567/5873], Loss: 4.8653\n",
      "Epoch [1/5], Step [1568/5873], Loss: 5.1524\n",
      "Epoch [1/5], Step [1569/5873], Loss: 4.6102\n",
      "Epoch [1/5], Step [1570/5873], Loss: 4.2754\n",
      "Epoch [1/5], Step [1571/5873], Loss: 1.3841\n",
      "Epoch [1/5], Step [1572/5873], Loss: 5.3074\n",
      "Epoch [1/5], Step [1573/5873], Loss: 2.8373\n",
      "Epoch [1/5], Step [1574/5873], Loss: 4.3593\n",
      "Epoch [1/5], Step [1575/5873], Loss: 4.4477\n",
      "Epoch [1/5], Step [1576/5873], Loss: 4.3329\n",
      "Epoch [1/5], Step [1577/5873], Loss: 3.1635\n",
      "Epoch [1/5], Step [1578/5873], Loss: 4.1413\n",
      "Epoch [1/5], Step [1579/5873], Loss: 5.0633\n",
      "Epoch [1/5], Step [1580/5873], Loss: 6.0582\n",
      "Epoch [1/5], Step [1581/5873], Loss: 5.6024\n",
      "Epoch [1/5], Step [1582/5873], Loss: 3.0634\n",
      "Epoch [1/5], Step [1583/5873], Loss: 3.8045\n",
      "Epoch [1/5], Step [1584/5873], Loss: 4.6223\n",
      "Epoch [1/5], Step [1585/5873], Loss: 3.6301\n",
      "Epoch [1/5], Step [1586/5873], Loss: 4.6235\n",
      "Epoch [1/5], Step [1587/5873], Loss: 5.1354\n",
      "Epoch [1/5], Step [1588/5873], Loss: 3.2101\n",
      "Epoch [1/5], Step [1589/5873], Loss: 4.6535\n",
      "Epoch [1/5], Step [1590/5873], Loss: 5.1478\n",
      "Epoch [1/5], Step [1591/5873], Loss: 4.0026\n",
      "Epoch [1/5], Step [1592/5873], Loss: 5.5351\n",
      "Epoch [1/5], Step [1593/5873], Loss: 3.5608\n",
      "Epoch [1/5], Step [1594/5873], Loss: 4.7881\n",
      "Epoch [1/5], Step [1595/5873], Loss: 4.9845\n",
      "Epoch [1/5], Step [1596/5873], Loss: 3.2719\n",
      "Epoch [1/5], Step [1597/5873], Loss: 4.5930\n",
      "Epoch [1/5], Step [1598/5873], Loss: 4.9430\n",
      "Epoch [1/5], Step [1599/5873], Loss: 4.2090\n",
      "Epoch [1/5], Step [1600/5873], Loss: 5.4096\n",
      "Epoch [1/5], Step [1601/5873], Loss: 4.9462\n",
      "Epoch [1/5], Step [1602/5873], Loss: 3.9210\n",
      "Epoch [1/5], Step [1603/5873], Loss: 4.3509\n",
      "Epoch [1/5], Step [1604/5873], Loss: 3.4473\n",
      "Epoch [1/5], Step [1605/5873], Loss: 4.2020\n",
      "Epoch [1/5], Step [1606/5873], Loss: 4.5392\n",
      "Epoch [1/5], Step [1607/5873], Loss: 3.6924\n",
      "Epoch [1/5], Step [1608/5873], Loss: 3.8371\n",
      "Epoch [1/5], Step [1609/5873], Loss: 4.7917\n",
      "Epoch [1/5], Step [1610/5873], Loss: 5.1366\n",
      "Epoch [1/5], Step [1611/5873], Loss: 4.0496\n",
      "Epoch [1/5], Step [1612/5873], Loss: 4.7144\n",
      "Epoch [1/5], Step [1613/5873], Loss: 4.5511\n",
      "Epoch [1/5], Step [1614/5873], Loss: 5.5205\n",
      "Epoch [1/5], Step [1615/5873], Loss: 2.9691\n",
      "Epoch [1/5], Step [1616/5873], Loss: 4.5619\n",
      "Epoch [1/5], Step [1617/5873], Loss: 4.4051\n",
      "Epoch [1/5], Step [1618/5873], Loss: 4.1735\n",
      "Epoch [1/5], Step [1619/5873], Loss: 4.0562\n",
      "Epoch [1/5], Step [1620/5873], Loss: 5.0052\n",
      "Epoch [1/5], Step [1621/5873], Loss: 3.4685\n",
      "Epoch [1/5], Step [1622/5873], Loss: 5.0422\n",
      "Epoch [1/5], Step [1623/5873], Loss: 4.4305\n",
      "Epoch [1/5], Step [1624/5873], Loss: 5.1579\n",
      "Epoch [1/5], Step [1625/5873], Loss: 4.1766\n",
      "Epoch [1/5], Step [1626/5873], Loss: 3.7228\n",
      "Epoch [1/5], Step [1627/5873], Loss: 4.0590\n",
      "Epoch [1/5], Step [1628/5873], Loss: 4.0140\n",
      "Epoch [1/5], Step [1629/5873], Loss: 5.5598\n",
      "Epoch [1/5], Step [1630/5873], Loss: 3.3509\n",
      "Epoch [1/5], Step [1631/5873], Loss: 4.9950\n",
      "Epoch [1/5], Step [1632/5873], Loss: 5.8891\n",
      "Epoch [1/5], Step [1633/5873], Loss: 3.5472\n",
      "Epoch [1/5], Step [1634/5873], Loss: 4.0833\n",
      "Epoch [1/5], Step [1635/5873], Loss: 3.5212\n",
      "Epoch [1/5], Step [1636/5873], Loss: 4.3659\n",
      "Epoch [1/5], Step [1637/5873], Loss: 4.5119\n",
      "Epoch [1/5], Step [1638/5873], Loss: 4.2903\n",
      "Epoch [1/5], Step [1639/5873], Loss: 5.2494\n",
      "Epoch [1/5], Step [1640/5873], Loss: 1.5679\n",
      "Epoch [1/5], Step [1641/5873], Loss: 4.9127\n",
      "Epoch [1/5], Step [1642/5873], Loss: 4.7298\n",
      "Epoch [1/5], Step [1643/5873], Loss: 3.9541\n",
      "Epoch [1/5], Step [1644/5873], Loss: 4.9883\n",
      "Epoch [1/5], Step [1645/5873], Loss: 4.1525\n",
      "Epoch [1/5], Step [1646/5873], Loss: 2.1605\n",
      "Epoch [1/5], Step [1647/5873], Loss: 4.0159\n",
      "Epoch [1/5], Step [1648/5873], Loss: 3.1796\n",
      "Epoch [1/5], Step [1649/5873], Loss: 5.7217\n",
      "Epoch [1/5], Step [1650/5873], Loss: 3.6248\n",
      "Epoch [1/5], Step [1651/5873], Loss: 3.0542\n",
      "Epoch [1/5], Step [1652/5873], Loss: 3.4556\n",
      "Epoch [1/5], Step [1653/5873], Loss: 3.1560\n",
      "Epoch [1/5], Step [1654/5873], Loss: 4.6874\n",
      "Epoch [1/5], Step [1655/5873], Loss: 4.7946\n",
      "Epoch [1/5], Step [1656/5873], Loss: 4.2488\n",
      "Epoch [1/5], Step [1657/5873], Loss: 4.1016\n",
      "Epoch [1/5], Step [1658/5873], Loss: 3.3477\n",
      "Epoch [1/5], Step [1659/5873], Loss: 3.5190\n",
      "Epoch [1/5], Step [1660/5873], Loss: 4.4066\n",
      "Epoch [1/5], Step [1661/5873], Loss: 4.3689\n",
      "Epoch [1/5], Step [1662/5873], Loss: 4.0810\n",
      "Epoch [1/5], Step [1663/5873], Loss: 5.5976\n",
      "Epoch [1/5], Step [1664/5873], Loss: 5.0228\n",
      "Epoch [1/5], Step [1665/5873], Loss: 3.4225\n",
      "Epoch [1/5], Step [1666/5873], Loss: 3.7929\n",
      "Epoch [1/5], Step [1667/5873], Loss: 5.4580\n",
      "Epoch [1/5], Step [1668/5873], Loss: 3.3333\n",
      "Epoch [1/5], Step [1669/5873], Loss: 5.0286\n",
      "Epoch [1/5], Step [1670/5873], Loss: 2.8733\n",
      "Epoch [1/5], Step [1671/5873], Loss: 4.7410\n",
      "Epoch [1/5], Step [1672/5873], Loss: 3.8021\n",
      "Epoch [1/5], Step [1673/5873], Loss: 2.0110\n",
      "Epoch [1/5], Step [1674/5873], Loss: 3.7507\n",
      "Epoch [1/5], Step [1675/5873], Loss: 2.2675\n",
      "Epoch [1/5], Step [1676/5873], Loss: 3.4341\n",
      "Epoch [1/5], Step [1677/5873], Loss: 4.6440\n",
      "Epoch [1/5], Step [1678/5873], Loss: 4.1422\n",
      "Epoch [1/5], Step [1679/5873], Loss: 2.7565\n",
      "Epoch [1/5], Step [1680/5873], Loss: 5.3300\n",
      "Epoch [1/5], Step [1681/5873], Loss: 4.8149\n",
      "Epoch [1/5], Step [1682/5873], Loss: 4.0822\n",
      "Epoch [1/5], Step [1683/5873], Loss: 1.2944\n",
      "Epoch [1/5], Step [1684/5873], Loss: 5.0204\n",
      "Epoch [1/5], Step [1685/5873], Loss: 4.4557\n",
      "Epoch [1/5], Step [1686/5873], Loss: 5.3424\n",
      "Epoch [1/5], Step [1687/5873], Loss: 6.4467\n",
      "Epoch [1/5], Step [1688/5873], Loss: 4.0346\n",
      "Epoch [1/5], Step [1689/5873], Loss: 3.7447\n",
      "Epoch [1/5], Step [1690/5873], Loss: 2.2506\n",
      "Epoch [1/5], Step [1691/5873], Loss: 4.7837\n",
      "Epoch [1/5], Step [1692/5873], Loss: 4.8642\n",
      "Epoch [1/5], Step [1693/5873], Loss: 2.5378\n",
      "Epoch [1/5], Step [1694/5873], Loss: 3.9428\n",
      "Epoch [1/5], Step [1695/5873], Loss: 5.8120\n",
      "Epoch [1/5], Step [1696/5873], Loss: 3.6266\n",
      "Epoch [1/5], Step [1697/5873], Loss: 4.8623\n",
      "Epoch [1/5], Step [1698/5873], Loss: 4.2833\n",
      "Epoch [1/5], Step [1699/5873], Loss: 5.0559\n",
      "Epoch [1/5], Step [1700/5873], Loss: 5.2364\n",
      "Epoch [1/5], Step [1701/5873], Loss: 3.9638\n",
      "Epoch [1/5], Step [1702/5873], Loss: 2.6676\n",
      "Epoch [1/5], Step [1703/5873], Loss: 4.3087\n",
      "Epoch [1/5], Step [1704/5873], Loss: 4.6403\n",
      "Epoch [1/5], Step [1705/5873], Loss: 3.4592\n",
      "Epoch [1/5], Step [1706/5873], Loss: 4.0677\n",
      "Epoch [1/5], Step [1707/5873], Loss: 5.0226\n",
      "Epoch [1/5], Step [1708/5873], Loss: 5.6001\n",
      "Epoch [1/5], Step [1709/5873], Loss: 4.8314\n",
      "Epoch [1/5], Step [1710/5873], Loss: 4.9168\n",
      "Epoch [1/5], Step [1711/5873], Loss: 4.9003\n",
      "Epoch [1/5], Step [1712/5873], Loss: 4.5849\n",
      "Epoch [1/5], Step [1713/5873], Loss: 3.4677\n",
      "Epoch [1/5], Step [1714/5873], Loss: 4.9165\n",
      "Epoch [1/5], Step [1715/5873], Loss: 4.4023\n",
      "Epoch [1/5], Step [1716/5873], Loss: 3.8521\n",
      "Epoch [1/5], Step [1717/5873], Loss: 3.7212\n",
      "Epoch [1/5], Step [1718/5873], Loss: 4.9085\n",
      "Epoch [1/5], Step [1719/5873], Loss: 2.7331\n",
      "Epoch [1/5], Step [1720/5873], Loss: 4.6348\n",
      "Epoch [1/5], Step [1721/5873], Loss: 5.7003\n",
      "Epoch [1/5], Step [1722/5873], Loss: 4.0378\n",
      "Epoch [1/5], Step [1723/5873], Loss: 3.7627\n",
      "Epoch [1/5], Step [1724/5873], Loss: 3.9770\n",
      "Epoch [1/5], Step [1725/5873], Loss: 3.4647\n",
      "Epoch [1/5], Step [1726/5873], Loss: 5.4307\n",
      "Epoch [1/5], Step [1727/5873], Loss: 4.4325\n",
      "Epoch [1/5], Step [1728/5873], Loss: 5.7515\n",
      "Epoch [1/5], Step [1729/5873], Loss: 5.0018\n",
      "Epoch [1/5], Step [1730/5873], Loss: 3.4193\n",
      "Epoch [1/5], Step [1731/5873], Loss: 4.4191\n",
      "Epoch [1/5], Step [1732/5873], Loss: 4.5131\n",
      "Epoch [1/5], Step [1733/5873], Loss: 5.0996\n",
      "Epoch [1/5], Step [1734/5873], Loss: 4.6007\n",
      "Epoch [1/5], Step [1735/5873], Loss: 5.9020\n",
      "Epoch [1/5], Step [1736/5873], Loss: 4.7952\n",
      "Epoch [1/5], Step [1737/5873], Loss: 3.9558\n",
      "Epoch [1/5], Step [1738/5873], Loss: 3.7569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1739/5873], Loss: 4.9152\n",
      "Epoch [1/5], Step [1740/5873], Loss: 2.0373\n",
      "Epoch [1/5], Step [1741/5873], Loss: 3.1755\n",
      "Epoch [1/5], Step [1742/5873], Loss: 3.2562\n",
      "Epoch [1/5], Step [1743/5873], Loss: 3.6622\n",
      "Epoch [1/5], Step [1744/5873], Loss: 4.9087\n",
      "Epoch [1/5], Step [1745/5873], Loss: 4.4338\n",
      "Epoch [1/5], Step [1746/5873], Loss: 3.5089\n",
      "Epoch [1/5], Step [1747/5873], Loss: 4.0602\n",
      "Epoch [1/5], Step [1748/5873], Loss: 4.4225\n",
      "Epoch [1/5], Step [1749/5873], Loss: 5.1619\n",
      "Epoch [1/5], Step [1750/5873], Loss: 4.0224\n",
      "Epoch [1/5], Step [1751/5873], Loss: 4.8532\n",
      "Epoch [1/5], Step [1752/5873], Loss: 4.9890\n",
      "Epoch [1/5], Step [1753/5873], Loss: 3.8021\n",
      "Epoch [1/5], Step [1754/5873], Loss: 4.1661\n",
      "Epoch [1/5], Step [1755/5873], Loss: 5.0885\n",
      "Epoch [1/5], Step [1756/5873], Loss: 4.3274\n",
      "Epoch [1/5], Step [1757/5873], Loss: 3.4884\n",
      "Epoch [1/5], Step [1758/5873], Loss: 4.0256\n",
      "Epoch [1/5], Step [1759/5873], Loss: 4.5077\n",
      "Epoch [1/5], Step [1760/5873], Loss: 4.2299\n",
      "Epoch [1/5], Step [1761/5873], Loss: 3.3132\n",
      "Epoch [1/5], Step [1762/5873], Loss: 5.1613\n",
      "Epoch [1/5], Step [1763/5873], Loss: 5.9121\n",
      "Epoch [1/5], Step [1764/5873], Loss: 4.6382\n",
      "Epoch [1/5], Step [1765/5873], Loss: 5.2714\n",
      "Epoch [1/5], Step [1766/5873], Loss: 4.8767\n",
      "Epoch [1/5], Step [1767/5873], Loss: 3.1216\n",
      "Epoch [1/5], Step [1768/5873], Loss: 5.2354\n",
      "Epoch [1/5], Step [1769/5873], Loss: 3.8524\n",
      "Epoch [1/5], Step [1770/5873], Loss: 4.9611\n",
      "Epoch [1/5], Step [1771/5873], Loss: 5.0296\n",
      "Epoch [1/5], Step [1772/5873], Loss: 3.1658\n",
      "Epoch [1/5], Step [1773/5873], Loss: 4.3301\n",
      "Epoch [1/5], Step [1774/5873], Loss: 4.4535\n",
      "Epoch [1/5], Step [1775/5873], Loss: 5.0349\n",
      "Epoch [1/5], Step [1776/5873], Loss: 4.3775\n",
      "Epoch [1/5], Step [1777/5873], Loss: 3.4082\n",
      "Epoch [1/5], Step [1778/5873], Loss: 3.5951\n",
      "Epoch [1/5], Step [1779/5873], Loss: 2.9423\n",
      "Epoch [1/5], Step [1780/5873], Loss: 4.5510\n",
      "Epoch [1/5], Step [1781/5873], Loss: 4.0760\n",
      "Epoch [1/5], Step [1782/5873], Loss: 4.8982\n",
      "Epoch [1/5], Step [1783/5873], Loss: 5.3521\n",
      "Epoch [1/5], Step [1784/5873], Loss: 3.9935\n",
      "Epoch [1/5], Step [1785/5873], Loss: 4.4740\n",
      "Epoch [1/5], Step [1786/5873], Loss: 5.0982\n",
      "Epoch [1/5], Step [1787/5873], Loss: 3.0099\n",
      "Epoch [1/5], Step [1788/5873], Loss: 4.3999\n",
      "Epoch [1/5], Step [1789/5873], Loss: 4.1960\n",
      "Epoch [1/5], Step [1790/5873], Loss: 5.0320\n",
      "Epoch [1/5], Step [1791/5873], Loss: 5.8593\n",
      "Epoch [1/5], Step [1792/5873], Loss: 4.8053\n",
      "Epoch [1/5], Step [1793/5873], Loss: 5.5937\n",
      "Epoch [1/5], Step [1794/5873], Loss: 4.8332\n",
      "Epoch [1/5], Step [1795/5873], Loss: 3.3455\n",
      "Epoch [1/5], Step [1796/5873], Loss: 4.0106\n",
      "Epoch [1/5], Step [1797/5873], Loss: 3.5407\n",
      "Epoch [1/5], Step [1798/5873], Loss: 4.6264\n",
      "Epoch [1/5], Step [1799/5873], Loss: 4.2533\n",
      "Epoch [1/5], Step [1800/5873], Loss: 4.4013\n",
      "Epoch [1/5], Step [1801/5873], Loss: 4.4268\n",
      "Epoch [1/5], Step [1802/5873], Loss: 5.0603\n",
      "Epoch [1/5], Step [1803/5873], Loss: 3.3399\n",
      "Epoch [1/5], Step [1804/5873], Loss: 4.7919\n",
      "Epoch [1/5], Step [1805/5873], Loss: 4.7501\n",
      "Epoch [1/5], Step [1806/5873], Loss: 4.8950\n",
      "Epoch [1/5], Step [1807/5873], Loss: 4.3038\n",
      "Epoch [1/5], Step [1808/5873], Loss: 4.4534\n",
      "Epoch [1/5], Step [1809/5873], Loss: 5.2594\n",
      "Epoch [1/5], Step [1810/5873], Loss: 3.5151\n",
      "Epoch [1/5], Step [1811/5873], Loss: 4.7369\n",
      "Epoch [1/5], Step [1812/5873], Loss: 3.8664\n",
      "Epoch [1/5], Step [1813/5873], Loss: 5.0953\n",
      "Epoch [1/5], Step [1814/5873], Loss: 3.4465\n",
      "Epoch [1/5], Step [1815/5873], Loss: 3.9507\n",
      "Epoch [1/5], Step [1816/5873], Loss: 3.7272\n",
      "Epoch [1/5], Step [1817/5873], Loss: 3.6545\n",
      "Epoch [1/5], Step [1818/5873], Loss: 3.7304\n",
      "Epoch [1/5], Step [1819/5873], Loss: 2.9357\n",
      "Epoch [1/5], Step [1820/5873], Loss: 5.0088\n",
      "Epoch [1/5], Step [1821/5873], Loss: 3.1056\n",
      "Epoch [1/5], Step [1822/5873], Loss: 5.0020\n",
      "Epoch [1/5], Step [1823/5873], Loss: 5.0742\n",
      "Epoch [1/5], Step [1824/5873], Loss: 4.1262\n",
      "Epoch [1/5], Step [1825/5873], Loss: 3.3001\n",
      "Epoch [1/5], Step [1826/5873], Loss: 5.1612\n",
      "Epoch [1/5], Step [1827/5873], Loss: 5.8465\n",
      "Epoch [1/5], Step [1828/5873], Loss: 6.5501\n",
      "Epoch [1/5], Step [1829/5873], Loss: 5.6873\n",
      "Epoch [1/5], Step [1830/5873], Loss: 4.0623\n",
      "Epoch [1/5], Step [1831/5873], Loss: 4.4199\n",
      "Epoch [1/5], Step [1832/5873], Loss: 2.9378\n",
      "Epoch [1/5], Step [1833/5873], Loss: 5.2895\n",
      "Epoch [1/5], Step [1834/5873], Loss: 4.0532\n",
      "Epoch [1/5], Step [1835/5873], Loss: 3.6297\n",
      "Epoch [1/5], Step [1836/5873], Loss: 3.7115\n",
      "Epoch [1/5], Step [1837/5873], Loss: 3.6079\n",
      "Epoch [1/5], Step [1838/5873], Loss: 5.6184\n",
      "Epoch [1/5], Step [1839/5873], Loss: 4.3091\n",
      "Epoch [1/5], Step [1840/5873], Loss: 3.7439\n",
      "Epoch [1/5], Step [1841/5873], Loss: 3.5041\n",
      "Epoch [1/5], Step [1842/5873], Loss: 4.3058\n",
      "Epoch [1/5], Step [1843/5873], Loss: 4.5155\n",
      "Epoch [1/5], Step [1844/5873], Loss: 4.9450\n",
      "Epoch [1/5], Step [1845/5873], Loss: 3.8611\n",
      "Epoch [1/5], Step [1846/5873], Loss: 5.6177\n",
      "Epoch [1/5], Step [1847/5873], Loss: 4.2608\n",
      "Epoch [1/5], Step [1848/5873], Loss: 3.0447\n",
      "Epoch [1/5], Step [1849/5873], Loss: 5.6013\n",
      "Epoch [1/5], Step [1850/5873], Loss: 3.7604\n",
      "Epoch [1/5], Step [1851/5873], Loss: 3.9087\n",
      "Epoch [1/5], Step [1852/5873], Loss: 5.0961\n",
      "Epoch [1/5], Step [1853/5873], Loss: 3.8935\n",
      "Epoch [1/5], Step [1854/5873], Loss: 3.0913\n",
      "Epoch [1/5], Step [1855/5873], Loss: 4.7520\n",
      "Epoch [1/5], Step [1856/5873], Loss: 3.0211\n",
      "Epoch [1/5], Step [1857/5873], Loss: 5.0518\n",
      "Epoch [1/5], Step [1858/5873], Loss: 5.0742\n",
      "Epoch [1/5], Step [1859/5873], Loss: 5.7177\n",
      "Epoch [1/5], Step [1860/5873], Loss: 3.4667\n",
      "Epoch [1/5], Step [1861/5873], Loss: 4.1253\n",
      "Epoch [1/5], Step [1862/5873], Loss: 5.1057\n",
      "Epoch [1/5], Step [1863/5873], Loss: 5.3089\n",
      "Epoch [1/5], Step [1864/5873], Loss: 4.7342\n",
      "Epoch [1/5], Step [1865/5873], Loss: 1.6395\n",
      "Epoch [1/5], Step [1866/5873], Loss: 3.3317\n",
      "Epoch [1/5], Step [1867/5873], Loss: 1.8548\n",
      "Epoch [1/5], Step [1868/5873], Loss: 5.2444\n",
      "Epoch [1/5], Step [1869/5873], Loss: 4.1789\n",
      "Epoch [1/5], Step [1870/5873], Loss: 2.7340\n",
      "Epoch [1/5], Step [1871/5873], Loss: 5.8825\n",
      "Epoch [1/5], Step [1872/5873], Loss: 6.0761\n",
      "Epoch [1/5], Step [1873/5873], Loss: 4.4128\n",
      "Epoch [1/5], Step [1874/5873], Loss: 5.6931\n",
      "Epoch [1/5], Step [1875/5873], Loss: 5.1990\n",
      "Epoch [1/5], Step [1876/5873], Loss: 4.7340\n",
      "Epoch [1/5], Step [1877/5873], Loss: 4.4102\n",
      "Epoch [1/5], Step [1878/5873], Loss: 2.8563\n",
      "Epoch [1/5], Step [1879/5873], Loss: 2.9296\n",
      "Epoch [1/5], Step [1880/5873], Loss: 4.9780\n",
      "Epoch [1/5], Step [1881/5873], Loss: 5.0958\n",
      "Epoch [1/5], Step [1882/5873], Loss: 4.3692\n",
      "Epoch [1/5], Step [1883/5873], Loss: 2.8920\n",
      "Epoch [1/5], Step [1884/5873], Loss: 4.2580\n",
      "Epoch [1/5], Step [1885/5873], Loss: 3.4119\n",
      "Epoch [1/5], Step [1886/5873], Loss: 3.6040\n",
      "Epoch [1/5], Step [1887/5873], Loss: 4.5409\n",
      "Epoch [1/5], Step [1888/5873], Loss: 4.8788\n",
      "Epoch [1/5], Step [1889/5873], Loss: 3.2572\n",
      "Epoch [1/5], Step [1890/5873], Loss: 2.8827\n",
      "Epoch [1/5], Step [1891/5873], Loss: 2.7387\n",
      "Epoch [1/5], Step [1892/5873], Loss: 5.5805\n",
      "Epoch [1/5], Step [1893/5873], Loss: 4.9089\n",
      "Epoch [1/5], Step [1894/5873], Loss: 2.5561\n",
      "Epoch [1/5], Step [1895/5873], Loss: 4.2634\n",
      "Epoch [1/5], Step [1896/5873], Loss: 5.7709\n",
      "Epoch [1/5], Step [1897/5873], Loss: 4.3288\n",
      "Epoch [1/5], Step [1898/5873], Loss: 3.7747\n",
      "Epoch [1/5], Step [1899/5873], Loss: 4.1205\n",
      "Epoch [1/5], Step [1900/5873], Loss: 4.7016\n",
      "Epoch [1/5], Step [1901/5873], Loss: 4.7061\n",
      "Epoch [1/5], Step [1902/5873], Loss: 3.6763\n",
      "Epoch [1/5], Step [1903/5873], Loss: 3.6720\n",
      "Epoch [1/5], Step [1904/5873], Loss: 4.1386\n",
      "Epoch [1/5], Step [1905/5873], Loss: 5.7633\n",
      "Epoch [1/5], Step [1906/5873], Loss: 3.0076\n",
      "Epoch [1/5], Step [1907/5873], Loss: 4.6096\n",
      "Epoch [1/5], Step [1908/5873], Loss: 2.0672\n",
      "Epoch [1/5], Step [1909/5873], Loss: 3.9306\n",
      "Epoch [1/5], Step [1910/5873], Loss: 5.9765\n",
      "Epoch [1/5], Step [1911/5873], Loss: 2.5335\n",
      "Epoch [1/5], Step [1912/5873], Loss: 5.4327\n",
      "Epoch [1/5], Step [1913/5873], Loss: 5.1407\n",
      "Epoch [1/5], Step [1914/5873], Loss: 5.4952\n",
      "Epoch [1/5], Step [1915/5873], Loss: 6.1002\n",
      "Epoch [1/5], Step [1916/5873], Loss: 4.7030\n",
      "Epoch [1/5], Step [1917/5873], Loss: 4.3505\n",
      "Epoch [1/5], Step [1918/5873], Loss: 4.0523\n",
      "Epoch [1/5], Step [1919/5873], Loss: 4.9030\n",
      "Epoch [1/5], Step [1920/5873], Loss: 4.5520\n",
      "Epoch [1/5], Step [1921/5873], Loss: 5.1799\n",
      "Epoch [1/5], Step [1922/5873], Loss: 4.6110\n",
      "Epoch [1/5], Step [1923/5873], Loss: 4.3971\n",
      "Epoch [1/5], Step [1924/5873], Loss: 3.0204\n",
      "Epoch [1/5], Step [1925/5873], Loss: 4.6865\n",
      "Epoch [1/5], Step [1926/5873], Loss: 4.0489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1927/5873], Loss: 2.7717\n",
      "Epoch [1/5], Step [1928/5873], Loss: 4.4655\n",
      "Epoch [1/5], Step [1929/5873], Loss: 2.9371\n",
      "Epoch [1/5], Step [1930/5873], Loss: 5.8122\n",
      "Epoch [1/5], Step [1931/5873], Loss: 4.7545\n",
      "Epoch [1/5], Step [1932/5873], Loss: 3.1910\n",
      "Epoch [1/5], Step [1933/5873], Loss: 4.2531\n",
      "Epoch [1/5], Step [1934/5873], Loss: 2.3308\n",
      "Epoch [1/5], Step [1935/5873], Loss: 3.8454\n",
      "Epoch [1/5], Step [1936/5873], Loss: 3.5643\n",
      "Epoch [1/5], Step [1937/5873], Loss: 4.5476\n",
      "Epoch [1/5], Step [1938/5873], Loss: 1.9509\n",
      "Epoch [1/5], Step [1939/5873], Loss: 5.2296\n",
      "Epoch [1/5], Step [1940/5873], Loss: 3.9450\n",
      "Epoch [1/5], Step [1941/5873], Loss: 5.2126\n",
      "Epoch [1/5], Step [1942/5873], Loss: 3.6913\n",
      "Epoch [1/5], Step [1943/5873], Loss: 3.7940\n",
      "Epoch [1/5], Step [1944/5873], Loss: 4.6425\n",
      "Epoch [1/5], Step [1945/5873], Loss: 4.3945\n",
      "Epoch [1/5], Step [1946/5873], Loss: 4.4437\n",
      "Epoch [1/5], Step [1947/5873], Loss: 4.7402\n",
      "Epoch [1/5], Step [1948/5873], Loss: 5.6578\n",
      "Epoch [1/5], Step [1949/5873], Loss: 4.1339\n",
      "Epoch [1/5], Step [1950/5873], Loss: 3.9040\n",
      "Epoch [1/5], Step [1951/5873], Loss: 3.2884\n",
      "Epoch [1/5], Step [1952/5873], Loss: 4.1849\n",
      "Epoch [1/5], Step [1953/5873], Loss: 2.0338\n",
      "Epoch [1/5], Step [1954/5873], Loss: 4.5836\n",
      "Epoch [1/5], Step [1955/5873], Loss: 4.6962\n",
      "Epoch [1/5], Step [1956/5873], Loss: 2.2667\n",
      "Epoch [1/5], Step [1957/5873], Loss: 3.9272\n",
      "Epoch [1/5], Step [1958/5873], Loss: 5.1129\n",
      "Epoch [1/5], Step [1959/5873], Loss: 4.5618\n",
      "Epoch [1/5], Step [1960/5873], Loss: 4.2417\n",
      "Epoch [1/5], Step [1961/5873], Loss: 5.1829\n",
      "Epoch [1/5], Step [1962/5873], Loss: 4.4802\n",
      "Epoch [1/5], Step [1963/5873], Loss: 2.3584\n",
      "Epoch [1/5], Step [1964/5873], Loss: 4.2743\n",
      "Epoch [1/5], Step [1965/5873], Loss: 3.5952\n",
      "Epoch [1/5], Step [1966/5873], Loss: 4.5674\n",
      "Epoch [1/5], Step [1967/5873], Loss: 5.4096\n",
      "Epoch [1/5], Step [1968/5873], Loss: 3.6821\n",
      "Epoch [1/5], Step [1969/5873], Loss: 4.1402\n",
      "Epoch [1/5], Step [1970/5873], Loss: 4.7720\n",
      "Epoch [1/5], Step [1971/5873], Loss: 4.2936\n",
      "Epoch [1/5], Step [1972/5873], Loss: 3.6713\n",
      "Epoch [1/5], Step [1973/5873], Loss: 1.9529\n",
      "Epoch [1/5], Step [1974/5873], Loss: 4.6927\n",
      "Epoch [1/5], Step [1975/5873], Loss: 4.0869\n",
      "Epoch [1/5], Step [1976/5873], Loss: 5.0466\n",
      "Epoch [1/5], Step [1977/5873], Loss: 2.5376\n",
      "Epoch [1/5], Step [1978/5873], Loss: 2.2437\n",
      "Epoch [1/5], Step [1979/5873], Loss: 4.4594\n",
      "Epoch [1/5], Step [1980/5873], Loss: 5.0369\n",
      "Epoch [1/5], Step [1981/5873], Loss: 3.5888\n",
      "Epoch [1/5], Step [1982/5873], Loss: 3.1784\n",
      "Epoch [1/5], Step [1983/5873], Loss: 3.3012\n",
      "Epoch [1/5], Step [1984/5873], Loss: 4.2269\n",
      "Epoch [1/5], Step [1985/5873], Loss: 2.9983\n",
      "Epoch [1/5], Step [1986/5873], Loss: 3.7511\n",
      "Epoch [1/5], Step [1987/5873], Loss: 1.3747\n",
      "Epoch [1/5], Step [1988/5873], Loss: 2.9864\n",
      "Epoch [1/5], Step [1989/5873], Loss: 4.0229\n",
      "Epoch [1/5], Step [1990/5873], Loss: 5.5395\n",
      "Epoch [1/5], Step [1991/5873], Loss: 5.5533\n",
      "Epoch [1/5], Step [1992/5873], Loss: 5.7117\n",
      "Epoch [1/5], Step [1993/5873], Loss: 5.3503\n",
      "Epoch [1/5], Step [1994/5873], Loss: 3.6284\n",
      "Epoch [1/5], Step [1995/5873], Loss: 4.8079\n",
      "Epoch [1/5], Step [1996/5873], Loss: 3.0922\n",
      "Epoch [1/5], Step [1997/5873], Loss: 3.7603\n",
      "Epoch [1/5], Step [1998/5873], Loss: 5.3104\n",
      "Epoch [1/5], Step [1999/5873], Loss: 5.4131\n",
      "Epoch [1/5], Step [2000/5873], Loss: 4.0777\n",
      "Epoch [1/5], Step [2001/5873], Loss: 1.7628\n",
      "Epoch [1/5], Step [2002/5873], Loss: 4.5074\n",
      "Epoch [1/5], Step [2003/5873], Loss: 5.0597\n",
      "Epoch [1/5], Step [2004/5873], Loss: 3.2660\n",
      "Epoch [1/5], Step [2005/5873], Loss: 5.6951\n",
      "Epoch [1/5], Step [2006/5873], Loss: 5.0812\n",
      "Epoch [1/5], Step [2007/5873], Loss: 3.7753\n",
      "Epoch [1/5], Step [2008/5873], Loss: 4.6447\n",
      "Epoch [1/5], Step [2009/5873], Loss: 4.5601\n",
      "Epoch [1/5], Step [2010/5873], Loss: 3.2801\n",
      "Epoch [1/5], Step [2011/5873], Loss: 5.7782\n",
      "Epoch [1/5], Step [2012/5873], Loss: 2.6001\n",
      "Epoch [1/5], Step [2013/5873], Loss: 4.3220\n",
      "Epoch [1/5], Step [2014/5873], Loss: 2.2244\n",
      "Epoch [1/5], Step [2015/5873], Loss: 2.2911\n",
      "Epoch [1/5], Step [2016/5873], Loss: 3.7764\n",
      "Epoch [1/5], Step [2017/5873], Loss: 4.6669\n",
      "Epoch [1/5], Step [2018/5873], Loss: 4.5305\n",
      "Epoch [1/5], Step [2019/5873], Loss: 2.6693\n",
      "Epoch [1/5], Step [2020/5873], Loss: 4.0303\n",
      "Epoch [1/5], Step [2021/5873], Loss: 4.4828\n",
      "Epoch [1/5], Step [2022/5873], Loss: 4.8258\n",
      "Epoch [1/5], Step [2023/5873], Loss: 4.4606\n",
      "Epoch [1/5], Step [2024/5873], Loss: 1.8005\n",
      "Epoch [1/5], Step [2025/5873], Loss: 5.2859\n",
      "Epoch [1/5], Step [2026/5873], Loss: 5.0644\n",
      "Epoch [1/5], Step [2027/5873], Loss: 3.7506\n",
      "Epoch [1/5], Step [2028/5873], Loss: 3.7167\n",
      "Epoch [1/5], Step [2029/5873], Loss: 6.3623\n",
      "Epoch [1/5], Step [2030/5873], Loss: 2.9518\n",
      "Epoch [1/5], Step [2031/5873], Loss: 4.0372\n",
      "Epoch [1/5], Step [2032/5873], Loss: 4.8224\n",
      "Epoch [1/5], Step [2033/5873], Loss: 4.3974\n",
      "Epoch [1/5], Step [2034/5873], Loss: 4.7055\n",
      "Epoch [1/5], Step [2035/5873], Loss: 3.5121\n",
      "Epoch [1/5], Step [2036/5873], Loss: 3.1076\n",
      "Epoch [1/5], Step [2037/5873], Loss: 5.5211\n",
      "Epoch [1/5], Step [2038/5873], Loss: 3.2685\n",
      "Epoch [1/5], Step [2039/5873], Loss: 4.6126\n",
      "Epoch [1/5], Step [2040/5873], Loss: 4.9599\n",
      "Epoch [1/5], Step [2041/5873], Loss: 4.2635\n",
      "Epoch [1/5], Step [2042/5873], Loss: 2.9933\n",
      "Epoch [1/5], Step [2043/5873], Loss: 5.1429\n",
      "Epoch [1/5], Step [2044/5873], Loss: 4.1712\n",
      "Epoch [1/5], Step [2045/5873], Loss: 3.6382\n",
      "Epoch [1/5], Step [2046/5873], Loss: 3.2940\n",
      "Epoch [1/5], Step [2047/5873], Loss: 5.6855\n",
      "Epoch [1/5], Step [2048/5873], Loss: 4.3723\n",
      "Epoch [1/5], Step [2049/5873], Loss: 4.6579\n",
      "Epoch [1/5], Step [2050/5873], Loss: 3.0554\n",
      "Epoch [1/5], Step [2051/5873], Loss: 4.6654\n",
      "Epoch [1/5], Step [2052/5873], Loss: 4.5998\n",
      "Epoch [1/5], Step [2053/5873], Loss: 4.0378\n",
      "Epoch [1/5], Step [2054/5873], Loss: 1.8456\n",
      "Epoch [1/5], Step [2055/5873], Loss: 6.1145\n",
      "Epoch [1/5], Step [2056/5873], Loss: 1.8134\n",
      "Epoch [1/5], Step [2057/5873], Loss: 2.5468\n",
      "Epoch [1/5], Step [2058/5873], Loss: 4.8479\n",
      "Epoch [1/5], Step [2059/5873], Loss: 3.4725\n",
      "Epoch [1/5], Step [2060/5873], Loss: 1.2540\n",
      "Epoch [1/5], Step [2061/5873], Loss: 5.1893\n",
      "Epoch [1/5], Step [2062/5873], Loss: 5.2112\n",
      "Epoch [1/5], Step [2063/5873], Loss: 1.2199\n",
      "Epoch [1/5], Step [2064/5873], Loss: 3.6843\n",
      "Epoch [1/5], Step [2065/5873], Loss: 3.1398\n",
      "Epoch [1/5], Step [2066/5873], Loss: 5.5077\n",
      "Epoch [1/5], Step [2067/5873], Loss: 4.3134\n",
      "Epoch [1/5], Step [2068/5873], Loss: 3.6417\n",
      "Epoch [1/5], Step [2069/5873], Loss: 5.0250\n",
      "Epoch [1/5], Step [2070/5873], Loss: 5.0394\n",
      "Epoch [1/5], Step [2071/5873], Loss: 4.5504\n",
      "Epoch [1/5], Step [2072/5873], Loss: 4.6834\n",
      "Epoch [1/5], Step [2073/5873], Loss: 4.0126\n",
      "Epoch [1/5], Step [2074/5873], Loss: 5.1170\n",
      "Epoch [1/5], Step [2075/5873], Loss: 0.9749\n",
      "Epoch [1/5], Step [2076/5873], Loss: 4.0952\n",
      "Epoch [1/5], Step [2077/5873], Loss: 5.4080\n",
      "Epoch [1/5], Step [2078/5873], Loss: 4.8235\n",
      "Epoch [1/5], Step [2079/5873], Loss: 2.5910\n",
      "Epoch [1/5], Step [2080/5873], Loss: 3.5831\n",
      "Epoch [1/5], Step [2081/5873], Loss: 4.3100\n",
      "Epoch [1/5], Step [2082/5873], Loss: 4.3403\n",
      "Epoch [1/5], Step [2083/5873], Loss: 6.0815\n",
      "Epoch [1/5], Step [2084/5873], Loss: 3.5300\n",
      "Epoch [1/5], Step [2085/5873], Loss: 5.2120\n",
      "Epoch [1/5], Step [2086/5873], Loss: 4.4125\n",
      "Epoch [1/5], Step [2087/5873], Loss: 4.1062\n",
      "Epoch [1/5], Step [2088/5873], Loss: 1.1804\n",
      "Epoch [1/5], Step [2089/5873], Loss: 1.8536\n",
      "Epoch [1/5], Step [2090/5873], Loss: 5.0899\n",
      "Epoch [1/5], Step [2091/5873], Loss: 4.6277\n",
      "Epoch [1/5], Step [2092/5873], Loss: 4.6649\n",
      "Epoch [1/5], Step [2093/5873], Loss: 5.2742\n",
      "Epoch [1/5], Step [2094/5873], Loss: 6.2275\n",
      "Epoch [1/5], Step [2095/5873], Loss: 3.6959\n",
      "Epoch [1/5], Step [2096/5873], Loss: 4.3665\n",
      "Epoch [1/5], Step [2097/5873], Loss: 2.3608\n",
      "Epoch [1/5], Step [2098/5873], Loss: 5.3153\n",
      "Epoch [1/5], Step [2099/5873], Loss: 4.1017\n",
      "Epoch [1/5], Step [2100/5873], Loss: 3.2258\n",
      "Epoch [1/5], Step [2101/5873], Loss: 3.9414\n",
      "Epoch [1/5], Step [2102/5873], Loss: 3.8431\n",
      "Epoch [1/5], Step [2103/5873], Loss: 3.7211\n",
      "Epoch [1/5], Step [2104/5873], Loss: 4.2936\n",
      "Epoch [1/5], Step [2105/5873], Loss: 5.8350\n",
      "Epoch [1/5], Step [2106/5873], Loss: 4.8494\n",
      "Epoch [1/5], Step [2107/5873], Loss: 3.4745\n",
      "Epoch [1/5], Step [2108/5873], Loss: 5.3713\n",
      "Epoch [1/5], Step [2109/5873], Loss: 3.4088\n",
      "Epoch [1/5], Step [2110/5873], Loss: 5.9134\n",
      "Epoch [1/5], Step [2111/5873], Loss: 3.4643\n",
      "Epoch [1/5], Step [2112/5873], Loss: 4.9934\n",
      "Epoch [1/5], Step [2113/5873], Loss: 4.9162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2114/5873], Loss: 3.7316\n",
      "Epoch [1/5], Step [2115/5873], Loss: 2.8230\n",
      "Epoch [1/5], Step [2116/5873], Loss: 4.1735\n",
      "Epoch [1/5], Step [2117/5873], Loss: 5.4077\n",
      "Epoch [1/5], Step [2118/5873], Loss: 5.1655\n",
      "Epoch [1/5], Step [2119/5873], Loss: 4.4453\n",
      "Epoch [1/5], Step [2120/5873], Loss: 2.4654\n",
      "Epoch [1/5], Step [2121/5873], Loss: 3.6880\n",
      "Epoch [1/5], Step [2122/5873], Loss: 3.5801\n",
      "Epoch [1/5], Step [2123/5873], Loss: 5.3988\n",
      "Epoch [1/5], Step [2124/5873], Loss: 4.9801\n",
      "Epoch [1/5], Step [2125/5873], Loss: 3.0647\n",
      "Epoch [1/5], Step [2126/5873], Loss: 4.8368\n",
      "Epoch [1/5], Step [2127/5873], Loss: 4.4066\n",
      "Epoch [1/5], Step [2128/5873], Loss: 5.7258\n",
      "Epoch [1/5], Step [2129/5873], Loss: 3.5226\n",
      "Epoch [1/5], Step [2130/5873], Loss: 2.6857\n",
      "Epoch [1/5], Step [2131/5873], Loss: 3.4664\n",
      "Epoch [1/5], Step [2132/5873], Loss: 3.8264\n",
      "Epoch [1/5], Step [2133/5873], Loss: 4.2789\n",
      "Epoch [1/5], Step [2134/5873], Loss: 5.3435\n",
      "Epoch [1/5], Step [2135/5873], Loss: 3.6020\n",
      "Epoch [1/5], Step [2136/5873], Loss: 3.0949\n",
      "Epoch [1/5], Step [2137/5873], Loss: 3.2889\n",
      "Epoch [1/5], Step [2138/5873], Loss: 4.6185\n",
      "Epoch [1/5], Step [2139/5873], Loss: 6.0330\n",
      "Epoch [1/5], Step [2140/5873], Loss: 3.6070\n",
      "Epoch [1/5], Step [2141/5873], Loss: 5.5765\n",
      "Epoch [1/5], Step [2142/5873], Loss: 5.0887\n",
      "Epoch [1/5], Step [2143/5873], Loss: 4.8195\n",
      "Epoch [1/5], Step [2144/5873], Loss: 3.2056\n",
      "Epoch [1/5], Step [2145/5873], Loss: 4.0211\n",
      "Epoch [1/5], Step [2146/5873], Loss: 4.1058\n",
      "Epoch [1/5], Step [2147/5873], Loss: 5.7148\n",
      "Epoch [1/5], Step [2148/5873], Loss: 4.3318\n",
      "Epoch [1/5], Step [2149/5873], Loss: 5.3877\n",
      "Epoch [1/5], Step [2150/5873], Loss: 4.0504\n",
      "Epoch [1/5], Step [2151/5873], Loss: 4.1543\n",
      "Epoch [1/5], Step [2152/5873], Loss: 4.4770\n",
      "Epoch [1/5], Step [2153/5873], Loss: 3.4970\n",
      "Epoch [1/5], Step [2154/5873], Loss: 4.2877\n",
      "Epoch [1/5], Step [2155/5873], Loss: 3.6341\n",
      "Epoch [1/5], Step [2156/5873], Loss: 4.6099\n",
      "Epoch [1/5], Step [2157/5873], Loss: 4.8474\n",
      "Epoch [1/5], Step [2158/5873], Loss: 3.7223\n",
      "Epoch [1/5], Step [2159/5873], Loss: 4.6656\n",
      "Epoch [1/5], Step [2160/5873], Loss: 3.2763\n",
      "Epoch [1/5], Step [2161/5873], Loss: 5.4880\n",
      "Epoch [1/5], Step [2162/5873], Loss: 1.8732\n",
      "Epoch [1/5], Step [2163/5873], Loss: 4.1520\n",
      "Epoch [1/5], Step [2164/5873], Loss: 4.5416\n",
      "Epoch [1/5], Step [2165/5873], Loss: 4.0026\n",
      "Epoch [1/5], Step [2166/5873], Loss: 3.5559\n",
      "Epoch [1/5], Step [2167/5873], Loss: 5.8384\n",
      "Epoch [1/5], Step [2168/5873], Loss: 4.8571\n",
      "Epoch [1/5], Step [2169/5873], Loss: 4.1961\n",
      "Epoch [1/5], Step [2170/5873], Loss: 1.4613\n",
      "Epoch [1/5], Step [2171/5873], Loss: 4.0000\n",
      "Epoch [1/5], Step [2172/5873], Loss: 4.2386\n",
      "Epoch [1/5], Step [2173/5873], Loss: 6.6135\n",
      "Epoch [1/5], Step [2174/5873], Loss: 5.1373\n",
      "Epoch [1/5], Step [2175/5873], Loss: 4.2770\n",
      "Epoch [1/5], Step [2176/5873], Loss: 5.0332\n",
      "Epoch [1/5], Step [2177/5873], Loss: 4.0306\n",
      "Epoch [1/5], Step [2178/5873], Loss: 5.6859\n",
      "Epoch [1/5], Step [2179/5873], Loss: 4.8453\n",
      "Epoch [1/5], Step [2180/5873], Loss: 2.8985\n",
      "Epoch [1/5], Step [2181/5873], Loss: 3.6191\n",
      "Epoch [1/5], Step [2182/5873], Loss: 5.5293\n",
      "Epoch [1/5], Step [2183/5873], Loss: 3.9735\n",
      "Epoch [1/5], Step [2184/5873], Loss: 4.7159\n",
      "Epoch [1/5], Step [2185/5873], Loss: 1.6588\n",
      "Epoch [1/5], Step [2186/5873], Loss: 4.7857\n",
      "Epoch [1/5], Step [2187/5873], Loss: 4.8529\n",
      "Epoch [1/5], Step [2188/5873], Loss: 4.6438\n",
      "Epoch [1/5], Step [2189/5873], Loss: 4.6176\n",
      "Epoch [1/5], Step [2190/5873], Loss: 3.9992\n",
      "Epoch [1/5], Step [2191/5873], Loss: 3.8882\n",
      "Epoch [1/5], Step [2192/5873], Loss: 5.6044\n",
      "Epoch [1/5], Step [2193/5873], Loss: 4.2349\n",
      "Epoch [1/5], Step [2194/5873], Loss: 3.1981\n",
      "Epoch [1/5], Step [2195/5873], Loss: 2.7168\n",
      "Epoch [1/5], Step [2196/5873], Loss: 4.2876\n",
      "Epoch [1/5], Step [2197/5873], Loss: 2.6686\n",
      "Epoch [1/5], Step [2198/5873], Loss: 3.2781\n",
      "Epoch [1/5], Step [2199/5873], Loss: 3.3484\n",
      "Epoch [1/5], Step [2200/5873], Loss: 4.4215\n",
      "Epoch [1/5], Step [2201/5873], Loss: 5.4526\n",
      "Epoch [1/5], Step [2202/5873], Loss: 4.5516\n",
      "Epoch [1/5], Step [2203/5873], Loss: 4.6441\n",
      "Epoch [1/5], Step [2204/5873], Loss: 3.9241\n",
      "Epoch [1/5], Step [2205/5873], Loss: 2.1417\n",
      "Epoch [1/5], Step [2206/5873], Loss: 5.8455\n",
      "Epoch [1/5], Step [2207/5873], Loss: 4.2524\n",
      "Epoch [1/5], Step [2208/5873], Loss: 5.8643\n",
      "Epoch [1/5], Step [2209/5873], Loss: 5.0938\n",
      "Epoch [1/5], Step [2210/5873], Loss: 3.6409\n",
      "Epoch [1/5], Step [2211/5873], Loss: 4.8282\n",
      "Epoch [1/5], Step [2212/5873], Loss: 3.7274\n",
      "Epoch [1/5], Step [2213/5873], Loss: 4.0972\n",
      "Epoch [1/5], Step [2214/5873], Loss: 4.8932\n",
      "Epoch [1/5], Step [2215/5873], Loss: 3.4429\n",
      "Epoch [1/5], Step [2216/5873], Loss: 3.4198\n",
      "Epoch [1/5], Step [2217/5873], Loss: 4.7989\n",
      "Epoch [1/5], Step [2218/5873], Loss: 5.1969\n",
      "Epoch [1/5], Step [2219/5873], Loss: 4.8282\n",
      "Epoch [1/5], Step [2220/5873], Loss: 6.0389\n",
      "Epoch [1/5], Step [2221/5873], Loss: 3.3669\n",
      "Epoch [1/5], Step [2222/5873], Loss: 4.9632\n",
      "Epoch [1/5], Step [2223/5873], Loss: 4.9929\n",
      "Epoch [1/5], Step [2224/5873], Loss: 3.8020\n",
      "Epoch [1/5], Step [2225/5873], Loss: 3.9337\n",
      "Epoch [1/5], Step [2226/5873], Loss: 4.3691\n",
      "Epoch [1/5], Step [2227/5873], Loss: 3.8641\n",
      "Epoch [1/5], Step [2228/5873], Loss: 4.7950\n",
      "Epoch [1/5], Step [2229/5873], Loss: 4.7659\n",
      "Epoch [1/5], Step [2230/5873], Loss: 5.3946\n",
      "Epoch [1/5], Step [2231/5873], Loss: 4.7911\n",
      "Epoch [1/5], Step [2232/5873], Loss: 4.4342\n",
      "Epoch [1/5], Step [2233/5873], Loss: 4.0870\n",
      "Epoch [1/5], Step [2234/5873], Loss: 4.6126\n",
      "Epoch [1/5], Step [2235/5873], Loss: 4.1027\n",
      "Epoch [1/5], Step [2236/5873], Loss: 4.6982\n",
      "Epoch [1/5], Step [2237/5873], Loss: 3.4313\n",
      "Epoch [1/5], Step [2238/5873], Loss: 3.7218\n",
      "Epoch [1/5], Step [2239/5873], Loss: 4.1963\n",
      "Epoch [1/5], Step [2240/5873], Loss: 4.2499\n",
      "Epoch [1/5], Step [2241/5873], Loss: 4.4664\n",
      "Epoch [1/5], Step [2242/5873], Loss: 4.4898\n",
      "Epoch [1/5], Step [2243/5873], Loss: 3.9928\n",
      "Epoch [1/5], Step [2244/5873], Loss: 4.7501\n",
      "Epoch [1/5], Step [2245/5873], Loss: 2.6796\n",
      "Epoch [1/5], Step [2246/5873], Loss: 4.1778\n",
      "Epoch [1/5], Step [2247/5873], Loss: 3.3101\n",
      "Epoch [1/5], Step [2248/5873], Loss: 4.1739\n",
      "Epoch [1/5], Step [2249/5873], Loss: 4.9669\n",
      "Epoch [1/5], Step [2250/5873], Loss: 4.8901\n",
      "Epoch [1/5], Step [2251/5873], Loss: 3.5742\n",
      "Epoch [1/5], Step [2252/5873], Loss: 3.0847\n",
      "Epoch [1/5], Step [2253/5873], Loss: 2.3903\n",
      "Epoch [1/5], Step [2254/5873], Loss: 2.3311\n",
      "Epoch [1/5], Step [2255/5873], Loss: 2.9628\n",
      "Epoch [1/5], Step [2256/5873], Loss: 5.0653\n",
      "Epoch [1/5], Step [2257/5873], Loss: 1.6193\n",
      "Epoch [1/5], Step [2258/5873], Loss: 2.1649\n",
      "Epoch [1/5], Step [2259/5873], Loss: 3.7704\n",
      "Epoch [1/5], Step [2260/5873], Loss: 5.6777\n",
      "Epoch [1/5], Step [2261/5873], Loss: 1.9347\n",
      "Epoch [1/5], Step [2262/5873], Loss: 6.3055\n",
      "Epoch [1/5], Step [2263/5873], Loss: 4.6600\n",
      "Epoch [1/5], Step [2264/5873], Loss: 4.0320\n",
      "Epoch [1/5], Step [2265/5873], Loss: 4.0092\n",
      "Epoch [1/5], Step [2266/5873], Loss: 3.4624\n",
      "Epoch [1/5], Step [2267/5873], Loss: 4.3878\n",
      "Epoch [1/5], Step [2268/5873], Loss: 2.7308\n",
      "Epoch [1/5], Step [2269/5873], Loss: 4.2018\n",
      "Epoch [1/5], Step [2270/5873], Loss: 4.4747\n",
      "Epoch [1/5], Step [2271/5873], Loss: 6.1641\n",
      "Epoch [1/5], Step [2272/5873], Loss: 4.4081\n",
      "Epoch [1/5], Step [2273/5873], Loss: 4.3361\n",
      "Epoch [1/5], Step [2274/5873], Loss: 4.7128\n",
      "Epoch [1/5], Step [2275/5873], Loss: 4.4245\n",
      "Epoch [1/5], Step [2276/5873], Loss: 3.2271\n",
      "Epoch [1/5], Step [2277/5873], Loss: 3.4238\n",
      "Epoch [1/5], Step [2278/5873], Loss: 5.2906\n",
      "Epoch [1/5], Step [2279/5873], Loss: 3.7599\n",
      "Epoch [1/5], Step [2280/5873], Loss: 3.8120\n",
      "Epoch [1/5], Step [2281/5873], Loss: 4.7135\n",
      "Epoch [1/5], Step [2282/5873], Loss: 4.9840\n",
      "Epoch [1/5], Step [2283/5873], Loss: 5.0984\n",
      "Epoch [1/5], Step [2284/5873], Loss: 4.7429\n",
      "Epoch [1/5], Step [2285/5873], Loss: 4.7088\n",
      "Epoch [1/5], Step [2286/5873], Loss: 3.2063\n",
      "Epoch [1/5], Step [2287/5873], Loss: 2.4808\n",
      "Epoch [1/5], Step [2288/5873], Loss: 3.7102\n",
      "Epoch [1/5], Step [2289/5873], Loss: 3.6517\n",
      "Epoch [1/5], Step [2290/5873], Loss: 3.7217\n",
      "Epoch [1/5], Step [2291/5873], Loss: 2.9420\n",
      "Epoch [1/5], Step [2292/5873], Loss: 4.9623\n",
      "Epoch [1/5], Step [2293/5873], Loss: 4.0206\n",
      "Epoch [1/5], Step [2294/5873], Loss: 4.1077\n",
      "Epoch [1/5], Step [2295/5873], Loss: 4.5943\n",
      "Epoch [1/5], Step [2296/5873], Loss: 3.4072\n",
      "Epoch [1/5], Step [2297/5873], Loss: 5.3246\n",
      "Epoch [1/5], Step [2298/5873], Loss: 3.5498\n",
      "Epoch [1/5], Step [2299/5873], Loss: 4.0542\n",
      "Epoch [1/5], Step [2300/5873], Loss: 4.9601\n",
      "Epoch [1/5], Step [2301/5873], Loss: 2.3161\n",
      "Epoch [1/5], Step [2302/5873], Loss: 3.3175\n",
      "Epoch [1/5], Step [2303/5873], Loss: 4.3037\n",
      "Epoch [1/5], Step [2304/5873], Loss: 3.6912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2305/5873], Loss: 2.7096\n",
      "Epoch [1/5], Step [2306/5873], Loss: 4.6057\n",
      "Epoch [1/5], Step [2307/5873], Loss: 4.2081\n",
      "Epoch [1/5], Step [2308/5873], Loss: 4.2627\n",
      "Epoch [1/5], Step [2309/5873], Loss: 2.4978\n",
      "Epoch [1/5], Step [2310/5873], Loss: 4.2674\n",
      "Epoch [1/5], Step [2311/5873], Loss: 4.1883\n",
      "Epoch [1/5], Step [2312/5873], Loss: 4.5358\n",
      "Epoch [1/5], Step [2313/5873], Loss: 1.9692\n",
      "Epoch [1/5], Step [2314/5873], Loss: 4.1575\n",
      "Epoch [1/5], Step [2315/5873], Loss: 2.3389\n",
      "Epoch [1/5], Step [2316/5873], Loss: 3.8665\n",
      "Epoch [1/5], Step [2317/5873], Loss: 5.5421\n",
      "Epoch [1/5], Step [2318/5873], Loss: 3.9726\n",
      "Epoch [1/5], Step [2319/5873], Loss: 4.3718\n",
      "Epoch [1/5], Step [2320/5873], Loss: 2.9997\n",
      "Epoch [1/5], Step [2321/5873], Loss: 3.2956\n",
      "Epoch [1/5], Step [2322/5873], Loss: 4.4449\n",
      "Epoch [1/5], Step [2323/5873], Loss: 3.8105\n",
      "Epoch [1/5], Step [2324/5873], Loss: 3.5557\n",
      "Epoch [1/5], Step [2325/5873], Loss: 5.0052\n",
      "Epoch [1/5], Step [2326/5873], Loss: 5.0701\n",
      "Epoch [1/5], Step [2327/5873], Loss: 1.4758\n",
      "Epoch [1/5], Step [2328/5873], Loss: 5.7237\n",
      "Epoch [1/5], Step [2329/5873], Loss: 4.6616\n",
      "Epoch [1/5], Step [2330/5873], Loss: 2.8336\n",
      "Epoch [1/5], Step [2331/5873], Loss: 3.8208\n",
      "Epoch [1/5], Step [2332/5873], Loss: 4.4230\n",
      "Epoch [1/5], Step [2333/5873], Loss: 1.8401\n",
      "Epoch [1/5], Step [2334/5873], Loss: 4.0871\n",
      "Epoch [1/5], Step [2335/5873], Loss: 3.4459\n",
      "Epoch [1/5], Step [2336/5873], Loss: 3.9674\n",
      "Epoch [1/5], Step [2337/5873], Loss: 5.3301\n",
      "Epoch [1/5], Step [2338/5873], Loss: 4.1397\n",
      "Epoch [1/5], Step [2339/5873], Loss: 5.6143\n",
      "Epoch [1/5], Step [2340/5873], Loss: 5.6761\n",
      "Epoch [1/5], Step [2341/5873], Loss: 4.5480\n",
      "Epoch [1/5], Step [2342/5873], Loss: 4.6236\n",
      "Epoch [1/5], Step [2343/5873], Loss: 3.4059\n",
      "Epoch [1/5], Step [2344/5873], Loss: 3.9590\n",
      "Epoch [1/5], Step [2345/5873], Loss: 4.2952\n",
      "Epoch [1/5], Step [2346/5873], Loss: 4.9445\n",
      "Epoch [1/5], Step [2347/5873], Loss: 4.1705\n",
      "Epoch [1/5], Step [2348/5873], Loss: 5.5667\n",
      "Epoch [1/5], Step [2349/5873], Loss: 1.6623\n",
      "Epoch [1/5], Step [2350/5873], Loss: 4.4412\n",
      "Epoch [1/5], Step [2351/5873], Loss: 5.9389\n",
      "Epoch [1/5], Step [2352/5873], Loss: 5.8231\n",
      "Epoch [1/5], Step [2353/5873], Loss: 5.7552\n",
      "Epoch [1/5], Step [2354/5873], Loss: 5.4014\n",
      "Epoch [1/5], Step [2355/5873], Loss: 5.5230\n",
      "Epoch [1/5], Step [2356/5873], Loss: 5.7974\n",
      "Epoch [1/5], Step [2357/5873], Loss: 5.5516\n",
      "Epoch [1/5], Step [2358/5873], Loss: 5.2195\n",
      "Epoch [1/5], Step [2359/5873], Loss: 5.0942\n",
      "Epoch [1/5], Step [2360/5873], Loss: 3.7857\n",
      "Epoch [1/5], Step [2361/5873], Loss: 5.1272\n",
      "Epoch [1/5], Step [2362/5873], Loss: 4.7496\n",
      "Epoch [1/5], Step [2363/5873], Loss: 3.8912\n",
      "Epoch [1/5], Step [2364/5873], Loss: 4.6670\n",
      "Epoch [1/5], Step [2365/5873], Loss: 4.8934\n",
      "Epoch [1/5], Step [2366/5873], Loss: 4.2464\n",
      "Epoch [1/5], Step [2367/5873], Loss: 4.8207\n",
      "Epoch [1/5], Step [2368/5873], Loss: 4.6459\n",
      "Epoch [1/5], Step [2369/5873], Loss: 3.7016\n",
      "Epoch [1/5], Step [2370/5873], Loss: 4.5213\n",
      "Epoch [1/5], Step [2371/5873], Loss: 3.3946\n",
      "Epoch [1/5], Step [2372/5873], Loss: 4.2931\n",
      "Epoch [1/5], Step [2373/5873], Loss: 3.6749\n",
      "Epoch [1/5], Step [2374/5873], Loss: 3.5264\n",
      "Epoch [1/5], Step [2375/5873], Loss: 4.5234\n",
      "Epoch [1/5], Step [2376/5873], Loss: 4.2535\n",
      "Epoch [1/5], Step [2377/5873], Loss: 3.7472\n",
      "Epoch [1/5], Step [2378/5873], Loss: 4.2690\n",
      "Epoch [1/5], Step [2379/5873], Loss: 3.7693\n",
      "Epoch [1/5], Step [2380/5873], Loss: 3.5748\n",
      "Epoch [1/5], Step [2381/5873], Loss: 2.4482\n",
      "Epoch [1/5], Step [2382/5873], Loss: 2.0836\n",
      "Epoch [1/5], Step [2383/5873], Loss: 4.1708\n",
      "Epoch [1/5], Step [2384/5873], Loss: 4.7834\n",
      "Epoch [1/5], Step [2385/5873], Loss: 2.9478\n",
      "Epoch [1/5], Step [2386/5873], Loss: 3.9313\n",
      "Epoch [1/5], Step [2387/5873], Loss: 2.3334\n",
      "Epoch [1/5], Step [2388/5873], Loss: 4.7046\n",
      "Epoch [1/5], Step [2389/5873], Loss: 4.5819\n",
      "Epoch [1/5], Step [2390/5873], Loss: 5.1689\n",
      "Epoch [1/5], Step [2391/5873], Loss: 3.6810\n",
      "Epoch [1/5], Step [2392/5873], Loss: 3.3651\n",
      "Epoch [1/5], Step [2393/5873], Loss: 4.2019\n",
      "Epoch [1/5], Step [2394/5873], Loss: 2.9947\n",
      "Epoch [1/5], Step [2395/5873], Loss: 5.3054\n",
      "Epoch [1/5], Step [2396/5873], Loss: 4.3744\n",
      "Epoch [1/5], Step [2397/5873], Loss: 3.4975\n",
      "Epoch [1/5], Step [2398/5873], Loss: 4.8151\n",
      "Epoch [1/5], Step [2399/5873], Loss: 5.9137\n",
      "Epoch [1/5], Step [2400/5873], Loss: 4.1731\n",
      "Epoch [1/5], Step [2401/5873], Loss: 3.1336\n",
      "Epoch [1/5], Step [2402/5873], Loss: 5.8004\n",
      "Epoch [1/5], Step [2403/5873], Loss: 2.3081\n",
      "Epoch [1/5], Step [2404/5873], Loss: 4.9098\n",
      "Epoch [1/5], Step [2405/5873], Loss: 5.3797\n",
      "Epoch [1/5], Step [2406/5873], Loss: 3.2152\n",
      "Epoch [1/5], Step [2407/5873], Loss: 4.5109\n",
      "Epoch [1/5], Step [2408/5873], Loss: 3.0603\n",
      "Epoch [1/5], Step [2409/5873], Loss: 3.8267\n",
      "Epoch [1/5], Step [2410/5873], Loss: 4.6895\n",
      "Epoch [1/5], Step [2411/5873], Loss: 3.9207\n",
      "Epoch [1/5], Step [2412/5873], Loss: 4.9375\n",
      "Epoch [1/5], Step [2413/5873], Loss: 3.5503\n",
      "Epoch [1/5], Step [2414/5873], Loss: 2.1051\n",
      "Epoch [1/5], Step [2415/5873], Loss: 5.0640\n",
      "Epoch [1/5], Step [2416/5873], Loss: 4.1992\n",
      "Epoch [1/5], Step [2417/5873], Loss: 5.3594\n",
      "Epoch [1/5], Step [2418/5873], Loss: 1.7713\n",
      "Epoch [1/5], Step [2419/5873], Loss: 4.3817\n",
      "Epoch [1/5], Step [2420/5873], Loss: 2.6989\n",
      "Epoch [1/5], Step [2421/5873], Loss: 5.1125\n",
      "Epoch [1/5], Step [2422/5873], Loss: 2.0573\n",
      "Epoch [1/5], Step [2423/5873], Loss: 4.4831\n",
      "Epoch [1/5], Step [2424/5873], Loss: 5.3847\n",
      "Epoch [1/5], Step [2425/5873], Loss: 4.4414\n",
      "Epoch [1/5], Step [2426/5873], Loss: 5.3191\n",
      "Epoch [1/5], Step [2427/5873], Loss: 5.0830\n",
      "Epoch [1/5], Step [2428/5873], Loss: 6.7096\n",
      "Epoch [1/5], Step [2429/5873], Loss: 2.0982\n",
      "Epoch [1/5], Step [2430/5873], Loss: 4.1750\n",
      "Epoch [1/5], Step [2431/5873], Loss: 2.3187\n",
      "Epoch [1/5], Step [2432/5873], Loss: 3.1279\n",
      "Epoch [1/5], Step [2433/5873], Loss: 4.5146\n",
      "Epoch [1/5], Step [2434/5873], Loss: 5.3872\n",
      "Epoch [1/5], Step [2435/5873], Loss: 3.6881\n",
      "Epoch [1/5], Step [2436/5873], Loss: 3.0657\n",
      "Epoch [1/5], Step [2437/5873], Loss: 4.4541\n",
      "Epoch [1/5], Step [2438/5873], Loss: 4.5529\n",
      "Epoch [1/5], Step [2439/5873], Loss: 5.8778\n",
      "Epoch [1/5], Step [2440/5873], Loss: 3.5015\n",
      "Epoch [1/5], Step [2441/5873], Loss: 4.9584\n",
      "Epoch [1/5], Step [2442/5873], Loss: 5.0035\n",
      "Epoch [1/5], Step [2443/5873], Loss: 3.1026\n",
      "Epoch [1/5], Step [2444/5873], Loss: 4.4913\n",
      "Epoch [1/5], Step [2445/5873], Loss: 4.2359\n",
      "Epoch [1/5], Step [2446/5873], Loss: 2.6058\n",
      "Epoch [1/5], Step [2447/5873], Loss: 2.7953\n",
      "Epoch [1/5], Step [2448/5873], Loss: 4.6254\n",
      "Epoch [1/5], Step [2449/5873], Loss: 2.8889\n",
      "Epoch [1/5], Step [2450/5873], Loss: 3.9494\n",
      "Epoch [1/5], Step [2451/5873], Loss: 4.2135\n",
      "Epoch [1/5], Step [2452/5873], Loss: 2.4639\n",
      "Epoch [1/5], Step [2453/5873], Loss: 4.5942\n",
      "Epoch [1/5], Step [2454/5873], Loss: 5.0693\n",
      "Epoch [1/5], Step [2455/5873], Loss: 3.3965\n",
      "Epoch [1/5], Step [2456/5873], Loss: 1.9566\n",
      "Epoch [1/5], Step [2457/5873], Loss: 5.3768\n",
      "Epoch [1/5], Step [2458/5873], Loss: 3.5568\n",
      "Epoch [1/5], Step [2459/5873], Loss: 4.8093\n",
      "Epoch [1/5], Step [2460/5873], Loss: 5.7990\n",
      "Epoch [1/5], Step [2461/5873], Loss: 2.5662\n",
      "Epoch [1/5], Step [2462/5873], Loss: 1.8826\n",
      "Epoch [1/5], Step [2463/5873], Loss: 2.9731\n",
      "Epoch [1/5], Step [2464/5873], Loss: 5.0672\n",
      "Epoch [1/5], Step [2465/5873], Loss: 4.5475\n",
      "Epoch [1/5], Step [2466/5873], Loss: 3.7767\n",
      "Epoch [1/5], Step [2467/5873], Loss: 6.2903\n",
      "Epoch [1/5], Step [2468/5873], Loss: 4.5716\n",
      "Epoch [1/5], Step [2469/5873], Loss: 4.1329\n",
      "Epoch [1/5], Step [2470/5873], Loss: 4.8674\n",
      "Epoch [1/5], Step [2471/5873], Loss: 4.6968\n",
      "Epoch [1/5], Step [2472/5873], Loss: 6.0436\n",
      "Epoch [1/5], Step [2473/5873], Loss: 2.0660\n",
      "Epoch [1/5], Step [2474/5873], Loss: 3.2191\n",
      "Epoch [1/5], Step [2475/5873], Loss: 3.9589\n",
      "Epoch [1/5], Step [2476/5873], Loss: 2.1524\n",
      "Epoch [1/5], Step [2477/5873], Loss: 5.2873\n",
      "Epoch [1/5], Step [2478/5873], Loss: 4.4203\n",
      "Epoch [1/5], Step [2479/5873], Loss: 5.5805\n",
      "Epoch [1/5], Step [2480/5873], Loss: 5.6970\n",
      "Epoch [1/5], Step [2481/5873], Loss: 2.4435\n",
      "Epoch [1/5], Step [2482/5873], Loss: 5.3324\n",
      "Epoch [1/5], Step [2483/5873], Loss: 4.8165\n",
      "Epoch [1/5], Step [2484/5873], Loss: 1.7376\n",
      "Epoch [1/5], Step [2485/5873], Loss: 4.7012\n",
      "Epoch [1/5], Step [2486/5873], Loss: 3.8355\n",
      "Epoch [1/5], Step [2487/5873], Loss: 1.9292\n",
      "Epoch [1/5], Step [2488/5873], Loss: 4.0597\n",
      "Epoch [1/5], Step [2489/5873], Loss: 4.9961\n",
      "Epoch [1/5], Step [2490/5873], Loss: 4.8826\n",
      "Epoch [1/5], Step [2491/5873], Loss: 5.0413\n",
      "Epoch [1/5], Step [2492/5873], Loss: 4.2401\n",
      "Epoch [1/5], Step [2493/5873], Loss: 4.3219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2494/5873], Loss: 3.3051\n",
      "Epoch [1/5], Step [2495/5873], Loss: 3.7668\n",
      "Epoch [1/5], Step [2496/5873], Loss: 3.6225\n",
      "Epoch [1/5], Step [2497/5873], Loss: 1.0871\n",
      "Epoch [1/5], Step [2498/5873], Loss: 3.4132\n",
      "Epoch [1/5], Step [2499/5873], Loss: 4.5694\n",
      "Epoch [1/5], Step [2500/5873], Loss: 2.3565\n",
      "Epoch [1/5], Step [2501/5873], Loss: 4.3949\n",
      "Epoch [1/5], Step [2502/5873], Loss: 2.3961\n",
      "Epoch [1/5], Step [2503/5873], Loss: 3.6834\n",
      "Epoch [1/5], Step [2504/5873], Loss: 4.5466\n",
      "Epoch [1/5], Step [2505/5873], Loss: 3.5873\n",
      "Epoch [1/5], Step [2506/5873], Loss: 1.3091\n",
      "Epoch [1/5], Step [2507/5873], Loss: 4.5104\n",
      "Epoch [1/5], Step [2508/5873], Loss: 4.7105\n",
      "Epoch [1/5], Step [2509/5873], Loss: 3.4997\n",
      "Epoch [1/5], Step [2510/5873], Loss: 5.3521\n",
      "Epoch [1/5], Step [2511/5873], Loss: 3.5805\n",
      "Epoch [1/5], Step [2512/5873], Loss: 4.6426\n",
      "Epoch [1/5], Step [2513/5873], Loss: 4.3792\n",
      "Epoch [1/5], Step [2514/5873], Loss: 3.8603\n",
      "Epoch [1/5], Step [2515/5873], Loss: 1.7302\n",
      "Epoch [1/5], Step [2516/5873], Loss: 3.3776\n",
      "Epoch [1/5], Step [2517/5873], Loss: 2.5971\n",
      "Epoch [1/5], Step [2518/5873], Loss: 4.0102\n",
      "Epoch [1/5], Step [2519/5873], Loss: 3.9615\n",
      "Epoch [1/5], Step [2520/5873], Loss: 3.9296\n",
      "Epoch [1/5], Step [2521/5873], Loss: 5.1570\n",
      "Epoch [1/5], Step [2522/5873], Loss: 4.1480\n",
      "Epoch [1/5], Step [2523/5873], Loss: 5.3779\n",
      "Epoch [1/5], Step [2524/5873], Loss: 4.3578\n",
      "Epoch [1/5], Step [2525/5873], Loss: 3.2788\n",
      "Epoch [1/5], Step [2526/5873], Loss: 5.2551\n",
      "Epoch [1/5], Step [2527/5873], Loss: 3.4960\n",
      "Epoch [1/5], Step [2528/5873], Loss: 4.0614\n",
      "Epoch [1/5], Step [2529/5873], Loss: 5.9453\n",
      "Epoch [1/5], Step [2530/5873], Loss: 6.4771\n",
      "Epoch [1/5], Step [2531/5873], Loss: 4.0558\n",
      "Epoch [1/5], Step [2532/5873], Loss: 2.5292\n",
      "Epoch [1/5], Step [2533/5873], Loss: 2.5378\n",
      "Epoch [1/5], Step [2534/5873], Loss: 3.5357\n",
      "Epoch [1/5], Step [2535/5873], Loss: 4.6766\n",
      "Epoch [1/5], Step [2536/5873], Loss: 5.4480\n",
      "Epoch [1/5], Step [2537/5873], Loss: 6.4645\n",
      "Epoch [1/5], Step [2538/5873], Loss: 2.4582\n",
      "Epoch [1/5], Step [2539/5873], Loss: 3.8236\n",
      "Epoch [1/5], Step [2540/5873], Loss: 1.7244\n",
      "Epoch [1/5], Step [2541/5873], Loss: 3.7005\n",
      "Epoch [1/5], Step [2542/5873], Loss: 2.6910\n",
      "Epoch [1/5], Step [2543/5873], Loss: 4.9088\n",
      "Epoch [1/5], Step [2544/5873], Loss: 4.1768\n",
      "Epoch [1/5], Step [2545/5873], Loss: 4.4300\n",
      "Epoch [1/5], Step [2546/5873], Loss: 3.8912\n",
      "Epoch [1/5], Step [2547/5873], Loss: 3.4393\n",
      "Epoch [1/5], Step [2548/5873], Loss: 4.3905\n",
      "Epoch [1/5], Step [2549/5873], Loss: 4.9019\n",
      "Epoch [1/5], Step [2550/5873], Loss: 2.4187\n",
      "Epoch [1/5], Step [2551/5873], Loss: 4.8221\n",
      "Epoch [1/5], Step [2552/5873], Loss: 6.1588\n",
      "Epoch [1/5], Step [2553/5873], Loss: 5.6364\n",
      "Epoch [1/5], Step [2554/5873], Loss: 2.7964\n",
      "Epoch [1/5], Step [2555/5873], Loss: 4.8575\n",
      "Epoch [1/5], Step [2556/5873], Loss: 2.6987\n",
      "Epoch [1/5], Step [2557/5873], Loss: 4.1830\n",
      "Epoch [1/5], Step [2558/5873], Loss: 2.7217\n",
      "Epoch [1/5], Step [2559/5873], Loss: 4.0046\n",
      "Epoch [1/5], Step [2560/5873], Loss: 4.2162\n",
      "Epoch [1/5], Step [2561/5873], Loss: 2.9818\n",
      "Epoch [1/5], Step [2562/5873], Loss: 5.5490\n",
      "Epoch [1/5], Step [2563/5873], Loss: 4.1879\n",
      "Epoch [1/5], Step [2564/5873], Loss: 4.9913\n",
      "Epoch [1/5], Step [2565/5873], Loss: 4.5136\n",
      "Epoch [1/5], Step [2566/5873], Loss: 5.2730\n",
      "Epoch [1/5], Step [2567/5873], Loss: 4.7191\n",
      "Epoch [1/5], Step [2568/5873], Loss: 4.8159\n",
      "Epoch [1/5], Step [2569/5873], Loss: 2.0671\n",
      "Epoch [1/5], Step [2570/5873], Loss: 2.4757\n",
      "Epoch [1/5], Step [2571/5873], Loss: 2.8462\n",
      "Epoch [1/5], Step [2572/5873], Loss: 6.0583\n",
      "Epoch [1/5], Step [2573/5873], Loss: 5.3551\n",
      "Epoch [1/5], Step [2574/5873], Loss: 5.2001\n",
      "Epoch [1/5], Step [2575/5873], Loss: 5.3903\n",
      "Epoch [1/5], Step [2576/5873], Loss: 3.2675\n",
      "Epoch [1/5], Step [2577/5873], Loss: 4.5655\n",
      "Epoch [1/5], Step [2578/5873], Loss: 3.4908\n",
      "Epoch [1/5], Step [2579/5873], Loss: 3.6144\n",
      "Epoch [1/5], Step [2580/5873], Loss: 4.9919\n",
      "Epoch [1/5], Step [2581/5873], Loss: 3.3973\n",
      "Epoch [1/5], Step [2582/5873], Loss: 3.2778\n",
      "Epoch [1/5], Step [2583/5873], Loss: 5.3633\n",
      "Epoch [1/5], Step [2584/5873], Loss: 4.6985\n",
      "Epoch [1/5], Step [2585/5873], Loss: 5.0814\n",
      "Epoch [1/5], Step [2586/5873], Loss: 5.4073\n",
      "Epoch [1/5], Step [2587/5873], Loss: 4.1810\n",
      "Epoch [1/5], Step [2588/5873], Loss: 1.7595\n",
      "Epoch [1/5], Step [2589/5873], Loss: 4.3174\n",
      "Epoch [1/5], Step [2590/5873], Loss: 4.7077\n",
      "Epoch [1/5], Step [2591/5873], Loss: 3.2945\n",
      "Epoch [1/5], Step [2592/5873], Loss: 4.6062\n",
      "Epoch [1/5], Step [2593/5873], Loss: 4.5152\n",
      "Epoch [1/5], Step [2594/5873], Loss: 4.2576\n",
      "Epoch [1/5], Step [2595/5873], Loss: 3.9272\n",
      "Epoch [1/5], Step [2596/5873], Loss: 3.5753\n",
      "Epoch [1/5], Step [2597/5873], Loss: 3.0437\n",
      "Epoch [1/5], Step [2598/5873], Loss: 5.1450\n",
      "Epoch [1/5], Step [2599/5873], Loss: 2.4066\n",
      "Epoch [1/5], Step [2600/5873], Loss: 3.4572\n",
      "Epoch [1/5], Step [2601/5873], Loss: 5.4401\n",
      "Epoch [1/5], Step [2602/5873], Loss: 4.2832\n",
      "Epoch [1/5], Step [2603/5873], Loss: 3.1951\n",
      "Epoch [1/5], Step [2604/5873], Loss: 4.8208\n",
      "Epoch [1/5], Step [2605/5873], Loss: 5.5154\n",
      "Epoch [1/5], Step [2606/5873], Loss: 2.0600\n",
      "Epoch [1/5], Step [2607/5873], Loss: 4.7359\n",
      "Epoch [1/5], Step [2608/5873], Loss: 2.1335\n",
      "Epoch [1/5], Step [2609/5873], Loss: 3.2185\n",
      "Epoch [1/5], Step [2610/5873], Loss: 5.0865\n",
      "Epoch [1/5], Step [2611/5873], Loss: 6.0573\n",
      "Epoch [1/5], Step [2612/5873], Loss: 4.9450\n",
      "Epoch [1/5], Step [2613/5873], Loss: 4.0364\n",
      "Epoch [1/5], Step [2614/5873], Loss: 4.2064\n",
      "Epoch [1/5], Step [2615/5873], Loss: 3.8065\n",
      "Epoch [1/5], Step [2616/5873], Loss: 2.3587\n",
      "Epoch [1/5], Step [2617/5873], Loss: 4.1690\n",
      "Epoch [1/5], Step [2618/5873], Loss: 3.7558\n",
      "Epoch [1/5], Step [2619/5873], Loss: 4.4468\n",
      "Epoch [1/5], Step [2620/5873], Loss: 5.2206\n",
      "Epoch [1/5], Step [2621/5873], Loss: 4.2102\n",
      "Epoch [1/5], Step [2622/5873], Loss: 4.5260\n",
      "Epoch [1/5], Step [2623/5873], Loss: 4.6484\n",
      "Epoch [1/5], Step [2624/5873], Loss: 4.3715\n",
      "Epoch [1/5], Step [2625/5873], Loss: 4.2413\n",
      "Epoch [1/5], Step [2626/5873], Loss: 4.8537\n",
      "Epoch [1/5], Step [2627/5873], Loss: 4.5269\n",
      "Epoch [1/5], Step [2628/5873], Loss: 4.8942\n",
      "Epoch [1/5], Step [2629/5873], Loss: 2.7813\n",
      "Epoch [1/5], Step [2630/5873], Loss: 3.6092\n",
      "Epoch [1/5], Step [2631/5873], Loss: 2.5863\n",
      "Epoch [1/5], Step [2632/5873], Loss: 5.1271\n",
      "Epoch [1/5], Step [2633/5873], Loss: 1.8486\n",
      "Epoch [1/5], Step [2634/5873], Loss: 3.8143\n",
      "Epoch [1/5], Step [2635/5873], Loss: 4.5773\n",
      "Epoch [1/5], Step [2636/5873], Loss: 5.4518\n",
      "Epoch [1/5], Step [2637/5873], Loss: 4.6175\n",
      "Epoch [1/5], Step [2638/5873], Loss: 4.3808\n",
      "Epoch [1/5], Step [2639/5873], Loss: 4.2362\n",
      "Epoch [1/5], Step [2640/5873], Loss: 4.2587\n",
      "Epoch [1/5], Step [2641/5873], Loss: 2.5737\n",
      "Epoch [1/5], Step [2642/5873], Loss: 2.4756\n",
      "Epoch [1/5], Step [2643/5873], Loss: 3.8384\n",
      "Epoch [1/5], Step [2644/5873], Loss: 5.5442\n",
      "Epoch [1/5], Step [2645/5873], Loss: 4.1758\n",
      "Epoch [1/5], Step [2646/5873], Loss: 4.4817\n",
      "Epoch [1/5], Step [2647/5873], Loss: 4.2286\n",
      "Epoch [1/5], Step [2648/5873], Loss: 3.6890\n",
      "Epoch [1/5], Step [2649/5873], Loss: 4.5768\n",
      "Epoch [1/5], Step [2650/5873], Loss: 5.4060\n",
      "Epoch [1/5], Step [2651/5873], Loss: 4.0303\n",
      "Epoch [1/5], Step [2652/5873], Loss: 5.2095\n",
      "Epoch [1/5], Step [2653/5873], Loss: 3.7858\n",
      "Epoch [1/5], Step [2654/5873], Loss: 4.5679\n",
      "Epoch [1/5], Step [2655/5873], Loss: 2.7003\n",
      "Epoch [1/5], Step [2656/5873], Loss: 4.3397\n",
      "Epoch [1/5], Step [2657/5873], Loss: 5.2169\n",
      "Epoch [1/5], Step [2658/5873], Loss: 1.3361\n",
      "Epoch [1/5], Step [2659/5873], Loss: 3.8997\n",
      "Epoch [1/5], Step [2660/5873], Loss: 3.9892\n",
      "Epoch [1/5], Step [2661/5873], Loss: 4.7875\n",
      "Epoch [1/5], Step [2662/5873], Loss: 3.2960\n",
      "Epoch [1/5], Step [2663/5873], Loss: 3.9809\n",
      "Epoch [1/5], Step [2664/5873], Loss: 2.0099\n",
      "Epoch [1/5], Step [2665/5873], Loss: 4.8757\n",
      "Epoch [1/5], Step [2666/5873], Loss: 4.3807\n",
      "Epoch [1/5], Step [2667/5873], Loss: 4.1804\n",
      "Epoch [1/5], Step [2668/5873], Loss: 2.9877\n",
      "Epoch [1/5], Step [2669/5873], Loss: 1.7881\n",
      "Epoch [1/5], Step [2670/5873], Loss: 5.0596\n",
      "Epoch [1/5], Step [2671/5873], Loss: 5.3183\n",
      "Epoch [1/5], Step [2672/5873], Loss: 1.4326\n",
      "Epoch [1/5], Step [2673/5873], Loss: 5.2553\n",
      "Epoch [1/5], Step [2674/5873], Loss: 3.9745\n",
      "Epoch [1/5], Step [2675/5873], Loss: 5.1547\n",
      "Epoch [1/5], Step [2676/5873], Loss: 3.1314\n",
      "Epoch [1/5], Step [2677/5873], Loss: 1.4812\n",
      "Epoch [1/5], Step [2678/5873], Loss: 5.9693\n",
      "Epoch [1/5], Step [2679/5873], Loss: 4.2445\n",
      "Epoch [1/5], Step [2680/5873], Loss: 4.3368\n",
      "Epoch [1/5], Step [2681/5873], Loss: 5.6384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2682/5873], Loss: 3.1140\n",
      "Epoch [1/5], Step [2683/5873], Loss: 4.3252\n",
      "Epoch [1/5], Step [2684/5873], Loss: 4.8842\n",
      "Epoch [1/5], Step [2685/5873], Loss: 4.8429\n",
      "Epoch [1/5], Step [2686/5873], Loss: 2.3865\n",
      "Epoch [1/5], Step [2687/5873], Loss: 2.3533\n",
      "Epoch [1/5], Step [2688/5873], Loss: 3.8531\n",
      "Epoch [1/5], Step [2689/5873], Loss: 5.7147\n",
      "Epoch [1/5], Step [2690/5873], Loss: 3.9043\n",
      "Epoch [1/5], Step [2691/5873], Loss: 5.0454\n",
      "Epoch [1/5], Step [2692/5873], Loss: 1.4921\n",
      "Epoch [1/5], Step [2693/5873], Loss: 4.7633\n",
      "Epoch [1/5], Step [2694/5873], Loss: 2.6353\n",
      "Epoch [1/5], Step [2695/5873], Loss: 3.4278\n",
      "Epoch [1/5], Step [2696/5873], Loss: 3.6026\n",
      "Epoch [1/5], Step [2697/5873], Loss: 5.1937\n",
      "Epoch [1/5], Step [2698/5873], Loss: 4.7113\n",
      "Epoch [1/5], Step [2699/5873], Loss: 6.2402\n",
      "Epoch [1/5], Step [2700/5873], Loss: 4.8168\n",
      "Epoch [1/5], Step [2701/5873], Loss: 3.0982\n",
      "Epoch [1/5], Step [2702/5873], Loss: 1.6161\n",
      "Epoch [1/5], Step [2703/5873], Loss: 4.8287\n",
      "Epoch [1/5], Step [2704/5873], Loss: 4.4664\n",
      "Epoch [1/5], Step [2705/5873], Loss: 6.4948\n",
      "Epoch [1/5], Step [2706/5873], Loss: 5.0206\n",
      "Epoch [1/5], Step [2707/5873], Loss: 4.2826\n",
      "Epoch [1/5], Step [2708/5873], Loss: 3.8978\n",
      "Epoch [1/5], Step [2709/5873], Loss: 3.4814\n",
      "Epoch [1/5], Step [2710/5873], Loss: 2.9312\n",
      "Epoch [1/5], Step [2711/5873], Loss: 5.9761\n",
      "Epoch [1/5], Step [2712/5873], Loss: 4.5292\n",
      "Epoch [1/5], Step [2713/5873], Loss: 4.7010\n",
      "Epoch [1/5], Step [2714/5873], Loss: 5.0641\n",
      "Epoch [1/5], Step [2715/5873], Loss: 4.3781\n",
      "Epoch [1/5], Step [2716/5873], Loss: 4.2037\n",
      "Epoch [1/5], Step [2717/5873], Loss: 2.1170\n",
      "Epoch [1/5], Step [2718/5873], Loss: 4.1114\n",
      "Epoch [1/5], Step [2719/5873], Loss: 3.3913\n",
      "Epoch [1/5], Step [2720/5873], Loss: 5.5587\n",
      "Epoch [1/5], Step [2721/5873], Loss: 4.5443\n",
      "Epoch [1/5], Step [2722/5873], Loss: 2.7671\n",
      "Epoch [1/5], Step [2723/5873], Loss: 5.7804\n",
      "Epoch [1/5], Step [2724/5873], Loss: 4.9347\n",
      "Epoch [1/5], Step [2725/5873], Loss: 4.5120\n",
      "Epoch [1/5], Step [2726/5873], Loss: 4.8176\n",
      "Epoch [1/5], Step [2727/5873], Loss: 4.7480\n",
      "Epoch [1/5], Step [2728/5873], Loss: 3.6537\n",
      "Epoch [1/5], Step [2729/5873], Loss: 4.8475\n",
      "Epoch [1/5], Step [2730/5873], Loss: 4.8753\n",
      "Epoch [1/5], Step [2731/5873], Loss: 5.0023\n",
      "Epoch [1/5], Step [2732/5873], Loss: 4.8743\n",
      "Epoch [1/5], Step [2733/5873], Loss: 3.9876\n",
      "Epoch [1/5], Step [2734/5873], Loss: 4.5402\n",
      "Epoch [1/5], Step [2735/5873], Loss: 4.3894\n",
      "Epoch [1/5], Step [2736/5873], Loss: 2.4995\n",
      "Epoch [1/5], Step [2737/5873], Loss: 4.3671\n",
      "Epoch [1/5], Step [2738/5873], Loss: 4.5044\n",
      "Epoch [1/5], Step [2739/5873], Loss: 4.3337\n",
      "Epoch [1/5], Step [2740/5873], Loss: 5.5451\n",
      "Epoch [1/5], Step [2741/5873], Loss: 3.7602\n",
      "Epoch [1/5], Step [2742/5873], Loss: 3.9726\n",
      "Epoch [1/5], Step [2743/5873], Loss: 4.2634\n",
      "Epoch [1/5], Step [2744/5873], Loss: 4.7755\n",
      "Epoch [1/5], Step [2745/5873], Loss: 3.4411\n",
      "Epoch [1/5], Step [2746/5873], Loss: 5.4600\n",
      "Epoch [1/5], Step [2747/5873], Loss: 4.5664\n",
      "Epoch [1/5], Step [2748/5873], Loss: 5.8336\n",
      "Epoch [1/5], Step [2749/5873], Loss: 4.1540\n",
      "Epoch [1/5], Step [2750/5873], Loss: 4.5524\n",
      "Epoch [1/5], Step [2751/5873], Loss: 4.4820\n",
      "Epoch [1/5], Step [2752/5873], Loss: 3.4940\n",
      "Epoch [1/5], Step [2753/5873], Loss: 4.2790\n",
      "Epoch [1/5], Step [2754/5873], Loss: 3.3155\n",
      "Epoch [1/5], Step [2755/5873], Loss: 3.6047\n",
      "Epoch [1/5], Step [2756/5873], Loss: 3.5537\n",
      "Epoch [1/5], Step [2757/5873], Loss: 3.1633\n",
      "Epoch [1/5], Step [2758/5873], Loss: 3.8771\n",
      "Epoch [1/5], Step [2759/5873], Loss: 3.0181\n",
      "Epoch [1/5], Step [2760/5873], Loss: 4.0340\n",
      "Epoch [1/5], Step [2761/5873], Loss: 3.5415\n",
      "Epoch [1/5], Step [2762/5873], Loss: 3.2686\n",
      "Epoch [1/5], Step [2763/5873], Loss: 4.7434\n",
      "Epoch [1/5], Step [2764/5873], Loss: 2.2417\n",
      "Epoch [1/5], Step [2765/5873], Loss: 5.5136\n",
      "Epoch [1/5], Step [2766/5873], Loss: 3.4941\n",
      "Epoch [1/5], Step [2767/5873], Loss: 4.4004\n",
      "Epoch [1/5], Step [2768/5873], Loss: 3.9725\n",
      "Epoch [1/5], Step [2769/5873], Loss: 4.8411\n",
      "Epoch [1/5], Step [2770/5873], Loss: 3.8637\n",
      "Epoch [1/5], Step [2771/5873], Loss: 4.9168\n",
      "Epoch [1/5], Step [2772/5873], Loss: 3.6621\n",
      "Epoch [1/5], Step [2773/5873], Loss: 3.5131\n",
      "Epoch [1/5], Step [2774/5873], Loss: 3.5065\n",
      "Epoch [1/5], Step [2775/5873], Loss: 5.4282\n",
      "Epoch [1/5], Step [2776/5873], Loss: 3.8336\n",
      "Epoch [1/5], Step [2777/5873], Loss: 4.5897\n",
      "Epoch [1/5], Step [2778/5873], Loss: 1.8442\n",
      "Epoch [1/5], Step [2779/5873], Loss: 1.7629\n",
      "Epoch [1/5], Step [2780/5873], Loss: 2.6133\n",
      "Epoch [1/5], Step [2781/5873], Loss: 4.7859\n",
      "Epoch [1/5], Step [2782/5873], Loss: 3.9100\n",
      "Epoch [1/5], Step [2783/5873], Loss: 2.8378\n",
      "Epoch [1/5], Step [2784/5873], Loss: 3.5658\n",
      "Epoch [1/5], Step [2785/5873], Loss: 4.1031\n",
      "Epoch [1/5], Step [2786/5873], Loss: 4.0888\n",
      "Epoch [1/5], Step [2787/5873], Loss: 4.9914\n",
      "Epoch [1/5], Step [2788/5873], Loss: 2.0960\n",
      "Epoch [1/5], Step [2789/5873], Loss: 4.5202\n",
      "Epoch [1/5], Step [2790/5873], Loss: 3.9528\n",
      "Epoch [1/5], Step [2791/5873], Loss: 4.6436\n",
      "Epoch [1/5], Step [2792/5873], Loss: 3.8058\n",
      "Epoch [1/5], Step [2793/5873], Loss: 4.4976\n",
      "Epoch [1/5], Step [2794/5873], Loss: 4.4683\n",
      "Epoch [1/5], Step [2795/5873], Loss: 4.5511\n",
      "Epoch [1/5], Step [2796/5873], Loss: 3.4527\n",
      "Epoch [1/5], Step [2797/5873], Loss: 3.5447\n",
      "Epoch [1/5], Step [2798/5873], Loss: 3.8342\n",
      "Epoch [1/5], Step [2799/5873], Loss: 3.3954\n",
      "Epoch [1/5], Step [2800/5873], Loss: 4.2014\n",
      "Epoch [1/5], Step [2801/5873], Loss: 3.1529\n",
      "Epoch [1/5], Step [2802/5873], Loss: 4.8857\n",
      "Epoch [1/5], Step [2803/5873], Loss: 2.2678\n",
      "Epoch [1/5], Step [2804/5873], Loss: 3.8064\n",
      "Epoch [1/5], Step [2805/5873], Loss: 4.5531\n",
      "Epoch [1/5], Step [2806/5873], Loss: 4.4044\n",
      "Epoch [1/5], Step [2807/5873], Loss: 4.1608\n",
      "Epoch [1/5], Step [2808/5873], Loss: 3.9394\n",
      "Epoch [1/5], Step [2809/5873], Loss: 3.2888\n",
      "Epoch [1/5], Step [2810/5873], Loss: 2.6324\n",
      "Epoch [1/5], Step [2811/5873], Loss: 4.3918\n",
      "Epoch [1/5], Step [2812/5873], Loss: 4.0570\n",
      "Epoch [1/5], Step [2813/5873], Loss: 3.8360\n",
      "Epoch [1/5], Step [2814/5873], Loss: 3.2414\n",
      "Epoch [1/5], Step [2815/5873], Loss: 4.4119\n",
      "Epoch [1/5], Step [2816/5873], Loss: 3.6564\n",
      "Epoch [1/5], Step [2817/5873], Loss: 4.3231\n",
      "Epoch [1/5], Step [2818/5873], Loss: 4.8642\n",
      "Epoch [1/5], Step [2819/5873], Loss: 4.4448\n",
      "Epoch [1/5], Step [2820/5873], Loss: 5.4084\n",
      "Epoch [1/5], Step [2821/5873], Loss: 4.1507\n",
      "Epoch [1/5], Step [2822/5873], Loss: 4.4945\n",
      "Epoch [1/5], Step [2823/5873], Loss: 3.2994\n",
      "Epoch [1/5], Step [2824/5873], Loss: 4.1213\n",
      "Epoch [1/5], Step [2825/5873], Loss: 4.0969\n",
      "Epoch [1/5], Step [2826/5873], Loss: 2.9929\n",
      "Epoch [1/5], Step [2827/5873], Loss: 3.4830\n",
      "Epoch [1/5], Step [2828/5873], Loss: 5.3049\n",
      "Epoch [1/5], Step [2829/5873], Loss: 4.4151\n",
      "Epoch [1/5], Step [2830/5873], Loss: 2.6709\n",
      "Epoch [1/5], Step [2831/5873], Loss: 3.2186\n",
      "Epoch [1/5], Step [2832/5873], Loss: 4.6178\n",
      "Epoch [1/5], Step [2833/5873], Loss: 4.6155\n",
      "Epoch [1/5], Step [2834/5873], Loss: 5.1343\n",
      "Epoch [1/5], Step [2835/5873], Loss: 4.0604\n",
      "Epoch [1/5], Step [2836/5873], Loss: 3.3101\n",
      "Epoch [1/5], Step [2837/5873], Loss: 3.1244\n",
      "Epoch [1/5], Step [2838/5873], Loss: 4.3162\n",
      "Epoch [1/5], Step [2839/5873], Loss: 3.4355\n",
      "Epoch [1/5], Step [2840/5873], Loss: 5.0269\n",
      "Epoch [1/5], Step [2841/5873], Loss: 4.1048\n",
      "Epoch [1/5], Step [2842/5873], Loss: 4.9474\n",
      "Epoch [1/5], Step [2843/5873], Loss: 3.6608\n",
      "Epoch [1/5], Step [2844/5873], Loss: 2.6936\n",
      "Epoch [1/5], Step [2845/5873], Loss: 4.6212\n",
      "Epoch [1/5], Step [2846/5873], Loss: 3.9472\n",
      "Epoch [1/5], Step [2847/5873], Loss: 6.4625\n",
      "Epoch [1/5], Step [2848/5873], Loss: 5.6654\n",
      "Epoch [1/5], Step [2849/5873], Loss: 5.1888\n",
      "Epoch [1/5], Step [2850/5873], Loss: 4.0629\n",
      "Epoch [1/5], Step [2851/5873], Loss: 4.1208\n",
      "Epoch [1/5], Step [2852/5873], Loss: 5.1928\n",
      "Epoch [1/5], Step [2853/5873], Loss: 5.4192\n",
      "Epoch [1/5], Step [2854/5873], Loss: 3.1210\n",
      "Epoch [1/5], Step [2855/5873], Loss: 4.1479\n",
      "Epoch [1/5], Step [2856/5873], Loss: 2.8985\n",
      "Epoch [1/5], Step [2857/5873], Loss: 5.1400\n",
      "Epoch [1/5], Step [2858/5873], Loss: 3.5672\n",
      "Epoch [1/5], Step [2859/5873], Loss: 1.6211\n",
      "Epoch [1/5], Step [2860/5873], Loss: 4.7593\n",
      "Epoch [1/5], Step [2861/5873], Loss: 4.2192\n",
      "Epoch [1/5], Step [2862/5873], Loss: 3.3041\n",
      "Epoch [1/5], Step [2863/5873], Loss: 3.7144\n",
      "Epoch [1/5], Step [2864/5873], Loss: 5.8125\n",
      "Epoch [1/5], Step [2865/5873], Loss: 4.0318\n",
      "Epoch [1/5], Step [2866/5873], Loss: 4.5655\n",
      "Epoch [1/5], Step [2867/5873], Loss: 2.9510\n",
      "Epoch [1/5], Step [2868/5873], Loss: 3.6852\n",
      "Epoch [1/5], Step [2869/5873], Loss: 4.8192\n",
      "Epoch [1/5], Step [2870/5873], Loss: 2.8409\n",
      "Epoch [1/5], Step [2871/5873], Loss: 3.6613\n",
      "Epoch [1/5], Step [2872/5873], Loss: 4.0053\n",
      "Epoch [1/5], Step [2873/5873], Loss: 5.4981\n",
      "Epoch [1/5], Step [2874/5873], Loss: 5.0941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2875/5873], Loss: 4.3059\n",
      "Epoch [1/5], Step [2876/5873], Loss: 3.9426\n",
      "Epoch [1/5], Step [2877/5873], Loss: 4.2535\n",
      "Epoch [1/5], Step [2878/5873], Loss: 3.5625\n",
      "Epoch [1/5], Step [2879/5873], Loss: 3.5351\n",
      "Epoch [1/5], Step [2880/5873], Loss: 3.5593\n",
      "Epoch [1/5], Step [2881/5873], Loss: 3.3658\n",
      "Epoch [1/5], Step [2882/5873], Loss: 4.8092\n",
      "Epoch [1/5], Step [2883/5873], Loss: 2.9232\n",
      "Epoch [1/5], Step [2884/5873], Loss: 3.4841\n",
      "Epoch [1/5], Step [2885/5873], Loss: 3.9321\n",
      "Epoch [1/5], Step [2886/5873], Loss: 4.2670\n",
      "Epoch [1/5], Step [2887/5873], Loss: 3.6454\n",
      "Epoch [1/5], Step [2888/5873], Loss: 2.3577\n",
      "Epoch [1/5], Step [2889/5873], Loss: 3.6662\n",
      "Epoch [1/5], Step [2890/5873], Loss: 4.9710\n",
      "Epoch [1/5], Step [2891/5873], Loss: 4.7844\n",
      "Epoch [1/5], Step [2892/5873], Loss: 2.2432\n",
      "Epoch [1/5], Step [2893/5873], Loss: 4.1085\n",
      "Epoch [1/5], Step [2894/5873], Loss: 5.1831\n",
      "Epoch [1/5], Step [2895/5873], Loss: 5.5645\n",
      "Epoch [1/5], Step [2896/5873], Loss: 2.0086\n",
      "Epoch [1/5], Step [2897/5873], Loss: 3.8395\n",
      "Epoch [1/5], Step [2898/5873], Loss: 2.6803\n",
      "Epoch [1/5], Step [2899/5873], Loss: 1.3448\n",
      "Epoch [1/5], Step [2900/5873], Loss: 5.0529\n",
      "Epoch [1/5], Step [2901/5873], Loss: 4.3039\n",
      "Epoch [1/5], Step [2902/5873], Loss: 5.6515\n",
      "Epoch [1/5], Step [2903/5873], Loss: 4.5596\n",
      "Epoch [1/5], Step [2904/5873], Loss: 4.5264\n",
      "Epoch [1/5], Step [2905/5873], Loss: 3.8614\n",
      "Epoch [1/5], Step [2906/5873], Loss: 2.9457\n",
      "Epoch [1/5], Step [2907/5873], Loss: 2.7979\n",
      "Epoch [1/5], Step [2908/5873], Loss: 6.7128\n",
      "Epoch [1/5], Step [2909/5873], Loss: 5.1894\n",
      "Epoch [1/5], Step [2910/5873], Loss: 3.9369\n",
      "Epoch [1/5], Step [2911/5873], Loss: 4.3803\n",
      "Epoch [1/5], Step [2912/5873], Loss: 4.6644\n",
      "Epoch [1/5], Step [2913/5873], Loss: 3.9228\n",
      "Epoch [1/5], Step [2914/5873], Loss: 2.8167\n",
      "Epoch [1/5], Step [2915/5873], Loss: 1.5616\n",
      "Epoch [1/5], Step [2916/5873], Loss: 5.0021\n",
      "Epoch [1/5], Step [2917/5873], Loss: 3.7037\n",
      "Epoch [1/5], Step [2918/5873], Loss: 5.5746\n",
      "Epoch [1/5], Step [2919/5873], Loss: 3.6335\n",
      "Epoch [1/5], Step [2920/5873], Loss: 3.9742\n",
      "Epoch [1/5], Step [2921/5873], Loss: 1.2411\n",
      "Epoch [1/5], Step [2922/5873], Loss: 4.6066\n",
      "Epoch [1/5], Step [2923/5873], Loss: 4.8746\n",
      "Epoch [1/5], Step [2924/5873], Loss: 3.9457\n",
      "Epoch [1/5], Step [2925/5873], Loss: 5.4549\n",
      "Epoch [1/5], Step [2926/5873], Loss: 5.2787\n",
      "Epoch [1/5], Step [2927/5873], Loss: 2.8008\n",
      "Epoch [1/5], Step [2928/5873], Loss: 3.1739\n",
      "Epoch [1/5], Step [2929/5873], Loss: 3.6124\n",
      "Epoch [1/5], Step [2930/5873], Loss: 4.7776\n",
      "Epoch [1/5], Step [2931/5873], Loss: 3.0940\n",
      "Epoch [1/5], Step [2932/5873], Loss: 2.9358\n",
      "Epoch [1/5], Step [2933/5873], Loss: 3.6662\n",
      "Epoch [1/5], Step [2934/5873], Loss: 4.2830\n",
      "Epoch [1/5], Step [2935/5873], Loss: 4.8829\n",
      "Epoch [1/5], Step [2936/5873], Loss: 4.3895\n",
      "Epoch [1/5], Step [2937/5873], Loss: 4.9038\n",
      "Epoch [1/5], Step [2938/5873], Loss: 4.6308\n",
      "Epoch [1/5], Step [2939/5873], Loss: 3.7847\n",
      "Epoch [1/5], Step [2940/5873], Loss: 3.1455\n",
      "Epoch [1/5], Step [2941/5873], Loss: 3.8805\n",
      "Epoch [1/5], Step [2942/5873], Loss: 3.9879\n",
      "Epoch [1/5], Step [2943/5873], Loss: 2.9128\n",
      "Epoch [1/5], Step [2944/5873], Loss: 5.1881\n",
      "Epoch [1/5], Step [2945/5873], Loss: 3.6811\n",
      "Epoch [1/5], Step [2946/5873], Loss: 2.4612\n",
      "Epoch [1/5], Step [2947/5873], Loss: 4.2998\n",
      "Epoch [1/5], Step [2948/5873], Loss: 5.7533\n",
      "Epoch [1/5], Step [2949/5873], Loss: 5.3123\n",
      "Epoch [1/5], Step [2950/5873], Loss: 5.1678\n",
      "Epoch [1/5], Step [2951/5873], Loss: 3.5677\n",
      "Epoch [1/5], Step [2952/5873], Loss: 4.4422\n",
      "Epoch [1/5], Step [2953/5873], Loss: 4.5424\n",
      "Epoch [1/5], Step [2954/5873], Loss: 4.3899\n",
      "Epoch [1/5], Step [2955/5873], Loss: 5.4327\n",
      "Epoch [1/5], Step [2956/5873], Loss: 4.6573\n",
      "Epoch [1/5], Step [2957/5873], Loss: 4.4213\n",
      "Epoch [1/5], Step [2958/5873], Loss: 4.3200\n",
      "Epoch [1/5], Step [2959/5873], Loss: 4.1203\n",
      "Epoch [1/5], Step [2960/5873], Loss: 1.9347\n",
      "Epoch [1/5], Step [2961/5873], Loss: 4.8052\n",
      "Epoch [1/5], Step [2962/5873], Loss: 3.7876\n",
      "Epoch [1/5], Step [2963/5873], Loss: 3.8031\n",
      "Epoch [1/5], Step [2964/5873], Loss: 3.2692\n",
      "Epoch [1/5], Step [2965/5873], Loss: 4.0437\n",
      "Epoch [1/5], Step [2966/5873], Loss: 4.6725\n",
      "Epoch [1/5], Step [2967/5873], Loss: 5.8140\n",
      "Epoch [1/5], Step [2968/5873], Loss: 3.2926\n",
      "Epoch [1/5], Step [2969/5873], Loss: 4.6939\n",
      "Epoch [1/5], Step [2970/5873], Loss: 2.9282\n",
      "Epoch [1/5], Step [2971/5873], Loss: 2.6426\n",
      "Epoch [1/5], Step [2972/5873], Loss: 4.2593\n",
      "Epoch [1/5], Step [2973/5873], Loss: 4.0014\n",
      "Epoch [1/5], Step [2974/5873], Loss: 3.7640\n",
      "Epoch [1/5], Step [2975/5873], Loss: 3.7086\n",
      "Epoch [1/5], Step [2976/5873], Loss: 3.8927\n",
      "Epoch [1/5], Step [2977/5873], Loss: 5.5678\n",
      "Epoch [1/5], Step [2978/5873], Loss: 3.3427\n",
      "Epoch [1/5], Step [2979/5873], Loss: 5.2913\n",
      "Epoch [1/5], Step [2980/5873], Loss: 4.2453\n",
      "Epoch [1/5], Step [2981/5873], Loss: 2.7797\n",
      "Epoch [1/5], Step [2982/5873], Loss: 3.2970\n",
      "Epoch [1/5], Step [2983/5873], Loss: 4.0355\n",
      "Epoch [1/5], Step [2984/5873], Loss: 4.9856\n",
      "Epoch [1/5], Step [2985/5873], Loss: 3.7152\n",
      "Epoch [1/5], Step [2986/5873], Loss: 4.7638\n",
      "Epoch [1/5], Step [2987/5873], Loss: 4.3249\n",
      "Epoch [1/5], Step [2988/5873], Loss: 2.5693\n",
      "Epoch [1/5], Step [2989/5873], Loss: 5.6569\n",
      "Epoch [1/5], Step [2990/5873], Loss: 4.4635\n",
      "Epoch [1/5], Step [2991/5873], Loss: 4.6237\n",
      "Epoch [1/5], Step [2992/5873], Loss: 4.0792\n",
      "Epoch [1/5], Step [2993/5873], Loss: 5.5565\n",
      "Epoch [1/5], Step [2994/5873], Loss: 5.2821\n",
      "Epoch [1/5], Step [2995/5873], Loss: 4.4670\n",
      "Epoch [1/5], Step [2996/5873], Loss: 5.1171\n",
      "Epoch [1/5], Step [2997/5873], Loss: 3.8820\n",
      "Epoch [1/5], Step [2998/5873], Loss: 2.0734\n",
      "Epoch [1/5], Step [2999/5873], Loss: 2.7347\n",
      "Epoch [1/5], Step [3000/5873], Loss: 4.6260\n",
      "Epoch [1/5], Step [3001/5873], Loss: 3.3453\n",
      "Epoch [1/5], Step [3002/5873], Loss: 4.8107\n",
      "Epoch [1/5], Step [3003/5873], Loss: 3.8530\n",
      "Epoch [1/5], Step [3004/5873], Loss: 5.4503\n",
      "Epoch [1/5], Step [3005/5873], Loss: 3.6693\n",
      "Epoch [1/5], Step [3006/5873], Loss: 4.0317\n",
      "Epoch [1/5], Step [3007/5873], Loss: 2.4073\n",
      "Epoch [1/5], Step [3008/5873], Loss: 5.5765\n",
      "Epoch [1/5], Step [3009/5873], Loss: 5.4164\n",
      "Epoch [1/5], Step [3010/5873], Loss: 4.8251\n",
      "Epoch [1/5], Step [3011/5873], Loss: 2.6201\n",
      "Epoch [1/5], Step [3012/5873], Loss: 3.8913\n",
      "Epoch [1/5], Step [3013/5873], Loss: 6.6351\n",
      "Epoch [1/5], Step [3014/5873], Loss: 5.5989\n",
      "Epoch [1/5], Step [3015/5873], Loss: 4.1445\n",
      "Epoch [1/5], Step [3016/5873], Loss: 5.3828\n",
      "Epoch [1/5], Step [3017/5873], Loss: 4.2704\n",
      "Epoch [1/5], Step [3018/5873], Loss: 3.0378\n",
      "Epoch [1/5], Step [3019/5873], Loss: 4.9289\n",
      "Epoch [1/5], Step [3020/5873], Loss: 3.9271\n",
      "Epoch [1/5], Step [3021/5873], Loss: 2.5105\n",
      "Epoch [1/5], Step [3022/5873], Loss: 5.6530\n",
      "Epoch [1/5], Step [3023/5873], Loss: 5.0248\n",
      "Epoch [1/5], Step [3024/5873], Loss: 4.8372\n",
      "Epoch [1/5], Step [3025/5873], Loss: 2.8610\n",
      "Epoch [1/5], Step [3026/5873], Loss: 5.1015\n",
      "Epoch [1/5], Step [3027/5873], Loss: 3.5739\n",
      "Epoch [1/5], Step [3028/5873], Loss: 4.5763\n",
      "Epoch [1/5], Step [3029/5873], Loss: 3.5832\n",
      "Epoch [1/5], Step [3030/5873], Loss: 1.9917\n",
      "Epoch [1/5], Step [3031/5873], Loss: 1.9044\n",
      "Epoch [1/5], Step [3032/5873], Loss: 5.6085\n",
      "Epoch [1/5], Step [3033/5873], Loss: 3.9158\n",
      "Epoch [1/5], Step [3034/5873], Loss: 3.7312\n",
      "Epoch [1/5], Step [3035/5873], Loss: 4.3437\n",
      "Epoch [1/5], Step [3036/5873], Loss: 3.5572\n",
      "Epoch [1/5], Step [3037/5873], Loss: 5.3831\n",
      "Epoch [1/5], Step [3038/5873], Loss: 4.4790\n",
      "Epoch [1/5], Step [3039/5873], Loss: 6.0512\n",
      "Epoch [1/5], Step [3040/5873], Loss: 4.5536\n",
      "Epoch [1/5], Step [3041/5873], Loss: 5.0528\n",
      "Epoch [1/5], Step [3042/5873], Loss: 2.2989\n",
      "Epoch [1/5], Step [3043/5873], Loss: 4.3818\n",
      "Epoch [1/5], Step [3044/5873], Loss: 4.9737\n",
      "Epoch [1/5], Step [3045/5873], Loss: 3.2729\n",
      "Epoch [1/5], Step [3046/5873], Loss: 3.9640\n",
      "Epoch [1/5], Step [3047/5873], Loss: 4.6291\n",
      "Epoch [1/5], Step [3048/5873], Loss: 5.3033\n",
      "Epoch [1/5], Step [3049/5873], Loss: 4.0844\n",
      "Epoch [1/5], Step [3050/5873], Loss: 4.7329\n",
      "Epoch [1/5], Step [3051/5873], Loss: 2.7802\n",
      "Epoch [1/5], Step [3052/5873], Loss: 3.5390\n",
      "Epoch [1/5], Step [3053/5873], Loss: 1.8465\n",
      "Epoch [1/5], Step [3054/5873], Loss: 3.1972\n",
      "Epoch [1/5], Step [3055/5873], Loss: 5.2068\n",
      "Epoch [1/5], Step [3056/5873], Loss: 2.4972\n",
      "Epoch [1/5], Step [3057/5873], Loss: 3.4973\n",
      "Epoch [1/5], Step [3058/5873], Loss: 3.9845\n",
      "Epoch [1/5], Step [3059/5873], Loss: 1.7221\n",
      "Epoch [1/5], Step [3060/5873], Loss: 2.9174\n",
      "Epoch [1/5], Step [3061/5873], Loss: 5.6390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3062/5873], Loss: 5.1152\n",
      "Epoch [1/5], Step [3063/5873], Loss: 3.1596\n",
      "Epoch [1/5], Step [3064/5873], Loss: 3.7822\n",
      "Epoch [1/5], Step [3065/5873], Loss: 6.7040\n",
      "Epoch [1/5], Step [3066/5873], Loss: 5.6496\n",
      "Epoch [1/5], Step [3067/5873], Loss: 4.8483\n",
      "Epoch [1/5], Step [3068/5873], Loss: 4.5912\n",
      "Epoch [1/5], Step [3069/5873], Loss: 4.1510\n",
      "Epoch [1/5], Step [3070/5873], Loss: 4.8941\n",
      "Epoch [1/5], Step [3071/5873], Loss: 5.7076\n",
      "Epoch [1/5], Step [3072/5873], Loss: 4.3564\n",
      "Epoch [1/5], Step [3073/5873], Loss: 4.4721\n",
      "Epoch [1/5], Step [3074/5873], Loss: 3.1216\n",
      "Epoch [1/5], Step [3075/5873], Loss: 1.9366\n",
      "Epoch [1/5], Step [3076/5873], Loss: 1.9833\n",
      "Epoch [1/5], Step [3077/5873], Loss: 2.4831\n",
      "Epoch [1/5], Step [3078/5873], Loss: 3.0312\n",
      "Epoch [1/5], Step [3079/5873], Loss: 3.8385\n",
      "Epoch [1/5], Step [3080/5873], Loss: 4.4695\n",
      "Epoch [1/5], Step [3081/5873], Loss: 1.8450\n",
      "Epoch [1/5], Step [3082/5873], Loss: 3.2322\n",
      "Epoch [1/5], Step [3083/5873], Loss: 4.2207\n",
      "Epoch [1/5], Step [3084/5873], Loss: 3.6230\n",
      "Epoch [1/5], Step [3085/5873], Loss: 4.4716\n",
      "Epoch [1/5], Step [3086/5873], Loss: 1.5319\n",
      "Epoch [1/5], Step [3087/5873], Loss: 1.6425\n",
      "Epoch [1/5], Step [3088/5873], Loss: 5.5867\n",
      "Epoch [1/5], Step [3089/5873], Loss: 3.9032\n",
      "Epoch [1/5], Step [3090/5873], Loss: 3.5289\n",
      "Epoch [1/5], Step [3091/5873], Loss: 3.0574\n",
      "Epoch [1/5], Step [3092/5873], Loss: 4.4429\n",
      "Epoch [1/5], Step [3093/5873], Loss: 2.7775\n",
      "Epoch [1/5], Step [3094/5873], Loss: 2.4339\n",
      "Epoch [1/5], Step [3095/5873], Loss: 5.0249\n",
      "Epoch [1/5], Step [3096/5873], Loss: 3.5420\n",
      "Epoch [1/5], Step [3097/5873], Loss: 6.7399\n",
      "Epoch [1/5], Step [3098/5873], Loss: 4.2035\n",
      "Epoch [1/5], Step [3099/5873], Loss: 4.5709\n",
      "Epoch [1/5], Step [3100/5873], Loss: 4.8382\n",
      "Epoch [1/5], Step [3101/5873], Loss: 3.0345\n",
      "Epoch [1/5], Step [3102/5873], Loss: 4.4345\n",
      "Epoch [1/5], Step [3103/5873], Loss: 4.5966\n",
      "Epoch [1/5], Step [3104/5873], Loss: 4.7717\n",
      "Epoch [1/5], Step [3105/5873], Loss: 3.7223\n",
      "Epoch [1/5], Step [3106/5873], Loss: 3.8327\n",
      "Epoch [1/5], Step [3107/5873], Loss: 5.2806\n",
      "Epoch [1/5], Step [3108/5873], Loss: 3.6906\n",
      "Epoch [1/5], Step [3109/5873], Loss: 4.2451\n",
      "Epoch [1/5], Step [3110/5873], Loss: 2.3422\n",
      "Epoch [1/5], Step [3111/5873], Loss: 5.1378\n",
      "Epoch [1/5], Step [3112/5873], Loss: 3.0816\n",
      "Epoch [1/5], Step [3113/5873], Loss: 2.6099\n",
      "Epoch [1/5], Step [3114/5873], Loss: 4.1238\n",
      "Epoch [1/5], Step [3115/5873], Loss: 3.0420\n",
      "Epoch [1/5], Step [3116/5873], Loss: 4.4792\n",
      "Epoch [1/5], Step [3117/5873], Loss: 3.7235\n",
      "Epoch [1/5], Step [3118/5873], Loss: 5.2409\n",
      "Epoch [1/5], Step [3119/5873], Loss: 4.7092\n",
      "Epoch [1/5], Step [3120/5873], Loss: 6.5063\n",
      "Epoch [1/5], Step [3121/5873], Loss: 3.1320\n",
      "Epoch [1/5], Step [3122/5873], Loss: 4.0222\n",
      "Epoch [1/5], Step [3123/5873], Loss: 5.5048\n",
      "Epoch [1/5], Step [3124/5873], Loss: 1.5741\n",
      "Epoch [1/5], Step [3125/5873], Loss: 3.3987\n",
      "Epoch [1/5], Step [3126/5873], Loss: 2.6955\n",
      "Epoch [1/5], Step [3127/5873], Loss: 3.5636\n",
      "Epoch [1/5], Step [3128/5873], Loss: 3.1800\n",
      "Epoch [1/5], Step [3129/5873], Loss: 3.4919\n",
      "Epoch [1/5], Step [3130/5873], Loss: 2.4958\n",
      "Epoch [1/5], Step [3131/5873], Loss: 4.2339\n",
      "Epoch [1/5], Step [3132/5873], Loss: 2.9215\n",
      "Epoch [1/5], Step [3133/5873], Loss: 2.7130\n",
      "Epoch [1/5], Step [3134/5873], Loss: 3.1998\n",
      "Epoch [1/5], Step [3135/5873], Loss: 3.1557\n",
      "Epoch [1/5], Step [3136/5873], Loss: 1.3203\n",
      "Epoch [1/5], Step [3137/5873], Loss: 4.7382\n",
      "Epoch [1/5], Step [3138/5873], Loss: 4.9162\n",
      "Epoch [1/5], Step [3139/5873], Loss: 4.1911\n",
      "Epoch [1/5], Step [3140/5873], Loss: 1.3748\n",
      "Epoch [1/5], Step [3141/5873], Loss: 3.4107\n",
      "Epoch [1/5], Step [3142/5873], Loss: 4.5707\n",
      "Epoch [1/5], Step [3143/5873], Loss: 4.2352\n",
      "Epoch [1/5], Step [3144/5873], Loss: 1.9749\n",
      "Epoch [1/5], Step [3145/5873], Loss: 3.8477\n",
      "Epoch [1/5], Step [3146/5873], Loss: 4.2856\n",
      "Epoch [1/5], Step [3147/5873], Loss: 3.0964\n",
      "Epoch [1/5], Step [3148/5873], Loss: 4.2890\n",
      "Epoch [1/5], Step [3149/5873], Loss: 3.9845\n",
      "Epoch [1/5], Step [3150/5873], Loss: 3.4013\n",
      "Epoch [1/5], Step [3151/5873], Loss: 3.3274\n",
      "Epoch [1/5], Step [3152/5873], Loss: 1.9076\n",
      "Epoch [1/5], Step [3153/5873], Loss: 5.3660\n",
      "Epoch [1/5], Step [3154/5873], Loss: 5.5597\n",
      "Epoch [1/5], Step [3155/5873], Loss: 3.8030\n",
      "Epoch [1/5], Step [3156/5873], Loss: 4.6374\n",
      "Epoch [1/5], Step [3157/5873], Loss: 1.3124\n",
      "Epoch [1/5], Step [3158/5873], Loss: 4.9814\n",
      "Epoch [1/5], Step [3159/5873], Loss: 6.1198\n",
      "Epoch [1/5], Step [3160/5873], Loss: 2.4586\n",
      "Epoch [1/5], Step [3161/5873], Loss: 4.1721\n",
      "Epoch [1/5], Step [3162/5873], Loss: 5.1694\n",
      "Epoch [1/5], Step [3163/5873], Loss: 2.7089\n",
      "Epoch [1/5], Step [3164/5873], Loss: 1.9084\n",
      "Epoch [1/5], Step [3165/5873], Loss: 4.2522\n",
      "Epoch [1/5], Step [3166/5873], Loss: 3.7450\n",
      "Epoch [1/5], Step [3167/5873], Loss: 5.0978\n",
      "Epoch [1/5], Step [3168/5873], Loss: 3.2338\n",
      "Epoch [1/5], Step [3169/5873], Loss: 1.7109\n",
      "Epoch [1/5], Step [3170/5873], Loss: 1.4146\n",
      "Epoch [1/5], Step [3171/5873], Loss: 3.1887\n",
      "Epoch [1/5], Step [3172/5873], Loss: 2.6721\n",
      "Epoch [1/5], Step [3173/5873], Loss: 3.0149\n",
      "Epoch [1/5], Step [3174/5873], Loss: 3.8973\n",
      "Epoch [1/5], Step [3175/5873], Loss: 4.9904\n",
      "Epoch [1/5], Step [3176/5873], Loss: 4.0039\n",
      "Epoch [1/5], Step [3177/5873], Loss: 1.5981\n",
      "Epoch [1/5], Step [3178/5873], Loss: 4.9491\n",
      "Epoch [1/5], Step [3179/5873], Loss: 5.8836\n",
      "Epoch [1/5], Step [3180/5873], Loss: 6.0350\n",
      "Epoch [1/5], Step [3181/5873], Loss: 5.5703\n",
      "Epoch [1/5], Step [3182/5873], Loss: 3.3980\n",
      "Epoch [1/5], Step [3183/5873], Loss: 0.6471\n",
      "Epoch [1/5], Step [3184/5873], Loss: 3.1005\n",
      "Epoch [1/5], Step [3185/5873], Loss: 3.9325\n",
      "Epoch [1/5], Step [3186/5873], Loss: 3.2058\n",
      "Epoch [1/5], Step [3187/5873], Loss: 3.7960\n",
      "Epoch [1/5], Step [3188/5873], Loss: 4.8774\n",
      "Epoch [1/5], Step [3189/5873], Loss: 5.1295\n",
      "Epoch [1/5], Step [3190/5873], Loss: 2.3999\n",
      "Epoch [1/5], Step [3191/5873], Loss: 3.8702\n",
      "Epoch [1/5], Step [3192/5873], Loss: 6.1462\n",
      "Epoch [1/5], Step [3193/5873], Loss: 4.6032\n",
      "Epoch [1/5], Step [3194/5873], Loss: 2.3031\n",
      "Epoch [1/5], Step [3195/5873], Loss: 2.4265\n",
      "Epoch [1/5], Step [3196/5873], Loss: 4.0471\n",
      "Epoch [1/5], Step [3197/5873], Loss: 3.9233\n",
      "Epoch [1/5], Step [3198/5873], Loss: 4.0591\n",
      "Epoch [1/5], Step [3199/5873], Loss: 2.2521\n",
      "Epoch [1/5], Step [3200/5873], Loss: 5.4890\n",
      "Epoch [1/5], Step [3201/5873], Loss: 3.7354\n",
      "Epoch [1/5], Step [3202/5873], Loss: 4.0080\n",
      "Epoch [1/5], Step [3203/5873], Loss: 4.2883\n",
      "Epoch [1/5], Step [3204/5873], Loss: 4.6502\n",
      "Epoch [1/5], Step [3205/5873], Loss: 3.8604\n",
      "Epoch [1/5], Step [3206/5873], Loss: 3.4461\n",
      "Epoch [1/5], Step [3207/5873], Loss: 4.5021\n",
      "Epoch [1/5], Step [3208/5873], Loss: 3.8433\n",
      "Epoch [1/5], Step [3209/5873], Loss: 4.2055\n",
      "Epoch [1/5], Step [3210/5873], Loss: 4.0358\n",
      "Epoch [1/5], Step [3211/5873], Loss: 3.3498\n",
      "Epoch [1/5], Step [3212/5873], Loss: 3.3408\n",
      "Epoch [1/5], Step [3213/5873], Loss: 3.6721\n",
      "Epoch [1/5], Step [3214/5873], Loss: 3.3009\n",
      "Epoch [1/5], Step [3215/5873], Loss: 3.7669\n",
      "Epoch [1/5], Step [3216/5873], Loss: 5.6261\n",
      "Epoch [1/5], Step [3217/5873], Loss: 3.4539\n",
      "Epoch [1/5], Step [3218/5873], Loss: 4.3249\n",
      "Epoch [1/5], Step [3219/5873], Loss: 5.5606\n",
      "Epoch [1/5], Step [3220/5873], Loss: 4.7541\n",
      "Epoch [1/5], Step [3221/5873], Loss: 5.3385\n",
      "Epoch [1/5], Step [3222/5873], Loss: 3.5339\n",
      "Epoch [1/5], Step [3223/5873], Loss: 3.7475\n",
      "Epoch [1/5], Step [3224/5873], Loss: 5.0106\n",
      "Epoch [1/5], Step [3225/5873], Loss: 4.5395\n",
      "Epoch [1/5], Step [3226/5873], Loss: 5.1963\n",
      "Epoch [1/5], Step [3227/5873], Loss: 4.3251\n",
      "Epoch [1/5], Step [3228/5873], Loss: 3.9834\n",
      "Epoch [1/5], Step [3229/5873], Loss: 4.7386\n",
      "Epoch [1/5], Step [3230/5873], Loss: 4.4757\n",
      "Epoch [1/5], Step [3231/5873], Loss: 3.4797\n",
      "Epoch [1/5], Step [3232/5873], Loss: 4.4159\n",
      "Epoch [1/5], Step [3233/5873], Loss: 3.0834\n",
      "Epoch [1/5], Step [3234/5873], Loss: 4.0657\n",
      "Epoch [1/5], Step [3235/5873], Loss: 3.8781\n",
      "Epoch [1/5], Step [3236/5873], Loss: 4.3471\n",
      "Epoch [1/5], Step [3237/5873], Loss: 3.7356\n",
      "Epoch [1/5], Step [3238/5873], Loss: 3.5143\n",
      "Epoch [1/5], Step [3239/5873], Loss: 4.0720\n",
      "Epoch [1/5], Step [3240/5873], Loss: 4.6500\n",
      "Epoch [1/5], Step [3241/5873], Loss: 2.9954\n",
      "Epoch [1/5], Step [3242/5873], Loss: 4.7581\n",
      "Epoch [1/5], Step [3243/5873], Loss: 3.2007\n",
      "Epoch [1/5], Step [3244/5873], Loss: 3.0313\n",
      "Epoch [1/5], Step [3245/5873], Loss: 4.0084\n",
      "Epoch [1/5], Step [3246/5873], Loss: 4.1878\n",
      "Epoch [1/5], Step [3247/5873], Loss: 3.5330\n",
      "Epoch [1/5], Step [3248/5873], Loss: 4.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3249/5873], Loss: 4.0047\n",
      "Epoch [1/5], Step [3250/5873], Loss: 3.9411\n",
      "Epoch [1/5], Step [3251/5873], Loss: 4.7558\n",
      "Epoch [1/5], Step [3252/5873], Loss: 4.9050\n",
      "Epoch [1/5], Step [3253/5873], Loss: 5.1931\n",
      "Epoch [1/5], Step [3254/5873], Loss: 5.0341\n",
      "Epoch [1/5], Step [3255/5873], Loss: 3.6801\n",
      "Epoch [1/5], Step [3256/5873], Loss: 2.2045\n",
      "Epoch [1/5], Step [3257/5873], Loss: 3.6656\n",
      "Epoch [1/5], Step [3258/5873], Loss: 4.4599\n",
      "Epoch [1/5], Step [3259/5873], Loss: 5.2668\n",
      "Epoch [1/5], Step [3260/5873], Loss: 3.1419\n",
      "Epoch [1/5], Step [3261/5873], Loss: 4.6536\n",
      "Epoch [1/5], Step [3262/5873], Loss: 3.2613\n",
      "Epoch [1/5], Step [3263/5873], Loss: 4.3724\n",
      "Epoch [1/5], Step [3264/5873], Loss: 2.1553\n",
      "Epoch [1/5], Step [3265/5873], Loss: 4.4293\n",
      "Epoch [1/5], Step [3266/5873], Loss: 6.1283\n",
      "Epoch [1/5], Step [3267/5873], Loss: 4.4967\n",
      "Epoch [1/5], Step [3268/5873], Loss: 4.6781\n",
      "Epoch [1/5], Step [3269/5873], Loss: 4.2199\n",
      "Epoch [1/5], Step [3270/5873], Loss: 4.2496\n",
      "Epoch [1/5], Step [3271/5873], Loss: 6.0097\n",
      "Epoch [1/5], Step [3272/5873], Loss: 1.3267\n",
      "Epoch [1/5], Step [3273/5873], Loss: 1.4685\n",
      "Epoch [1/5], Step [3274/5873], Loss: 1.7965\n",
      "Epoch [1/5], Step [3275/5873], Loss: 4.7254\n",
      "Epoch [1/5], Step [3276/5873], Loss: 2.3180\n",
      "Epoch [1/5], Step [3277/5873], Loss: 4.7339\n",
      "Epoch [1/5], Step [3278/5873], Loss: 4.8880\n",
      "Epoch [1/5], Step [3279/5873], Loss: 4.4387\n",
      "Epoch [1/5], Step [3280/5873], Loss: 4.4422\n",
      "Epoch [1/5], Step [3281/5873], Loss: 3.6069\n",
      "Epoch [1/5], Step [3282/5873], Loss: 4.7652\n",
      "Epoch [1/5], Step [3283/5873], Loss: 4.0769\n",
      "Epoch [1/5], Step [3284/5873], Loss: 5.5407\n",
      "Epoch [1/5], Step [3285/5873], Loss: 3.8905\n",
      "Epoch [1/5], Step [3286/5873], Loss: 3.8429\n",
      "Epoch [1/5], Step [3287/5873], Loss: 3.2394\n",
      "Epoch [1/5], Step [3288/5873], Loss: 3.8904\n",
      "Epoch [1/5], Step [3289/5873], Loss: 3.6969\n",
      "Epoch [1/5], Step [3290/5873], Loss: 3.3713\n",
      "Epoch [1/5], Step [3291/5873], Loss: 3.6609\n",
      "Epoch [1/5], Step [3292/5873], Loss: 3.9592\n",
      "Epoch [1/5], Step [3293/5873], Loss: 3.8913\n",
      "Epoch [1/5], Step [3294/5873], Loss: 3.0867\n",
      "Epoch [1/5], Step [3295/5873], Loss: 2.5584\n",
      "Epoch [1/5], Step [3296/5873], Loss: 2.9166\n",
      "Epoch [1/5], Step [3297/5873], Loss: 2.9831\n",
      "Epoch [1/5], Step [3298/5873], Loss: 2.5611\n",
      "Epoch [1/5], Step [3299/5873], Loss: 3.5664\n",
      "Epoch [1/5], Step [3300/5873], Loss: 2.8653\n",
      "Epoch [1/5], Step [3301/5873], Loss: 4.2987\n",
      "Epoch [1/5], Step [3302/5873], Loss: 3.8588\n",
      "Epoch [1/5], Step [3303/5873], Loss: 2.6358\n",
      "Epoch [1/5], Step [3304/5873], Loss: 3.5175\n",
      "Epoch [1/5], Step [3305/5873], Loss: 3.3035\n",
      "Epoch [1/5], Step [3306/5873], Loss: 3.6748\n",
      "Epoch [1/5], Step [3307/5873], Loss: 1.6239\n",
      "Epoch [1/5], Step [3308/5873], Loss: 4.0517\n",
      "Epoch [1/5], Step [3309/5873], Loss: 5.4355\n",
      "Epoch [1/5], Step [3310/5873], Loss: 4.5020\n",
      "Epoch [1/5], Step [3311/5873], Loss: 2.8165\n",
      "Epoch [1/5], Step [3312/5873], Loss: 1.1491\n",
      "Epoch [1/5], Step [3313/5873], Loss: 2.6388\n",
      "Epoch [1/5], Step [3314/5873], Loss: 6.8841\n",
      "Epoch [1/5], Step [3315/5873], Loss: 3.8779\n",
      "Epoch [1/5], Step [3316/5873], Loss: 3.3109\n",
      "Epoch [1/5], Step [3317/5873], Loss: 1.3411\n",
      "Epoch [1/5], Step [3318/5873], Loss: 1.4967\n",
      "Epoch [1/5], Step [3319/5873], Loss: 4.3913\n",
      "Epoch [1/5], Step [3320/5873], Loss: 3.3993\n",
      "Epoch [1/5], Step [3321/5873], Loss: 1.1028\n",
      "Epoch [1/5], Step [3322/5873], Loss: 4.2061\n",
      "Epoch [1/5], Step [3323/5873], Loss: 4.3197\n",
      "Epoch [1/5], Step [3324/5873], Loss: 3.6474\n",
      "Epoch [1/5], Step [3325/5873], Loss: 3.3962\n",
      "Epoch [1/5], Step [3326/5873], Loss: 3.8298\n",
      "Epoch [1/5], Step [3327/5873], Loss: 3.3148\n",
      "Epoch [1/5], Step [3328/5873], Loss: 4.9561\n",
      "Epoch [1/5], Step [3329/5873], Loss: 3.8574\n",
      "Epoch [1/5], Step [3330/5873], Loss: 5.2062\n",
      "Epoch [1/5], Step [3331/5873], Loss: 4.6805\n",
      "Epoch [1/5], Step [3332/5873], Loss: 5.9728\n",
      "Epoch [1/5], Step [3333/5873], Loss: 4.7943\n",
      "Epoch [1/5], Step [3334/5873], Loss: 4.2081\n",
      "Epoch [1/5], Step [3335/5873], Loss: 3.6722\n",
      "Epoch [1/5], Step [3336/5873], Loss: 5.3453\n",
      "Epoch [1/5], Step [3337/5873], Loss: 5.1622\n",
      "Epoch [1/5], Step [3338/5873], Loss: 4.2459\n",
      "Epoch [1/5], Step [3339/5873], Loss: 1.9953\n",
      "Epoch [1/5], Step [3340/5873], Loss: 3.7509\n",
      "Epoch [1/5], Step [3341/5873], Loss: 2.6837\n",
      "Epoch [1/5], Step [3342/5873], Loss: 5.0269\n",
      "Epoch [1/5], Step [3343/5873], Loss: 2.9185\n",
      "Epoch [1/5], Step [3344/5873], Loss: 3.2643\n",
      "Epoch [1/5], Step [3345/5873], Loss: 3.2193\n",
      "Epoch [1/5], Step [3346/5873], Loss: 5.3861\n",
      "Epoch [1/5], Step [3347/5873], Loss: 4.0385\n",
      "Epoch [1/5], Step [3348/5873], Loss: 4.9194\n",
      "Epoch [1/5], Step [3349/5873], Loss: 4.2760\n",
      "Epoch [1/5], Step [3350/5873], Loss: 4.3223\n",
      "Epoch [1/5], Step [3351/5873], Loss: 4.8300\n",
      "Epoch [1/5], Step [3352/5873], Loss: 3.2529\n",
      "Epoch [1/5], Step [3353/5873], Loss: 3.3711\n",
      "Epoch [1/5], Step [3354/5873], Loss: 5.1938\n",
      "Epoch [1/5], Step [3355/5873], Loss: 3.6514\n",
      "Epoch [1/5], Step [3356/5873], Loss: 3.5424\n",
      "Epoch [1/5], Step [3357/5873], Loss: 4.1939\n",
      "Epoch [1/5], Step [3358/5873], Loss: 2.4370\n",
      "Epoch [1/5], Step [3359/5873], Loss: 3.6893\n",
      "Epoch [1/5], Step [3360/5873], Loss: 4.3061\n",
      "Epoch [1/5], Step [3361/5873], Loss: 3.8256\n",
      "Epoch [1/5], Step [3362/5873], Loss: 4.0393\n",
      "Epoch [1/5], Step [3363/5873], Loss: 2.5041\n",
      "Epoch [1/5], Step [3364/5873], Loss: 4.3010\n",
      "Epoch [1/5], Step [3365/5873], Loss: 5.1443\n",
      "Epoch [1/5], Step [3366/5873], Loss: 1.9280\n",
      "Epoch [1/5], Step [3367/5873], Loss: 4.6519\n",
      "Epoch [1/5], Step [3368/5873], Loss: 3.2675\n",
      "Epoch [1/5], Step [3369/5873], Loss: 5.6232\n",
      "Epoch [1/5], Step [3370/5873], Loss: 1.7443\n",
      "Epoch [1/5], Step [3371/5873], Loss: 4.8636\n",
      "Epoch [1/5], Step [3372/5873], Loss: 3.2091\n",
      "Epoch [1/5], Step [3373/5873], Loss: 3.9480\n",
      "Epoch [1/5], Step [3374/5873], Loss: 5.5844\n",
      "Epoch [1/5], Step [3375/5873], Loss: 3.1411\n",
      "Epoch [1/5], Step [3376/5873], Loss: 3.7049\n",
      "Epoch [1/5], Step [3377/5873], Loss: 5.4311\n",
      "Epoch [1/5], Step [3378/5873], Loss: 3.7885\n",
      "Epoch [1/5], Step [3379/5873], Loss: 4.1382\n",
      "Epoch [1/5], Step [3380/5873], Loss: 3.4929\n",
      "Epoch [1/5], Step [3381/5873], Loss: 3.6942\n",
      "Epoch [1/5], Step [3382/5873], Loss: 2.4377\n",
      "Epoch [1/5], Step [3383/5873], Loss: 4.2984\n",
      "Epoch [1/5], Step [3384/5873], Loss: 4.7790\n",
      "Epoch [1/5], Step [3385/5873], Loss: 2.8562\n",
      "Epoch [1/5], Step [3386/5873], Loss: 3.3068\n",
      "Epoch [1/5], Step [3387/5873], Loss: 3.5563\n",
      "Epoch [1/5], Step [3388/5873], Loss: 1.4388\n",
      "Epoch [1/5], Step [3389/5873], Loss: 1.6365\n",
      "Epoch [1/5], Step [3390/5873], Loss: 4.3685\n",
      "Epoch [1/5], Step [3391/5873], Loss: 3.9346\n",
      "Epoch [1/5], Step [3392/5873], Loss: 3.0917\n",
      "Epoch [1/5], Step [3393/5873], Loss: 4.6230\n",
      "Epoch [1/5], Step [3394/5873], Loss: 3.7408\n",
      "Epoch [1/5], Step [3395/5873], Loss: 3.9045\n",
      "Epoch [1/5], Step [3396/5873], Loss: 4.5576\n",
      "Epoch [1/5], Step [3397/5873], Loss: 4.3741\n",
      "Epoch [1/5], Step [3398/5873], Loss: 3.5733\n",
      "Epoch [1/5], Step [3399/5873], Loss: 4.3445\n",
      "Epoch [1/5], Step [3400/5873], Loss: 6.0415\n",
      "Epoch [1/5], Step [3401/5873], Loss: 5.9231\n",
      "Epoch [1/5], Step [3402/5873], Loss: 5.6631\n",
      "Epoch [1/5], Step [3403/5873], Loss: 6.3989\n",
      "Epoch [1/5], Step [3404/5873], Loss: 6.8698\n",
      "Epoch [1/5], Step [3405/5873], Loss: 5.4748\n",
      "Epoch [1/5], Step [3406/5873], Loss: 5.5646\n",
      "Epoch [1/5], Step [3407/5873], Loss: 5.0301\n",
      "Epoch [1/5], Step [3408/5873], Loss: 5.7071\n",
      "Epoch [1/5], Step [3409/5873], Loss: 3.6914\n",
      "Epoch [1/5], Step [3410/5873], Loss: 5.2761\n",
      "Epoch [1/5], Step [3411/5873], Loss: 5.9793\n",
      "Epoch [1/5], Step [3412/5873], Loss: 5.0748\n",
      "Epoch [1/5], Step [3413/5873], Loss: 5.3689\n",
      "Epoch [1/5], Step [3414/5873], Loss: 5.3683\n",
      "Epoch [1/5], Step [3415/5873], Loss: 4.6982\n",
      "Epoch [1/5], Step [3416/5873], Loss: 2.9603\n",
      "Epoch [1/5], Step [3417/5873], Loss: 4.7788\n",
      "Epoch [1/5], Step [3418/5873], Loss: 4.9344\n",
      "Epoch [1/5], Step [3419/5873], Loss: 4.9659\n",
      "Epoch [1/5], Step [3420/5873], Loss: 3.9886\n",
      "Epoch [1/5], Step [3421/5873], Loss: 5.5611\n",
      "Epoch [1/5], Step [3422/5873], Loss: 4.1115\n",
      "Epoch [1/5], Step [3423/5873], Loss: 2.7499\n",
      "Epoch [1/5], Step [3424/5873], Loss: 4.3548\n",
      "Epoch [1/5], Step [3425/5873], Loss: 3.7594\n",
      "Epoch [1/5], Step [3426/5873], Loss: 3.5934\n",
      "Epoch [1/5], Step [3427/5873], Loss: 5.4713\n",
      "Epoch [1/5], Step [3428/5873], Loss: 5.1934\n",
      "Epoch [1/5], Step [3429/5873], Loss: 3.3722\n",
      "Epoch [1/5], Step [3430/5873], Loss: 4.1662\n",
      "Epoch [1/5], Step [3431/5873], Loss: 3.4924\n",
      "Epoch [1/5], Step [3432/5873], Loss: 6.8242\n",
      "Epoch [1/5], Step [3433/5873], Loss: 4.9814\n",
      "Epoch [1/5], Step [3434/5873], Loss: 5.4651\n",
      "Epoch [1/5], Step [3435/5873], Loss: 6.1346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3436/5873], Loss: 3.6104\n",
      "Epoch [1/5], Step [3437/5873], Loss: 4.2734\n",
      "Epoch [1/5], Step [3438/5873], Loss: 4.4003\n",
      "Epoch [1/5], Step [3439/5873], Loss: 3.9826\n",
      "Epoch [1/5], Step [3440/5873], Loss: 4.8654\n",
      "Epoch [1/5], Step [3441/5873], Loss: 5.2558\n",
      "Epoch [1/5], Step [3442/5873], Loss: 4.6755\n",
      "Epoch [1/5], Step [3443/5873], Loss: 4.5362\n",
      "Epoch [1/5], Step [3444/5873], Loss: 4.4084\n",
      "Epoch [1/5], Step [3445/5873], Loss: 4.1425\n",
      "Epoch [1/5], Step [3446/5873], Loss: 5.2198\n",
      "Epoch [1/5], Step [3447/5873], Loss: 3.8162\n",
      "Epoch [1/5], Step [3448/5873], Loss: 4.5005\n",
      "Epoch [1/5], Step [3449/5873], Loss: 3.6128\n",
      "Epoch [1/5], Step [3450/5873], Loss: 3.3645\n",
      "Epoch [1/5], Step [3451/5873], Loss: 4.6204\n",
      "Epoch [1/5], Step [3452/5873], Loss: 3.5737\n",
      "Epoch [1/5], Step [3453/5873], Loss: 5.9140\n",
      "Epoch [1/5], Step [3454/5873], Loss: 3.8709\n",
      "Epoch [1/5], Step [3455/5873], Loss: 4.5579\n",
      "Epoch [1/5], Step [3456/5873], Loss: 4.4064\n",
      "Epoch [1/5], Step [3457/5873], Loss: 4.7222\n",
      "Epoch [1/5], Step [3458/5873], Loss: 2.6917\n",
      "Epoch [1/5], Step [3459/5873], Loss: 4.3713\n",
      "Epoch [1/5], Step [3460/5873], Loss: 4.8580\n",
      "Epoch [1/5], Step [3461/5873], Loss: 3.0101\n",
      "Epoch [1/5], Step [3462/5873], Loss: 4.5585\n",
      "Epoch [1/5], Step [3463/5873], Loss: 4.0730\n",
      "Epoch [1/5], Step [3464/5873], Loss: 3.3836\n",
      "Epoch [1/5], Step [3465/5873], Loss: 3.1414\n",
      "Epoch [1/5], Step [3466/5873], Loss: 4.2571\n",
      "Epoch [1/5], Step [3467/5873], Loss: 4.8069\n",
      "Epoch [1/5], Step [3468/5873], Loss: 4.8084\n",
      "Epoch [1/5], Step [3469/5873], Loss: 3.7217\n",
      "Epoch [1/5], Step [3470/5873], Loss: 4.2665\n",
      "Epoch [1/5], Step [3471/5873], Loss: 3.9696\n",
      "Epoch [1/5], Step [3472/5873], Loss: 5.1542\n",
      "Epoch [1/5], Step [3473/5873], Loss: 3.7456\n",
      "Epoch [1/5], Step [3474/5873], Loss: 4.6887\n",
      "Epoch [1/5], Step [3475/5873], Loss: 4.7221\n",
      "Epoch [1/5], Step [3476/5873], Loss: 3.1394\n",
      "Epoch [1/5], Step [3477/5873], Loss: 4.3475\n",
      "Epoch [1/5], Step [3478/5873], Loss: 5.7317\n",
      "Epoch [1/5], Step [3479/5873], Loss: 2.8702\n",
      "Epoch [1/5], Step [3480/5873], Loss: 3.5039\n",
      "Epoch [1/5], Step [3481/5873], Loss: 5.1239\n",
      "Epoch [1/5], Step [3482/5873], Loss: 4.9566\n",
      "Epoch [1/5], Step [3483/5873], Loss: 3.3269\n",
      "Epoch [1/5], Step [3484/5873], Loss: 1.9491\n",
      "Epoch [1/5], Step [3485/5873], Loss: 5.3058\n",
      "Epoch [1/5], Step [3486/5873], Loss: 5.2897\n",
      "Epoch [1/5], Step [3487/5873], Loss: 3.4293\n",
      "Epoch [1/5], Step [3488/5873], Loss: 2.2142\n",
      "Epoch [1/5], Step [3489/5873], Loss: 4.5406\n",
      "Epoch [1/5], Step [3490/5873], Loss: 5.4500\n",
      "Epoch [1/5], Step [3491/5873], Loss: 3.7198\n",
      "Epoch [1/5], Step [3492/5873], Loss: 4.5748\n",
      "Epoch [1/5], Step [3493/5873], Loss: 4.6747\n",
      "Epoch [1/5], Step [3494/5873], Loss: 2.1110\n",
      "Epoch [1/5], Step [3495/5873], Loss: 4.9693\n",
      "Epoch [1/5], Step [3496/5873], Loss: 3.0908\n",
      "Epoch [1/5], Step [3497/5873], Loss: 3.5627\n",
      "Epoch [1/5], Step [3498/5873], Loss: 2.5275\n",
      "Epoch [1/5], Step [3499/5873], Loss: 4.8016\n",
      "Epoch [1/5], Step [3500/5873], Loss: 5.6311\n",
      "Epoch [1/5], Step [3501/5873], Loss: 4.9837\n",
      "Epoch [1/5], Step [3502/5873], Loss: 7.1772\n",
      "Epoch [1/5], Step [3503/5873], Loss: 5.7496\n",
      "Epoch [1/5], Step [3504/5873], Loss: 5.0986\n",
      "Epoch [1/5], Step [3505/5873], Loss: 4.1834\n",
      "Epoch [1/5], Step [3506/5873], Loss: 5.0099\n",
      "Epoch [1/5], Step [3507/5873], Loss: 3.9355\n",
      "Epoch [1/5], Step [3508/5873], Loss: 4.8549\n",
      "Epoch [1/5], Step [3509/5873], Loss: 3.1308\n",
      "Epoch [1/5], Step [3510/5873], Loss: 4.0554\n",
      "Epoch [1/5], Step [3511/5873], Loss: 5.0964\n",
      "Epoch [1/5], Step [3512/5873], Loss: 4.2429\n",
      "Epoch [1/5], Step [3513/5873], Loss: 3.7074\n",
      "Epoch [1/5], Step [3514/5873], Loss: 4.2119\n",
      "Epoch [1/5], Step [3515/5873], Loss: 4.1380\n",
      "Epoch [1/5], Step [3516/5873], Loss: 4.0589\n",
      "Epoch [1/5], Step [3517/5873], Loss: 4.1203\n",
      "Epoch [1/5], Step [3518/5873], Loss: 4.7103\n",
      "Epoch [1/5], Step [3519/5873], Loss: 5.6159\n",
      "Epoch [1/5], Step [3520/5873], Loss: 5.1115\n",
      "Epoch [1/5], Step [3521/5873], Loss: 4.2162\n",
      "Epoch [1/5], Step [3522/5873], Loss: 4.7608\n",
      "Epoch [1/5], Step [3523/5873], Loss: 2.9706\n",
      "Epoch [1/5], Step [3524/5873], Loss: 3.3726\n",
      "Epoch [1/5], Step [3525/5873], Loss: 4.4211\n",
      "Epoch [1/5], Step [3526/5873], Loss: 4.1056\n",
      "Epoch [1/5], Step [3527/5873], Loss: 3.7877\n",
      "Epoch [1/5], Step [3528/5873], Loss: 3.6622\n",
      "Epoch [1/5], Step [3529/5873], Loss: 5.4116\n",
      "Epoch [1/5], Step [3530/5873], Loss: 4.4999\n",
      "Epoch [1/5], Step [3531/5873], Loss: 3.3366\n",
      "Epoch [1/5], Step [3532/5873], Loss: 3.4255\n",
      "Epoch [1/5], Step [3533/5873], Loss: 4.6510\n",
      "Epoch [1/5], Step [3534/5873], Loss: 3.5080\n",
      "Epoch [1/5], Step [3535/5873], Loss: 3.8518\n",
      "Epoch [1/5], Step [3536/5873], Loss: 2.4034\n",
      "Epoch [1/5], Step [3537/5873], Loss: 4.7437\n",
      "Epoch [1/5], Step [3538/5873], Loss: 2.0597\n",
      "Epoch [1/5], Step [3539/5873], Loss: 3.8883\n",
      "Epoch [1/5], Step [3540/5873], Loss: 4.3490\n",
      "Epoch [1/5], Step [3541/5873], Loss: 3.8915\n",
      "Epoch [1/5], Step [3542/5873], Loss: 3.6704\n",
      "Epoch [1/5], Step [3543/5873], Loss: 3.9045\n",
      "Epoch [1/5], Step [3544/5873], Loss: 4.3160\n",
      "Epoch [1/5], Step [3545/5873], Loss: 4.1798\n",
      "Epoch [1/5], Step [3546/5873], Loss: 1.6027\n",
      "Epoch [1/5], Step [3547/5873], Loss: 3.3781\n",
      "Epoch [1/5], Step [3548/5873], Loss: 4.3495\n",
      "Epoch [1/5], Step [3549/5873], Loss: 4.6388\n",
      "Epoch [1/5], Step [3550/5873], Loss: 4.3926\n",
      "Epoch [1/5], Step [3551/5873], Loss: 4.0313\n",
      "Epoch [1/5], Step [3552/5873], Loss: 1.8894\n",
      "Epoch [1/5], Step [3553/5873], Loss: 4.1970\n",
      "Epoch [1/5], Step [3554/5873], Loss: 3.8216\n",
      "Epoch [1/5], Step [3555/5873], Loss: 5.3076\n",
      "Epoch [1/5], Step [3556/5873], Loss: 5.1488\n",
      "Epoch [1/5], Step [3557/5873], Loss: 4.3079\n",
      "Epoch [1/5], Step [3558/5873], Loss: 4.6980\n",
      "Epoch [1/5], Step [3559/5873], Loss: 3.7455\n",
      "Epoch [1/5], Step [3560/5873], Loss: 3.7263\n",
      "Epoch [1/5], Step [3561/5873], Loss: 3.4190\n",
      "Epoch [1/5], Step [3562/5873], Loss: 5.6667\n",
      "Epoch [1/5], Step [3563/5873], Loss: 4.6618\n",
      "Epoch [1/5], Step [3564/5873], Loss: 3.5873\n",
      "Epoch [1/5], Step [3565/5873], Loss: 4.3964\n",
      "Epoch [1/5], Step [3566/5873], Loss: 3.9848\n",
      "Epoch [1/5], Step [3567/5873], Loss: 4.2907\n",
      "Epoch [1/5], Step [3568/5873], Loss: 4.4351\n",
      "Epoch [1/5], Step [3569/5873], Loss: 4.1586\n",
      "Epoch [1/5], Step [3570/5873], Loss: 4.0851\n",
      "Epoch [1/5], Step [3571/5873], Loss: 3.3743\n",
      "Epoch [1/5], Step [3572/5873], Loss: 3.5624\n",
      "Epoch [1/5], Step [3573/5873], Loss: 2.7188\n",
      "Epoch [1/5], Step [3574/5873], Loss: 4.5975\n",
      "Epoch [1/5], Step [3575/5873], Loss: 4.7345\n",
      "Epoch [1/5], Step [3576/5873], Loss: 3.9269\n",
      "Epoch [1/5], Step [3577/5873], Loss: 4.8356\n",
      "Epoch [1/5], Step [3578/5873], Loss: 5.0548\n",
      "Epoch [1/5], Step [3579/5873], Loss: 5.7313\n",
      "Epoch [1/5], Step [3580/5873], Loss: 4.0899\n",
      "Epoch [1/5], Step [3581/5873], Loss: 4.2513\n",
      "Epoch [1/5], Step [3582/5873], Loss: 4.2316\n",
      "Epoch [1/5], Step [3583/5873], Loss: 4.3066\n",
      "Epoch [1/5], Step [3584/5873], Loss: 2.6726\n",
      "Epoch [1/5], Step [3585/5873], Loss: 2.3599\n",
      "Epoch [1/5], Step [3586/5873], Loss: 4.1374\n",
      "Epoch [1/5], Step [3587/5873], Loss: 4.0288\n",
      "Epoch [1/5], Step [3588/5873], Loss: 3.1408\n",
      "Epoch [1/5], Step [3589/5873], Loss: 4.5794\n",
      "Epoch [1/5], Step [3590/5873], Loss: 3.1390\n",
      "Epoch [1/5], Step [3591/5873], Loss: 3.1213\n",
      "Epoch [1/5], Step [3592/5873], Loss: 1.6985\n",
      "Epoch [1/5], Step [3593/5873], Loss: 4.0404\n",
      "Epoch [1/5], Step [3594/5873], Loss: 2.5381\n",
      "Epoch [1/5], Step [3595/5873], Loss: 2.5900\n",
      "Epoch [1/5], Step [3596/5873], Loss: 4.8986\n",
      "Epoch [1/5], Step [3597/5873], Loss: 3.5666\n",
      "Epoch [1/5], Step [3598/5873], Loss: 3.4299\n",
      "Epoch [1/5], Step [3599/5873], Loss: 2.6172\n",
      "Epoch [1/5], Step [3600/5873], Loss: 6.1157\n",
      "Epoch [1/5], Step [3601/5873], Loss: 4.4129\n",
      "Epoch [1/5], Step [3602/5873], Loss: 3.7041\n",
      "Epoch [1/5], Step [3603/5873], Loss: 3.3577\n",
      "Epoch [1/5], Step [3604/5873], Loss: 3.0275\n",
      "Epoch [1/5], Step [3605/5873], Loss: 5.9082\n",
      "Epoch [1/5], Step [3606/5873], Loss: 3.6957\n",
      "Epoch [1/5], Step [3607/5873], Loss: 5.4927\n",
      "Epoch [1/5], Step [3608/5873], Loss: 3.2333\n",
      "Epoch [1/5], Step [3609/5873], Loss: 2.8600\n",
      "Epoch [1/5], Step [3610/5873], Loss: 4.5485\n",
      "Epoch [1/5], Step [3611/5873], Loss: 3.0277\n",
      "Epoch [1/5], Step [3612/5873], Loss: 2.9872\n",
      "Epoch [1/5], Step [3613/5873], Loss: 2.0608\n",
      "Epoch [1/5], Step [3614/5873], Loss: 3.4067\n",
      "Epoch [1/5], Step [3615/5873], Loss: 3.1554\n",
      "Epoch [1/5], Step [3616/5873], Loss: 3.0137\n",
      "Epoch [1/5], Step [3617/5873], Loss: 1.5300\n",
      "Epoch [1/5], Step [3618/5873], Loss: 5.8100\n",
      "Epoch [1/5], Step [3619/5873], Loss: 5.3487\n",
      "Epoch [1/5], Step [3620/5873], Loss: 6.6604\n",
      "Epoch [1/5], Step [3621/5873], Loss: 4.6114\n",
      "Epoch [1/5], Step [3622/5873], Loss: 3.6154\n",
      "Epoch [1/5], Step [3623/5873], Loss: 3.1464\n",
      "Epoch [1/5], Step [3624/5873], Loss: 4.0012\n",
      "Epoch [1/5], Step [3625/5873], Loss: 5.9207\n",
      "Epoch [1/5], Step [3626/5873], Loss: 3.7491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3627/5873], Loss: 5.6410\n",
      "Epoch [1/5], Step [3628/5873], Loss: 5.2190\n",
      "Epoch [1/5], Step [3629/5873], Loss: 4.2580\n",
      "Epoch [1/5], Step [3630/5873], Loss: 1.0485\n",
      "Epoch [1/5], Step [3631/5873], Loss: 4.8407\n",
      "Epoch [1/5], Step [3632/5873], Loss: 4.3205\n",
      "Epoch [1/5], Step [3633/5873], Loss: 4.9870\n",
      "Epoch [1/5], Step [3634/5873], Loss: 4.6723\n",
      "Epoch [1/5], Step [3635/5873], Loss: 5.2837\n",
      "Epoch [1/5], Step [3636/5873], Loss: 1.7218\n",
      "Epoch [1/5], Step [3637/5873], Loss: 1.5168\n",
      "Epoch [1/5], Step [3638/5873], Loss: 3.3852\n",
      "Epoch [1/5], Step [3639/5873], Loss: 3.3312\n",
      "Epoch [1/5], Step [3640/5873], Loss: 5.1929\n",
      "Epoch [1/5], Step [3641/5873], Loss: 4.2270\n",
      "Epoch [1/5], Step [3642/5873], Loss: 3.9361\n",
      "Epoch [1/5], Step [3643/5873], Loss: 5.0373\n",
      "Epoch [1/5], Step [3644/5873], Loss: 4.8655\n",
      "Epoch [1/5], Step [3645/5873], Loss: 3.7447\n",
      "Epoch [1/5], Step [3646/5873], Loss: 3.7902\n",
      "Epoch [1/5], Step [3647/5873], Loss: 5.0472\n",
      "Epoch [1/5], Step [3648/5873], Loss: 3.3389\n",
      "Epoch [1/5], Step [3649/5873], Loss: 2.3910\n",
      "Epoch [1/5], Step [3650/5873], Loss: 4.7529\n",
      "Epoch [1/5], Step [3651/5873], Loss: 3.0673\n",
      "Epoch [1/5], Step [3652/5873], Loss: 3.2591\n",
      "Epoch [1/5], Step [3653/5873], Loss: 2.9634\n",
      "Epoch [1/5], Step [3654/5873], Loss: 3.2825\n",
      "Epoch [1/5], Step [3655/5873], Loss: 4.3203\n",
      "Epoch [1/5], Step [3656/5873], Loss: 5.5869\n",
      "Epoch [1/5], Step [3657/5873], Loss: 3.6273\n",
      "Epoch [1/5], Step [3658/5873], Loss: 6.1148\n",
      "Epoch [1/5], Step [3659/5873], Loss: 4.1418\n",
      "Epoch [1/5], Step [3660/5873], Loss: 2.6168\n",
      "Epoch [1/5], Step [3661/5873], Loss: 5.4996\n",
      "Epoch [1/5], Step [3662/5873], Loss: 1.6555\n",
      "Epoch [1/5], Step [3663/5873], Loss: 4.1394\n",
      "Epoch [1/5], Step [3664/5873], Loss: 4.6439\n",
      "Epoch [1/5], Step [3665/5873], Loss: 4.1893\n",
      "Epoch [1/5], Step [3666/5873], Loss: 5.6384\n",
      "Epoch [1/5], Step [3667/5873], Loss: 3.7139\n",
      "Epoch [1/5], Step [3668/5873], Loss: 3.5057\n",
      "Epoch [1/5], Step [3669/5873], Loss: 2.3268\n",
      "Epoch [1/5], Step [3670/5873], Loss: 2.0081\n",
      "Epoch [1/5], Step [3671/5873], Loss: 3.4743\n",
      "Epoch [1/5], Step [3672/5873], Loss: 4.9549\n",
      "Epoch [1/5], Step [3673/5873], Loss: 4.9651\n",
      "Epoch [1/5], Step [3674/5873], Loss: 4.1996\n",
      "Epoch [1/5], Step [3675/5873], Loss: 1.7685\n",
      "Epoch [1/5], Step [3676/5873], Loss: 3.9681\n",
      "Epoch [1/5], Step [3677/5873], Loss: 4.9372\n",
      "Epoch [1/5], Step [3678/5873], Loss: 3.7870\n",
      "Epoch [1/5], Step [3679/5873], Loss: 2.6690\n",
      "Epoch [1/5], Step [3680/5873], Loss: 3.0502\n",
      "Epoch [1/5], Step [3681/5873], Loss: 4.2373\n",
      "Epoch [1/5], Step [3682/5873], Loss: 4.6311\n",
      "Epoch [1/5], Step [3683/5873], Loss: 3.9408\n",
      "Epoch [1/5], Step [3684/5873], Loss: 3.0911\n",
      "Epoch [1/5], Step [3685/5873], Loss: 2.4291\n",
      "Epoch [1/5], Step [3686/5873], Loss: 1.1041\n",
      "Epoch [1/5], Step [3687/5873], Loss: 5.0354\n",
      "Epoch [1/5], Step [3688/5873], Loss: 5.0497\n",
      "Epoch [1/5], Step [3689/5873], Loss: 4.1936\n",
      "Epoch [1/5], Step [3690/5873], Loss: 5.3524\n",
      "Epoch [1/5], Step [3691/5873], Loss: 5.1897\n",
      "Epoch [1/5], Step [3692/5873], Loss: 4.6589\n",
      "Epoch [1/5], Step [3693/5873], Loss: 4.4424\n",
      "Epoch [1/5], Step [3694/5873], Loss: 4.2749\n",
      "Epoch [1/5], Step [3695/5873], Loss: 3.2966\n",
      "Epoch [1/5], Step [3696/5873], Loss: 4.1467\n",
      "Epoch [1/5], Step [3697/5873], Loss: 3.3423\n",
      "Epoch [1/5], Step [3698/5873], Loss: 5.1437\n",
      "Epoch [1/5], Step [3699/5873], Loss: 1.2984\n",
      "Epoch [1/5], Step [3700/5873], Loss: 3.5397\n",
      "Epoch [1/5], Step [3701/5873], Loss: 3.7459\n",
      "Epoch [1/5], Step [3702/5873], Loss: 3.4057\n",
      "Epoch [1/5], Step [3703/5873], Loss: 5.1620\n",
      "Epoch [1/5], Step [3704/5873], Loss: 4.3815\n",
      "Epoch [1/5], Step [3705/5873], Loss: 4.8433\n",
      "Epoch [1/5], Step [3706/5873], Loss: 4.4781\n",
      "Epoch [1/5], Step [3707/5873], Loss: 3.6924\n",
      "Epoch [1/5], Step [3708/5873], Loss: 4.4103\n",
      "Epoch [1/5], Step [3709/5873], Loss: 4.3207\n",
      "Epoch [1/5], Step [3710/5873], Loss: 4.2892\n",
      "Epoch [1/5], Step [3711/5873], Loss: 4.3481\n",
      "Epoch [1/5], Step [3712/5873], Loss: 1.8826\n",
      "Epoch [1/5], Step [3713/5873], Loss: 4.0229\n",
      "Epoch [1/5], Step [3714/5873], Loss: 3.6489\n",
      "Epoch [1/5], Step [3715/5873], Loss: 3.6521\n",
      "Epoch [1/5], Step [3716/5873], Loss: 5.4541\n",
      "Epoch [1/5], Step [3717/5873], Loss: 3.5461\n",
      "Epoch [1/5], Step [3718/5873], Loss: 4.4299\n",
      "Epoch [1/5], Step [3719/5873], Loss: 3.5551\n",
      "Epoch [1/5], Step [3720/5873], Loss: 3.0537\n",
      "Epoch [1/5], Step [3721/5873], Loss: 4.7005\n",
      "Epoch [1/5], Step [3722/5873], Loss: 3.8503\n",
      "Epoch [1/5], Step [3723/5873], Loss: 2.9486\n",
      "Epoch [1/5], Step [3724/5873], Loss: 6.3421\n",
      "Epoch [1/5], Step [3725/5873], Loss: 3.5930\n",
      "Epoch [1/5], Step [3726/5873], Loss: 2.3017\n",
      "Epoch [1/5], Step [3727/5873], Loss: 3.6607\n",
      "Epoch [1/5], Step [3728/5873], Loss: 4.5274\n",
      "Epoch [1/5], Step [3729/5873], Loss: 4.6257\n",
      "Epoch [1/5], Step [3730/5873], Loss: 4.4841\n",
      "Epoch [1/5], Step [3731/5873], Loss: 4.5672\n",
      "Epoch [1/5], Step [3732/5873], Loss: 2.9797\n",
      "Epoch [1/5], Step [3733/5873], Loss: 5.6807\n",
      "Epoch [1/5], Step [3734/5873], Loss: 3.7459\n",
      "Epoch [1/5], Step [3735/5873], Loss: 5.6461\n",
      "Epoch [1/5], Step [3736/5873], Loss: 4.7895\n",
      "Epoch [1/5], Step [3737/5873], Loss: 4.3778\n",
      "Epoch [1/5], Step [3738/5873], Loss: 3.9156\n",
      "Epoch [1/5], Step [3739/5873], Loss: 4.4985\n",
      "Epoch [1/5], Step [3740/5873], Loss: 3.7174\n",
      "Epoch [1/5], Step [3741/5873], Loss: 4.9519\n",
      "Epoch [1/5], Step [3742/5873], Loss: 5.2823\n",
      "Epoch [1/5], Step [3743/5873], Loss: 4.1617\n",
      "Epoch [1/5], Step [3744/5873], Loss: 4.0420\n",
      "Epoch [1/5], Step [3745/5873], Loss: 5.4280\n",
      "Epoch [1/5], Step [3746/5873], Loss: 2.1615\n",
      "Epoch [1/5], Step [3747/5873], Loss: 5.2406\n",
      "Epoch [1/5], Step [3748/5873], Loss: 2.3222\n",
      "Epoch [1/5], Step [3749/5873], Loss: 5.4671\n",
      "Epoch [1/5], Step [3750/5873], Loss: 4.5400\n",
      "Epoch [1/5], Step [3751/5873], Loss: 5.2859\n",
      "Epoch [1/5], Step [3752/5873], Loss: 4.6201\n",
      "Epoch [1/5], Step [3753/5873], Loss: 2.8581\n",
      "Epoch [1/5], Step [3754/5873], Loss: 2.5217\n",
      "Epoch [1/5], Step [3755/5873], Loss: 4.2893\n",
      "Epoch [1/5], Step [3756/5873], Loss: 3.8859\n",
      "Epoch [1/5], Step [3757/5873], Loss: 5.3694\n",
      "Epoch [1/5], Step [3758/5873], Loss: 5.0814\n",
      "Epoch [1/5], Step [3759/5873], Loss: 2.1355\n",
      "Epoch [1/5], Step [3760/5873], Loss: 4.1576\n",
      "Epoch [1/5], Step [3761/5873], Loss: 3.4439\n",
      "Epoch [1/5], Step [3762/5873], Loss: 3.5614\n",
      "Epoch [1/5], Step [3763/5873], Loss: 4.7259\n",
      "Epoch [1/5], Step [3764/5873], Loss: 4.0067\n",
      "Epoch [1/5], Step [3765/5873], Loss: 3.4167\n",
      "Epoch [1/5], Step [3766/5873], Loss: 6.1876\n",
      "Epoch [1/5], Step [3767/5873], Loss: 1.9989\n",
      "Epoch [1/5], Step [3768/5873], Loss: 3.3909\n",
      "Epoch [1/5], Step [3769/5873], Loss: 4.1558\n",
      "Epoch [1/5], Step [3770/5873], Loss: 3.7503\n",
      "Epoch [1/5], Step [3771/5873], Loss: 4.3668\n",
      "Epoch [1/5], Step [3772/5873], Loss: 3.0261\n",
      "Epoch [1/5], Step [3773/5873], Loss: 4.6488\n",
      "Epoch [1/5], Step [3774/5873], Loss: 2.6634\n",
      "Epoch [1/5], Step [3775/5873], Loss: 2.6525\n",
      "Epoch [1/5], Step [3776/5873], Loss: 2.3751\n",
      "Epoch [1/5], Step [3777/5873], Loss: 5.2974\n",
      "Epoch [1/5], Step [3778/5873], Loss: 3.3864\n",
      "Epoch [1/5], Step [3779/5873], Loss: 5.1085\n",
      "Epoch [1/5], Step [3780/5873], Loss: 4.0627\n",
      "Epoch [1/5], Step [3781/5873], Loss: 4.1727\n",
      "Epoch [1/5], Step [3782/5873], Loss: 2.4118\n",
      "Epoch [1/5], Step [3783/5873], Loss: 3.4620\n",
      "Epoch [1/5], Step [3784/5873], Loss: 2.3884\n",
      "Epoch [1/5], Step [3785/5873], Loss: 3.7233\n",
      "Epoch [1/5], Step [3786/5873], Loss: 4.7869\n",
      "Epoch [1/5], Step [3787/5873], Loss: 2.5603\n",
      "Epoch [1/5], Step [3788/5873], Loss: 2.9273\n",
      "Epoch [1/5], Step [3789/5873], Loss: 4.3588\n",
      "Epoch [1/5], Step [3790/5873], Loss: 2.3607\n",
      "Epoch [1/5], Step [3791/5873], Loss: 4.9071\n",
      "Epoch [1/5], Step [3792/5873], Loss: 4.2945\n",
      "Epoch [1/5], Step [3793/5873], Loss: 4.8377\n",
      "Epoch [1/5], Step [3794/5873], Loss: 5.7565\n",
      "Epoch [1/5], Step [3795/5873], Loss: 3.6290\n",
      "Epoch [1/5], Step [3796/5873], Loss: 4.0432\n",
      "Epoch [1/5], Step [3797/5873], Loss: 2.0358\n",
      "Epoch [1/5], Step [3798/5873], Loss: 3.8707\n",
      "Epoch [1/5], Step [3799/5873], Loss: 4.2421\n",
      "Epoch [1/5], Step [3800/5873], Loss: 2.3737\n",
      "Epoch [1/5], Step [3801/5873], Loss: 3.9181\n",
      "Epoch [1/5], Step [3802/5873], Loss: 2.9774\n",
      "Epoch [1/5], Step [3803/5873], Loss: 2.4985\n",
      "Epoch [1/5], Step [3804/5873], Loss: 4.5766\n",
      "Epoch [1/5], Step [3805/5873], Loss: 5.0705\n",
      "Epoch [1/5], Step [3806/5873], Loss: 2.3342\n",
      "Epoch [1/5], Step [3807/5873], Loss: 3.0169\n",
      "Epoch [1/5], Step [3808/5873], Loss: 5.0785\n",
      "Epoch [1/5], Step [3809/5873], Loss: 3.9109\n",
      "Epoch [1/5], Step [3810/5873], Loss: 4.6775\n",
      "Epoch [1/5], Step [3811/5873], Loss: 5.1941\n",
      "Epoch [1/5], Step [3812/5873], Loss: 3.3242\n",
      "Epoch [1/5], Step [3813/5873], Loss: 3.1833\n",
      "Epoch [1/5], Step [3814/5873], Loss: 5.3049\n",
      "Epoch [1/5], Step [3815/5873], Loss: 4.5903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3816/5873], Loss: 6.7124\n",
      "Epoch [1/5], Step [3817/5873], Loss: 5.0724\n",
      "Epoch [1/5], Step [3818/5873], Loss: 3.9274\n",
      "Epoch [1/5], Step [3819/5873], Loss: 3.5097\n",
      "Epoch [1/5], Step [3820/5873], Loss: 4.5234\n",
      "Epoch [1/5], Step [3821/5873], Loss: 3.3235\n",
      "Epoch [1/5], Step [3822/5873], Loss: 3.1163\n",
      "Epoch [1/5], Step [3823/5873], Loss: 3.6012\n",
      "Epoch [1/5], Step [3824/5873], Loss: 3.0283\n",
      "Epoch [1/5], Step [3825/5873], Loss: 3.5821\n",
      "Epoch [1/5], Step [3826/5873], Loss: 3.1464\n",
      "Epoch [1/5], Step [3827/5873], Loss: 3.6381\n",
      "Epoch [1/5], Step [3828/5873], Loss: 3.0899\n",
      "Epoch [1/5], Step [3829/5873], Loss: 2.7591\n",
      "Epoch [1/5], Step [3830/5873], Loss: 5.1911\n",
      "Epoch [1/5], Step [3831/5873], Loss: 4.6977\n",
      "Epoch [1/5], Step [3832/5873], Loss: 3.7471\n",
      "Epoch [1/5], Step [3833/5873], Loss: 4.3173\n",
      "Epoch [1/5], Step [3834/5873], Loss: 3.8408\n",
      "Epoch [1/5], Step [3835/5873], Loss: 4.1510\n",
      "Epoch [1/5], Step [3836/5873], Loss: 4.1385\n",
      "Epoch [1/5], Step [3837/5873], Loss: 6.1706\n",
      "Epoch [1/5], Step [3838/5873], Loss: 5.0729\n",
      "Epoch [1/5], Step [3839/5873], Loss: 4.8251\n",
      "Epoch [1/5], Step [3840/5873], Loss: 5.0660\n",
      "Epoch [1/5], Step [3841/5873], Loss: 4.1619\n",
      "Epoch [1/5], Step [3842/5873], Loss: 5.0409\n",
      "Epoch [1/5], Step [3843/5873], Loss: 3.7192\n",
      "Epoch [1/5], Step [3844/5873], Loss: 4.6619\n",
      "Epoch [1/5], Step [3845/5873], Loss: 3.8248\n",
      "Epoch [1/5], Step [3846/5873], Loss: 4.0382\n",
      "Epoch [1/5], Step [3847/5873], Loss: 5.1665\n",
      "Epoch [1/5], Step [3848/5873], Loss: 2.0985\n",
      "Epoch [1/5], Step [3849/5873], Loss: 4.0481\n",
      "Epoch [1/5], Step [3850/5873], Loss: 5.6404\n",
      "Epoch [1/5], Step [3851/5873], Loss: 4.5017\n",
      "Epoch [1/5], Step [3852/5873], Loss: 3.2259\n",
      "Epoch [1/5], Step [3853/5873], Loss: 3.2568\n",
      "Epoch [1/5], Step [3854/5873], Loss: 4.4356\n",
      "Epoch [1/5], Step [3855/5873], Loss: 5.4246\n",
      "Epoch [1/5], Step [3856/5873], Loss: 5.3936\n",
      "Epoch [1/5], Step [3857/5873], Loss: 4.9208\n",
      "Epoch [1/5], Step [3858/5873], Loss: 3.2159\n",
      "Epoch [1/5], Step [3859/5873], Loss: 2.7970\n",
      "Epoch [1/5], Step [3860/5873], Loss: 3.5603\n",
      "Epoch [1/5], Step [3861/5873], Loss: 4.0974\n",
      "Epoch [1/5], Step [3862/5873], Loss: 3.9617\n",
      "Epoch [1/5], Step [3863/5873], Loss: 3.3635\n",
      "Epoch [1/5], Step [3864/5873], Loss: 3.7390\n",
      "Epoch [1/5], Step [3865/5873], Loss: 4.7659\n",
      "Epoch [1/5], Step [3866/5873], Loss: 2.3415\n",
      "Epoch [1/5], Step [3867/5873], Loss: 2.0705\n",
      "Epoch [1/5], Step [3868/5873], Loss: 3.6580\n",
      "Epoch [1/5], Step [3869/5873], Loss: 5.6102\n",
      "Epoch [1/5], Step [3870/5873], Loss: 3.4384\n",
      "Epoch [1/5], Step [3871/5873], Loss: 3.7160\n",
      "Epoch [1/5], Step [3872/5873], Loss: 5.3282\n",
      "Epoch [1/5], Step [3873/5873], Loss: 4.2429\n",
      "Epoch [1/5], Step [3874/5873], Loss: 3.5889\n",
      "Epoch [1/5], Step [3875/5873], Loss: 2.5849\n",
      "Epoch [1/5], Step [3876/5873], Loss: 3.9107\n",
      "Epoch [1/5], Step [3877/5873], Loss: 3.7271\n",
      "Epoch [1/5], Step [3878/5873], Loss: 4.8280\n",
      "Epoch [1/5], Step [3879/5873], Loss: 2.5439\n",
      "Epoch [1/5], Step [3880/5873], Loss: 5.3657\n",
      "Epoch [1/5], Step [3881/5873], Loss: 5.2478\n",
      "Epoch [1/5], Step [3882/5873], Loss: 2.8098\n",
      "Epoch [1/5], Step [3883/5873], Loss: 4.9676\n",
      "Epoch [1/5], Step [3884/5873], Loss: 4.3425\n",
      "Epoch [1/5], Step [3885/5873], Loss: 3.9650\n",
      "Epoch [1/5], Step [3886/5873], Loss: 5.9573\n",
      "Epoch [1/5], Step [3887/5873], Loss: 4.1593\n",
      "Epoch [1/5], Step [3888/5873], Loss: 6.4229\n",
      "Epoch [1/5], Step [3889/5873], Loss: 3.9363\n",
      "Epoch [1/5], Step [3890/5873], Loss: 2.5465\n",
      "Epoch [1/5], Step [3891/5873], Loss: 3.6752\n",
      "Epoch [1/5], Step [3892/5873], Loss: 4.2833\n",
      "Epoch [1/5], Step [3893/5873], Loss: 4.8320\n",
      "Epoch [1/5], Step [3894/5873], Loss: 2.3440\n",
      "Epoch [1/5], Step [3895/5873], Loss: 3.2596\n",
      "Epoch [1/5], Step [3896/5873], Loss: 1.5057\n",
      "Epoch [1/5], Step [3897/5873], Loss: 2.3308\n",
      "Epoch [1/5], Step [3898/5873], Loss: 2.6581\n",
      "Epoch [1/5], Step [3899/5873], Loss: 3.9519\n",
      "Epoch [1/5], Step [3900/5873], Loss: 4.2696\n",
      "Epoch [1/5], Step [3901/5873], Loss: 3.5052\n",
      "Epoch [1/5], Step [3902/5873], Loss: 4.4584\n",
      "Epoch [1/5], Step [3903/5873], Loss: 3.8124\n",
      "Epoch [1/5], Step [3904/5873], Loss: 2.6274\n",
      "Epoch [1/5], Step [3905/5873], Loss: 4.1950\n",
      "Epoch [1/5], Step [3906/5873], Loss: 1.4637\n",
      "Epoch [1/5], Step [3907/5873], Loss: 2.0588\n",
      "Epoch [1/5], Step [3908/5873], Loss: 5.3359\n",
      "Epoch [1/5], Step [3909/5873], Loss: 3.9720\n",
      "Epoch [1/5], Step [3910/5873], Loss: 2.5812\n",
      "Epoch [1/5], Step [3911/5873], Loss: 3.4512\n",
      "Epoch [1/5], Step [3912/5873], Loss: 4.4528\n",
      "Epoch [1/5], Step [3913/5873], Loss: 2.4715\n",
      "Epoch [1/5], Step [3914/5873], Loss: 3.8977\n",
      "Epoch [1/5], Step [3915/5873], Loss: 1.7063\n",
      "Epoch [1/5], Step [3916/5873], Loss: 1.7763\n",
      "Epoch [1/5], Step [3917/5873], Loss: 3.0159\n",
      "Epoch [1/5], Step [3918/5873], Loss: 4.2016\n",
      "Epoch [1/5], Step [3919/5873], Loss: 2.4711\n",
      "Epoch [1/5], Step [3920/5873], Loss: 3.5937\n",
      "Epoch [1/5], Step [3921/5873], Loss: 4.7465\n",
      "Epoch [1/5], Step [3922/5873], Loss: 2.6698\n",
      "Epoch [1/5], Step [3923/5873], Loss: 4.3813\n",
      "Epoch [1/5], Step [3924/5873], Loss: 2.9306\n",
      "Epoch [1/5], Step [3925/5873], Loss: 2.4314\n",
      "Epoch [1/5], Step [3926/5873], Loss: 2.0120\n",
      "Epoch [1/5], Step [3927/5873], Loss: 5.8307\n",
      "Epoch [1/5], Step [3928/5873], Loss: 4.2845\n",
      "Epoch [1/5], Step [3929/5873], Loss: 4.8693\n",
      "Epoch [1/5], Step [3930/5873], Loss: 3.1901\n",
      "Epoch [1/5], Step [3931/5873], Loss: 4.4808\n",
      "Epoch [1/5], Step [3932/5873], Loss: 5.0858\n",
      "Epoch [1/5], Step [3933/5873], Loss: 3.1989\n",
      "Epoch [1/5], Step [3934/5873], Loss: 4.1505\n",
      "Epoch [1/5], Step [3935/5873], Loss: 4.2542\n",
      "Epoch [1/5], Step [3936/5873], Loss: 4.9848\n",
      "Epoch [1/5], Step [3937/5873], Loss: 4.5673\n",
      "Epoch [1/5], Step [3938/5873], Loss: 4.6636\n",
      "Epoch [1/5], Step [3939/5873], Loss: 4.7385\n",
      "Epoch [1/5], Step [3940/5873], Loss: 5.3598\n",
      "Epoch [1/5], Step [3941/5873], Loss: 3.8023\n",
      "Epoch [1/5], Step [3942/5873], Loss: 3.6329\n",
      "Epoch [1/5], Step [3943/5873], Loss: 4.1386\n",
      "Epoch [1/5], Step [3944/5873], Loss: 3.1413\n",
      "Epoch [1/5], Step [3945/5873], Loss: 4.4874\n",
      "Epoch [1/5], Step [3946/5873], Loss: 3.6130\n",
      "Epoch [1/5], Step [3947/5873], Loss: 4.6381\n",
      "Epoch [1/5], Step [3948/5873], Loss: 3.1561\n",
      "Epoch [1/5], Step [3949/5873], Loss: 4.7651\n",
      "Epoch [1/5], Step [3950/5873], Loss: 3.5121\n",
      "Epoch [1/5], Step [3951/5873], Loss: 3.9160\n",
      "Epoch [1/5], Step [3952/5873], Loss: 4.3062\n",
      "Epoch [1/5], Step [3953/5873], Loss: 3.0795\n",
      "Epoch [1/5], Step [3954/5873], Loss: 4.6517\n",
      "Epoch [1/5], Step [3955/5873], Loss: 4.3817\n",
      "Epoch [1/5], Step [3956/5873], Loss: 5.1169\n",
      "Epoch [1/5], Step [3957/5873], Loss: 3.1961\n",
      "Epoch [1/5], Step [3958/5873], Loss: 3.3427\n",
      "Epoch [1/5], Step [3959/5873], Loss: 4.4565\n",
      "Epoch [1/5], Step [3960/5873], Loss: 3.2321\n",
      "Epoch [1/5], Step [3961/5873], Loss: 2.4953\n",
      "Epoch [1/5], Step [3962/5873], Loss: 4.0522\n",
      "Epoch [1/5], Step [3963/5873], Loss: 2.6633\n",
      "Epoch [1/5], Step [3964/5873], Loss: 3.7243\n",
      "Epoch [1/5], Step [3965/5873], Loss: 3.0796\n",
      "Epoch [1/5], Step [3966/5873], Loss: 3.6653\n",
      "Epoch [1/5], Step [3967/5873], Loss: 2.9379\n",
      "Epoch [1/5], Step [3968/5873], Loss: 2.3000\n",
      "Epoch [1/5], Step [3969/5873], Loss: 4.2020\n",
      "Epoch [1/5], Step [3970/5873], Loss: 4.7314\n",
      "Epoch [1/5], Step [3971/5873], Loss: 6.5193\n",
      "Epoch [1/5], Step [3972/5873], Loss: 3.3624\n",
      "Epoch [1/5], Step [3973/5873], Loss: 3.4775\n",
      "Epoch [1/5], Step [3974/5873], Loss: 4.6127\n",
      "Epoch [1/5], Step [3975/5873], Loss: 4.7806\n",
      "Epoch [1/5], Step [3976/5873], Loss: 1.9435\n",
      "Epoch [1/5], Step [3977/5873], Loss: 5.0987\n",
      "Epoch [1/5], Step [3978/5873], Loss: 4.3886\n",
      "Epoch [1/5], Step [3979/5873], Loss: 4.3482\n",
      "Epoch [1/5], Step [3980/5873], Loss: 2.6132\n",
      "Epoch [1/5], Step [3981/5873], Loss: 3.6390\n",
      "Epoch [1/5], Step [3982/5873], Loss: 3.0082\n",
      "Epoch [1/5], Step [3983/5873], Loss: 2.3859\n",
      "Epoch [1/5], Step [3984/5873], Loss: 2.9893\n",
      "Epoch [1/5], Step [3985/5873], Loss: 4.7530\n",
      "Epoch [1/5], Step [3986/5873], Loss: 3.2186\n",
      "Epoch [1/5], Step [3987/5873], Loss: 2.1714\n",
      "Epoch [1/5], Step [3988/5873], Loss: 3.1341\n",
      "Epoch [1/5], Step [3989/5873], Loss: 1.9112\n",
      "Epoch [1/5], Step [3990/5873], Loss: 4.4146\n",
      "Epoch [1/5], Step [3991/5873], Loss: 3.2616\n",
      "Epoch [1/5], Step [3992/5873], Loss: 5.3008\n",
      "Epoch [1/5], Step [3993/5873], Loss: 4.7632\n",
      "Epoch [1/5], Step [3994/5873], Loss: 5.1717\n",
      "Epoch [1/5], Step [3995/5873], Loss: 3.8122\n",
      "Epoch [1/5], Step [3996/5873], Loss: 4.5220\n",
      "Epoch [1/5], Step [3997/5873], Loss: 4.6032\n",
      "Epoch [1/5], Step [3998/5873], Loss: 3.6919\n",
      "Epoch [1/5], Step [3999/5873], Loss: 4.2887\n",
      "Epoch [1/5], Step [4000/5873], Loss: 2.8247\n",
      "Epoch [1/5], Step [4001/5873], Loss: 4.5552\n",
      "Epoch [1/5], Step [4002/5873], Loss: 3.5710\n",
      "Epoch [1/5], Step [4003/5873], Loss: 3.1884\n",
      "Epoch [1/5], Step [4004/5873], Loss: 1.8689\n",
      "Epoch [1/5], Step [4005/5873], Loss: 2.4820\n",
      "Epoch [1/5], Step [4006/5873], Loss: 4.7785\n",
      "Epoch [1/5], Step [4007/5873], Loss: 5.6930\n",
      "Epoch [1/5], Step [4008/5873], Loss: 4.4881\n",
      "Epoch [1/5], Step [4009/5873], Loss: 2.8866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4010/5873], Loss: 4.2645\n",
      "Epoch [1/5], Step [4011/5873], Loss: 4.2153\n",
      "Epoch [1/5], Step [4012/5873], Loss: 5.9850\n",
      "Epoch [1/5], Step [4013/5873], Loss: 4.8940\n",
      "Epoch [1/5], Step [4014/5873], Loss: 6.3626\n",
      "Epoch [1/5], Step [4015/5873], Loss: 1.9459\n",
      "Epoch [1/5], Step [4016/5873], Loss: 2.0786\n",
      "Epoch [1/5], Step [4017/5873], Loss: 5.4556\n",
      "Epoch [1/5], Step [4018/5873], Loss: 4.1169\n",
      "Epoch [1/5], Step [4019/5873], Loss: 4.5768\n",
      "Epoch [1/5], Step [4020/5873], Loss: 4.7421\n",
      "Epoch [1/5], Step [4021/5873], Loss: 3.1889\n",
      "Epoch [1/5], Step [4022/5873], Loss: 4.9972\n",
      "Epoch [1/5], Step [4023/5873], Loss: 4.0401\n",
      "Epoch [1/5], Step [4024/5873], Loss: 2.0602\n",
      "Epoch [1/5], Step [4025/5873], Loss: 2.9809\n",
      "Epoch [1/5], Step [4026/5873], Loss: 4.3129\n",
      "Epoch [1/5], Step [4027/5873], Loss: 3.1252\n",
      "Epoch [1/5], Step [4028/5873], Loss: 2.8951\n",
      "Epoch [1/5], Step [4029/5873], Loss: 5.5288\n",
      "Epoch [1/5], Step [4030/5873], Loss: 5.6585\n",
      "Epoch [1/5], Step [4031/5873], Loss: 2.7535\n",
      "Epoch [1/5], Step [4032/5873], Loss: 5.0612\n",
      "Epoch [1/5], Step [4033/5873], Loss: 4.0402\n",
      "Epoch [1/5], Step [4034/5873], Loss: 3.6602\n",
      "Epoch [1/5], Step [4035/5873], Loss: 5.0193\n",
      "Epoch [1/5], Step [4036/5873], Loss: 3.4246\n",
      "Epoch [1/5], Step [4037/5873], Loss: 2.2940\n",
      "Epoch [1/5], Step [4038/5873], Loss: 3.2493\n",
      "Epoch [1/5], Step [4039/5873], Loss: 3.6875\n",
      "Epoch [1/5], Step [4040/5873], Loss: 3.9214\n",
      "Epoch [1/5], Step [4041/5873], Loss: 4.2810\n",
      "Epoch [1/5], Step [4042/5873], Loss: 2.8409\n",
      "Epoch [1/5], Step [4043/5873], Loss: 2.2505\n",
      "Epoch [1/5], Step [4044/5873], Loss: 3.5125\n",
      "Epoch [1/5], Step [4045/5873], Loss: 0.8895\n",
      "Epoch [1/5], Step [4046/5873], Loss: 5.6613\n",
      "Epoch [1/5], Step [4047/5873], Loss: 3.1546\n",
      "Epoch [1/5], Step [4048/5873], Loss: 6.0329\n",
      "Epoch [1/5], Step [4049/5873], Loss: 5.2178\n",
      "Epoch [1/5], Step [4050/5873], Loss: 2.1710\n",
      "Epoch [1/5], Step [4051/5873], Loss: 2.7985\n",
      "Epoch [1/5], Step [4052/5873], Loss: 3.9989\n",
      "Epoch [1/5], Step [4053/5873], Loss: 4.7775\n",
      "Epoch [1/5], Step [4054/5873], Loss: 2.6602\n",
      "Epoch [1/5], Step [4055/5873], Loss: 3.8968\n",
      "Epoch [1/5], Step [4056/5873], Loss: 3.7613\n",
      "Epoch [1/5], Step [4057/5873], Loss: 4.8746\n",
      "Epoch [1/5], Step [4058/5873], Loss: 2.0535\n",
      "Epoch [1/5], Step [4059/5873], Loss: 4.7120\n",
      "Epoch [1/5], Step [4060/5873], Loss: 4.6964\n",
      "Epoch [1/5], Step [4061/5873], Loss: 2.5933\n",
      "Epoch [1/5], Step [4062/5873], Loss: 6.1495\n",
      "Epoch [1/5], Step [4063/5873], Loss: 3.1495\n",
      "Epoch [1/5], Step [4064/5873], Loss: 5.2428\n",
      "Epoch [1/5], Step [4065/5873], Loss: 2.6202\n",
      "Epoch [1/5], Step [4066/5873], Loss: 1.9499\n",
      "Epoch [1/5], Step [4067/5873], Loss: 4.6168\n",
      "Epoch [1/5], Step [4068/5873], Loss: 4.5584\n",
      "Epoch [1/5], Step [4069/5873], Loss: 3.9672\n",
      "Epoch [1/5], Step [4070/5873], Loss: 1.8368\n",
      "Epoch [1/5], Step [4071/5873], Loss: 3.4922\n",
      "Epoch [1/5], Step [4072/5873], Loss: 4.1756\n",
      "Epoch [1/5], Step [4073/5873], Loss: 2.3320\n",
      "Epoch [1/5], Step [4074/5873], Loss: 5.9341\n",
      "Epoch [1/5], Step [4075/5873], Loss: 3.4837\n",
      "Epoch [1/5], Step [4076/5873], Loss: 4.2713\n",
      "Epoch [1/5], Step [4077/5873], Loss: 3.4233\n",
      "Epoch [1/5], Step [4078/5873], Loss: 2.3299\n",
      "Epoch [1/5], Step [4079/5873], Loss: 4.8768\n",
      "Epoch [1/5], Step [4080/5873], Loss: 5.3308\n",
      "Epoch [1/5], Step [4081/5873], Loss: 4.2847\n",
      "Epoch [1/5], Step [4082/5873], Loss: 5.7899\n",
      "Epoch [1/5], Step [4083/5873], Loss: 4.8853\n",
      "Epoch [1/5], Step [4084/5873], Loss: 4.6892\n",
      "Epoch [1/5], Step [4085/5873], Loss: 3.8346\n",
      "Epoch [1/5], Step [4086/5873], Loss: 5.4864\n",
      "Epoch [1/5], Step [4087/5873], Loss: 3.9248\n",
      "Epoch [1/5], Step [4088/5873], Loss: 5.7191\n",
      "Epoch [1/5], Step [4089/5873], Loss: 3.5609\n",
      "Epoch [1/5], Step [4090/5873], Loss: 3.8142\n",
      "Epoch [1/5], Step [4091/5873], Loss: 4.2399\n",
      "Epoch [1/5], Step [4092/5873], Loss: 4.1819\n",
      "Epoch [1/5], Step [4093/5873], Loss: 1.7015\n",
      "Epoch [1/5], Step [4094/5873], Loss: 4.6715\n",
      "Epoch [1/5], Step [4095/5873], Loss: 4.5803\n",
      "Epoch [1/5], Step [4096/5873], Loss: 4.4483\n",
      "Epoch [1/5], Step [4097/5873], Loss: 3.8026\n",
      "Epoch [1/5], Step [4098/5873], Loss: 4.6746\n",
      "Epoch [1/5], Step [4099/5873], Loss: 2.8332\n",
      "Epoch [1/5], Step [4100/5873], Loss: 2.5640\n",
      "Epoch [1/5], Step [4101/5873], Loss: 4.0845\n",
      "Epoch [1/5], Step [4102/5873], Loss: 4.6842\n",
      "Epoch [1/5], Step [4103/5873], Loss: 4.5740\n",
      "Epoch [1/5], Step [4104/5873], Loss: 3.5202\n",
      "Epoch [1/5], Step [4105/5873], Loss: 4.7255\n",
      "Epoch [1/5], Step [4106/5873], Loss: 3.9431\n",
      "Epoch [1/5], Step [4107/5873], Loss: 4.9871\n",
      "Epoch [1/5], Step [4108/5873], Loss: 6.0250\n",
      "Epoch [1/5], Step [4109/5873], Loss: 2.2566\n",
      "Epoch [1/5], Step [4110/5873], Loss: 3.0348\n",
      "Epoch [1/5], Step [4111/5873], Loss: 2.4402\n",
      "Epoch [1/5], Step [4112/5873], Loss: 3.2208\n",
      "Epoch [1/5], Step [4113/5873], Loss: 4.6054\n",
      "Epoch [1/5], Step [4114/5873], Loss: 5.7011\n",
      "Epoch [1/5], Step [4115/5873], Loss: 3.2880\n",
      "Epoch [1/5], Step [4116/5873], Loss: 2.9983\n",
      "Epoch [1/5], Step [4117/5873], Loss: 4.1224\n",
      "Epoch [1/5], Step [4118/5873], Loss: 1.9549\n",
      "Epoch [1/5], Step [4119/5873], Loss: 3.7952\n",
      "Epoch [1/5], Step [4120/5873], Loss: 3.6713\n",
      "Epoch [1/5], Step [4121/5873], Loss: 3.5072\n",
      "Epoch [1/5], Step [4122/5873], Loss: 4.6236\n",
      "Epoch [1/5], Step [4123/5873], Loss: 5.7617\n",
      "Epoch [1/5], Step [4124/5873], Loss: 2.8346\n",
      "Epoch [1/5], Step [4125/5873], Loss: 4.7026\n",
      "Epoch [1/5], Step [4126/5873], Loss: 1.2964\n",
      "Epoch [1/5], Step [4127/5873], Loss: 2.2535\n",
      "Epoch [1/5], Step [4128/5873], Loss: 3.7607\n",
      "Epoch [1/5], Step [4129/5873], Loss: 3.1816\n",
      "Epoch [1/5], Step [4130/5873], Loss: 3.1882\n",
      "Epoch [1/5], Step [4131/5873], Loss: 0.8372\n",
      "Epoch [1/5], Step [4132/5873], Loss: 3.5760\n",
      "Epoch [1/5], Step [4133/5873], Loss: 3.8328\n",
      "Epoch [1/5], Step [4134/5873], Loss: 4.8395\n",
      "Epoch [1/5], Step [4135/5873], Loss: 4.1112\n",
      "Epoch [1/5], Step [4136/5873], Loss: 4.8594\n",
      "Epoch [1/5], Step [4137/5873], Loss: 5.6244\n",
      "Epoch [1/5], Step [4138/5873], Loss: 4.0052\n",
      "Epoch [1/5], Step [4139/5873], Loss: 4.5644\n",
      "Epoch [1/5], Step [4140/5873], Loss: 3.8344\n",
      "Epoch [1/5], Step [4141/5873], Loss: 4.2542\n",
      "Epoch [1/5], Step [4142/5873], Loss: 2.1732\n",
      "Epoch [1/5], Step [4143/5873], Loss: 3.6249\n",
      "Epoch [1/5], Step [4144/5873], Loss: 1.5460\n",
      "Epoch [1/5], Step [4145/5873], Loss: 6.6389\n",
      "Epoch [1/5], Step [4146/5873], Loss: 0.9533\n",
      "Epoch [1/5], Step [4147/5873], Loss: 3.2497\n",
      "Epoch [1/5], Step [4148/5873], Loss: 4.3376\n",
      "Epoch [1/5], Step [4149/5873], Loss: 4.5041\n",
      "Epoch [1/5], Step [4150/5873], Loss: 4.0972\n",
      "Epoch [1/5], Step [4151/5873], Loss: 1.1981\n",
      "Epoch [1/5], Step [4152/5873], Loss: 4.3593\n",
      "Epoch [1/5], Step [4153/5873], Loss: 3.0244\n",
      "Epoch [1/5], Step [4154/5873], Loss: 1.4746\n",
      "Epoch [1/5], Step [4155/5873], Loss: 3.2640\n",
      "Epoch [1/5], Step [4156/5873], Loss: 2.3780\n",
      "Epoch [1/5], Step [4157/5873], Loss: 4.6050\n",
      "Epoch [1/5], Step [4158/5873], Loss: 4.7116\n",
      "Epoch [1/5], Step [4159/5873], Loss: 3.4085\n",
      "Epoch [1/5], Step [4160/5873], Loss: 5.3209\n",
      "Epoch [1/5], Step [4161/5873], Loss: 2.5378\n",
      "Epoch [1/5], Step [4162/5873], Loss: 2.9972\n",
      "Epoch [1/5], Step [4163/5873], Loss: 1.7753\n",
      "Epoch [1/5], Step [4164/5873], Loss: 4.9968\n",
      "Epoch [1/5], Step [4165/5873], Loss: 4.4982\n",
      "Epoch [1/5], Step [4166/5873], Loss: 3.9082\n",
      "Epoch [1/5], Step [4167/5873], Loss: 3.7302\n",
      "Epoch [1/5], Step [4168/5873], Loss: 3.3405\n",
      "Epoch [1/5], Step [4169/5873], Loss: 3.6968\n",
      "Epoch [1/5], Step [4170/5873], Loss: 1.5304\n",
      "Epoch [1/5], Step [4171/5873], Loss: 6.0187\n",
      "Epoch [1/5], Step [4172/5873], Loss: 2.4829\n",
      "Epoch [1/5], Step [4173/5873], Loss: 4.8342\n",
      "Epoch [1/5], Step [4174/5873], Loss: 4.7611\n",
      "Epoch [1/5], Step [4175/5873], Loss: 5.8934\n",
      "Epoch [1/5], Step [4176/5873], Loss: 5.2729\n",
      "Epoch [1/5], Step [4177/5873], Loss: 3.8421\n",
      "Epoch [1/5], Step [4178/5873], Loss: 4.8529\n",
      "Epoch [1/5], Step [4179/5873], Loss: 4.2373\n",
      "Epoch [1/5], Step [4180/5873], Loss: 4.4827\n",
      "Epoch [1/5], Step [4181/5873], Loss: 3.0125\n",
      "Epoch [1/5], Step [4182/5873], Loss: 2.9623\n",
      "Epoch [1/5], Step [4183/5873], Loss: 4.1714\n",
      "Epoch [1/5], Step [4184/5873], Loss: 4.0847\n",
      "Epoch [1/5], Step [4185/5873], Loss: 5.0126\n",
      "Epoch [1/5], Step [4186/5873], Loss: 2.9399\n",
      "Epoch [1/5], Step [4187/5873], Loss: 4.7573\n",
      "Epoch [1/5], Step [4188/5873], Loss: 2.7124\n",
      "Epoch [1/5], Step [4189/5873], Loss: 4.7925\n",
      "Epoch [1/5], Step [4190/5873], Loss: 3.6187\n",
      "Epoch [1/5], Step [4191/5873], Loss: 2.1121\n",
      "Epoch [1/5], Step [4192/5873], Loss: 3.5926\n",
      "Epoch [1/5], Step [4193/5873], Loss: 4.2517\n",
      "Epoch [1/5], Step [4194/5873], Loss: 5.1316\n",
      "Epoch [1/5], Step [4195/5873], Loss: 2.3678\n",
      "Epoch [1/5], Step [4196/5873], Loss: 4.5401\n",
      "Epoch [1/5], Step [4197/5873], Loss: 1.0888\n",
      "Epoch [1/5], Step [4198/5873], Loss: 3.5762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4199/5873], Loss: 4.3977\n",
      "Epoch [1/5], Step [4200/5873], Loss: 3.2728\n",
      "Epoch [1/5], Step [4201/5873], Loss: 4.4021\n",
      "Epoch [1/5], Step [4202/5873], Loss: 4.6770\n",
      "Epoch [1/5], Step [4203/5873], Loss: 3.8897\n",
      "Epoch [1/5], Step [4204/5873], Loss: 4.9964\n",
      "Epoch [1/5], Step [4205/5873], Loss: 1.3968\n",
      "Epoch [1/5], Step [4206/5873], Loss: 0.9978\n",
      "Epoch [1/5], Step [4207/5873], Loss: 5.9346\n",
      "Epoch [1/5], Step [4208/5873], Loss: 3.8021\n",
      "Epoch [1/5], Step [4209/5873], Loss: 4.6396\n",
      "Epoch [1/5], Step [4210/5873], Loss: 3.9163\n",
      "Epoch [1/5], Step [4211/5873], Loss: 5.2305\n",
      "Epoch [1/5], Step [4212/5873], Loss: 3.0661\n",
      "Epoch [1/5], Step [4213/5873], Loss: 5.4138\n",
      "Epoch [1/5], Step [4214/5873], Loss: 5.6117\n",
      "Epoch [1/5], Step [4215/5873], Loss: 4.5605\n",
      "Epoch [1/5], Step [4216/5873], Loss: 5.2313\n",
      "Epoch [1/5], Step [4217/5873], Loss: 3.6056\n",
      "Epoch [1/5], Step [4218/5873], Loss: 3.6495\n",
      "Epoch [1/5], Step [4219/5873], Loss: 4.4490\n",
      "Epoch [1/5], Step [4220/5873], Loss: 4.5190\n",
      "Epoch [1/5], Step [4221/5873], Loss: 2.7912\n",
      "Epoch [1/5], Step [4222/5873], Loss: 3.4071\n",
      "Epoch [1/5], Step [4223/5873], Loss: 4.7947\n",
      "Epoch [1/5], Step [4224/5873], Loss: 3.4488\n",
      "Epoch [1/5], Step [4225/5873], Loss: 4.6531\n",
      "Epoch [1/5], Step [4226/5873], Loss: 3.1489\n",
      "Epoch [1/5], Step [4227/5873], Loss: 1.3625\n",
      "Epoch [1/5], Step [4228/5873], Loss: 4.8019\n",
      "Epoch [1/5], Step [4229/5873], Loss: 5.1243\n",
      "Epoch [1/5], Step [4230/5873], Loss: 3.6375\n",
      "Epoch [1/5], Step [4231/5873], Loss: 0.9854\n",
      "Epoch [1/5], Step [4232/5873], Loss: 5.2337\n",
      "Epoch [1/5], Step [4233/5873], Loss: 3.8935\n",
      "Epoch [1/5], Step [4234/5873], Loss: 1.7663\n",
      "Epoch [1/5], Step [4235/5873], Loss: 4.1281\n",
      "Epoch [1/5], Step [4236/5873], Loss: 2.6179\n",
      "Epoch [1/5], Step [4237/5873], Loss: 5.0567\n",
      "Epoch [1/5], Step [4238/5873], Loss: 5.0500\n",
      "Epoch [1/5], Step [4239/5873], Loss: 4.4216\n",
      "Epoch [1/5], Step [4240/5873], Loss: 2.9578\n",
      "Epoch [1/5], Step [4241/5873], Loss: 4.5538\n",
      "Epoch [1/5], Step [4242/5873], Loss: 2.9740\n",
      "Epoch [1/5], Step [4243/5873], Loss: 3.8388\n",
      "Epoch [1/5], Step [4244/5873], Loss: 4.8430\n",
      "Epoch [1/5], Step [4245/5873], Loss: 3.4236\n",
      "Epoch [1/5], Step [4246/5873], Loss: 0.8481\n",
      "Epoch [1/5], Step [4247/5873], Loss: 5.4773\n",
      "Epoch [1/5], Step [4248/5873], Loss: 3.4679\n",
      "Epoch [1/5], Step [4249/5873], Loss: 5.5691\n",
      "Epoch [1/5], Step [4250/5873], Loss: 5.3030\n",
      "Epoch [1/5], Step [4251/5873], Loss: 5.3166\n",
      "Epoch [1/5], Step [4252/5873], Loss: 2.8856\n",
      "Epoch [1/5], Step [4253/5873], Loss: 3.4492\n",
      "Epoch [1/5], Step [4254/5873], Loss: 3.8036\n",
      "Epoch [1/5], Step [4255/5873], Loss: 4.9248\n",
      "Epoch [1/5], Step [4256/5873], Loss: 4.0842\n",
      "Epoch [1/5], Step [4257/5873], Loss: 4.3152\n",
      "Epoch [1/5], Step [4258/5873], Loss: 5.1923\n",
      "Epoch [1/5], Step [4259/5873], Loss: 3.3169\n",
      "Epoch [1/5], Step [4260/5873], Loss: 2.1274\n",
      "Epoch [1/5], Step [4261/5873], Loss: 2.7826\n",
      "Epoch [1/5], Step [4262/5873], Loss: 3.1085\n",
      "Epoch [1/5], Step [4263/5873], Loss: 2.4250\n",
      "Epoch [1/5], Step [4264/5873], Loss: 3.5548\n",
      "Epoch [1/5], Step [4265/5873], Loss: 4.5149\n",
      "Epoch [1/5], Step [4266/5873], Loss: 5.8583\n",
      "Epoch [1/5], Step [4267/5873], Loss: 4.0664\n",
      "Epoch [1/5], Step [4268/5873], Loss: 3.6786\n",
      "Epoch [1/5], Step [4269/5873], Loss: 4.3658\n",
      "Epoch [1/5], Step [4270/5873], Loss: 4.4956\n",
      "Epoch [1/5], Step [4271/5873], Loss: 4.4355\n",
      "Epoch [1/5], Step [4272/5873], Loss: 3.9109\n",
      "Epoch [1/5], Step [4273/5873], Loss: 0.3220\n",
      "Epoch [1/5], Step [4274/5873], Loss: 4.3300\n",
      "Epoch [1/5], Step [4275/5873], Loss: 5.6069\n",
      "Epoch [1/5], Step [4276/5873], Loss: 4.9416\n",
      "Epoch [1/5], Step [4277/5873], Loss: 4.5584\n",
      "Epoch [1/5], Step [4278/5873], Loss: 3.9803\n",
      "Epoch [1/5], Step [4279/5873], Loss: 4.2148\n",
      "Epoch [1/5], Step [4280/5873], Loss: 5.7774\n",
      "Epoch [1/5], Step [4281/5873], Loss: 4.0883\n",
      "Epoch [1/5], Step [4282/5873], Loss: 4.6267\n",
      "Epoch [1/5], Step [4283/5873], Loss: 3.6499\n",
      "Epoch [1/5], Step [4284/5873], Loss: 4.7136\n",
      "Epoch [1/5], Step [4285/5873], Loss: 3.7094\n",
      "Epoch [1/5], Step [4286/5873], Loss: 5.5537\n",
      "Epoch [1/5], Step [4287/5873], Loss: 4.8592\n",
      "Epoch [1/5], Step [4288/5873], Loss: 4.0253\n",
      "Epoch [1/5], Step [4289/5873], Loss: 3.7689\n",
      "Epoch [1/5], Step [4290/5873], Loss: 2.5033\n",
      "Epoch [1/5], Step [4291/5873], Loss: 1.6701\n",
      "Epoch [1/5], Step [4292/5873], Loss: 1.9114\n",
      "Epoch [1/5], Step [4293/5873], Loss: 4.3899\n",
      "Epoch [1/5], Step [4294/5873], Loss: 2.0440\n",
      "Epoch [1/5], Step [4295/5873], Loss: 3.9887\n",
      "Epoch [1/5], Step [4296/5873], Loss: 4.4789\n",
      "Epoch [1/5], Step [4297/5873], Loss: 4.4412\n",
      "Epoch [1/5], Step [4298/5873], Loss: 2.4191\n",
      "Epoch [1/5], Step [4299/5873], Loss: 5.0856\n",
      "Epoch [1/5], Step [4300/5873], Loss: 4.3792\n",
      "Epoch [1/5], Step [4301/5873], Loss: 3.6121\n",
      "Epoch [1/5], Step [4302/5873], Loss: 3.5578\n",
      "Epoch [1/5], Step [4303/5873], Loss: 3.4903\n",
      "Epoch [1/5], Step [4304/5873], Loss: 3.1357\n",
      "Epoch [1/5], Step [4305/5873], Loss: 3.0902\n",
      "Epoch [1/5], Step [4306/5873], Loss: 4.5468\n",
      "Epoch [1/5], Step [4307/5873], Loss: 4.5988\n",
      "Epoch [1/5], Step [4308/5873], Loss: 4.3702\n",
      "Epoch [1/5], Step [4309/5873], Loss: 4.7504\n",
      "Epoch [1/5], Step [4310/5873], Loss: 4.4975\n",
      "Epoch [1/5], Step [4311/5873], Loss: 4.6404\n",
      "Epoch [1/5], Step [4312/5873], Loss: 3.5987\n",
      "Epoch [1/5], Step [4313/5873], Loss: 2.6346\n",
      "Epoch [1/5], Step [4314/5873], Loss: 2.8816\n",
      "Epoch [1/5], Step [4315/5873], Loss: 4.6071\n",
      "Epoch [1/5], Step [4316/5873], Loss: 4.1378\n",
      "Epoch [1/5], Step [4317/5873], Loss: 2.0866\n",
      "Epoch [1/5], Step [4318/5873], Loss: 3.7914\n",
      "Epoch [1/5], Step [4319/5873], Loss: 2.3337\n",
      "Epoch [1/5], Step [4320/5873], Loss: 4.4817\n",
      "Epoch [1/5], Step [4321/5873], Loss: 3.7094\n",
      "Epoch [1/5], Step [4322/5873], Loss: 2.2062\n",
      "Epoch [1/5], Step [4323/5873], Loss: 5.6391\n",
      "Epoch [1/5], Step [4324/5873], Loss: 3.2786\n",
      "Epoch [1/5], Step [4325/5873], Loss: 5.0287\n",
      "Epoch [1/5], Step [4326/5873], Loss: 4.0401\n",
      "Epoch [1/5], Step [4327/5873], Loss: 4.0836\n",
      "Epoch [1/5], Step [4328/5873], Loss: 5.5268\n",
      "Epoch [1/5], Step [4329/5873], Loss: 3.9446\n",
      "Epoch [1/5], Step [4330/5873], Loss: 3.6764\n",
      "Epoch [1/5], Step [4331/5873], Loss: 3.8379\n",
      "Epoch [1/5], Step [4332/5873], Loss: 3.3567\n",
      "Epoch [1/5], Step [4333/5873], Loss: 3.4370\n",
      "Epoch [1/5], Step [4334/5873], Loss: 3.4509\n",
      "Epoch [1/5], Step [4335/5873], Loss: 4.5948\n",
      "Epoch [1/5], Step [4336/5873], Loss: 4.1065\n",
      "Epoch [1/5], Step [4337/5873], Loss: 4.7219\n",
      "Epoch [1/5], Step [4338/5873], Loss: 4.4703\n",
      "Epoch [1/5], Step [4339/5873], Loss: 3.9531\n",
      "Epoch [1/5], Step [4340/5873], Loss: 2.8484\n",
      "Epoch [1/5], Step [4341/5873], Loss: 2.2749\n",
      "Epoch [1/5], Step [4342/5873], Loss: 4.9610\n",
      "Epoch [1/5], Step [4343/5873], Loss: 3.8483\n",
      "Epoch [1/5], Step [4344/5873], Loss: 2.6906\n",
      "Epoch [1/5], Step [4345/5873], Loss: 4.4834\n",
      "Epoch [1/5], Step [4346/5873], Loss: 4.9200\n",
      "Epoch [1/5], Step [4347/5873], Loss: 3.9710\n",
      "Epoch [1/5], Step [4348/5873], Loss: 4.9007\n",
      "Epoch [1/5], Step [4349/5873], Loss: 4.0576\n",
      "Epoch [1/5], Step [4350/5873], Loss: 3.4502\n",
      "Epoch [1/5], Step [4351/5873], Loss: 3.9672\n",
      "Epoch [1/5], Step [4352/5873], Loss: 4.3155\n",
      "Epoch [1/5], Step [4353/5873], Loss: 4.0488\n",
      "Epoch [1/5], Step [4354/5873], Loss: 2.5202\n",
      "Epoch [1/5], Step [4355/5873], Loss: 1.1237\n",
      "Epoch [1/5], Step [4356/5873], Loss: 4.6361\n",
      "Epoch [1/5], Step [4357/5873], Loss: 3.9194\n",
      "Epoch [1/5], Step [4358/5873], Loss: 3.7805\n",
      "Epoch [1/5], Step [4359/5873], Loss: 4.8176\n",
      "Epoch [1/5], Step [4360/5873], Loss: 5.5543\n",
      "Epoch [1/5], Step [4361/5873], Loss: 4.4827\n",
      "Epoch [1/5], Step [4362/5873], Loss: 4.7891\n",
      "Epoch [1/5], Step [4363/5873], Loss: 3.3349\n",
      "Epoch [1/5], Step [4364/5873], Loss: 3.2549\n",
      "Epoch [1/5], Step [4365/5873], Loss: 3.3353\n",
      "Epoch [1/5], Step [4366/5873], Loss: 1.5581\n",
      "Epoch [1/5], Step [4367/5873], Loss: 4.0980\n",
      "Epoch [1/5], Step [4368/5873], Loss: 4.1103\n",
      "Epoch [1/5], Step [4369/5873], Loss: 2.6882\n",
      "Epoch [1/5], Step [4370/5873], Loss: 2.8399\n",
      "Epoch [1/5], Step [4371/5873], Loss: 4.9582\n",
      "Epoch [1/5], Step [4372/5873], Loss: 4.3557\n",
      "Epoch [1/5], Step [4373/5873], Loss: 4.0907\n",
      "Epoch [1/5], Step [4374/5873], Loss: 4.3953\n",
      "Epoch [1/5], Step [4375/5873], Loss: 1.0369\n",
      "Epoch [1/5], Step [4376/5873], Loss: 4.5703\n",
      "Epoch [1/5], Step [4377/5873], Loss: 3.2771\n",
      "Epoch [1/5], Step [4378/5873], Loss: 3.7785\n",
      "Epoch [1/5], Step [4379/5873], Loss: 3.9972\n",
      "Epoch [1/5], Step [4380/5873], Loss: 1.9898\n",
      "Epoch [1/5], Step [4381/5873], Loss: 4.7491\n",
      "Epoch [1/5], Step [4382/5873], Loss: 3.1928\n",
      "Epoch [1/5], Step [4383/5873], Loss: 3.8238\n",
      "Epoch [1/5], Step [4384/5873], Loss: 3.2691\n",
      "Epoch [1/5], Step [4385/5873], Loss: 3.0767\n",
      "Epoch [1/5], Step [4386/5873], Loss: 4.1982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4387/5873], Loss: 4.9673\n",
      "Epoch [1/5], Step [4388/5873], Loss: 4.1076\n",
      "Epoch [1/5], Step [4389/5873], Loss: 3.5078\n",
      "Epoch [1/5], Step [4390/5873], Loss: 4.1054\n",
      "Epoch [1/5], Step [4391/5873], Loss: 2.6467\n",
      "Epoch [1/5], Step [4392/5873], Loss: 4.7563\n",
      "Epoch [1/5], Step [4393/5873], Loss: 4.8980\n",
      "Epoch [1/5], Step [4394/5873], Loss: 3.5760\n",
      "Epoch [1/5], Step [4395/5873], Loss: 3.5401\n",
      "Epoch [1/5], Step [4396/5873], Loss: 2.8487\n",
      "Epoch [1/5], Step [4397/5873], Loss: 0.7616\n",
      "Epoch [1/5], Step [4398/5873], Loss: 2.5457\n",
      "Epoch [1/5], Step [4399/5873], Loss: 3.3287\n",
      "Epoch [1/5], Step [4400/5873], Loss: 3.5702\n",
      "Epoch [1/5], Step [4401/5873], Loss: 5.4310\n",
      "Epoch [1/5], Step [4402/5873], Loss: 4.2032\n",
      "Epoch [1/5], Step [4403/5873], Loss: 4.5778\n",
      "Epoch [1/5], Step [4404/5873], Loss: 4.2439\n",
      "Epoch [1/5], Step [4405/5873], Loss: 3.1799\n",
      "Epoch [1/5], Step [4406/5873], Loss: 3.2469\n",
      "Epoch [1/5], Step [4407/5873], Loss: 4.7936\n",
      "Epoch [1/5], Step [4408/5873], Loss: 3.5361\n",
      "Epoch [1/5], Step [4409/5873], Loss: 4.8944\n",
      "Epoch [1/5], Step [4410/5873], Loss: 4.1044\n",
      "Epoch [1/5], Step [4411/5873], Loss: 5.3272\n",
      "Epoch [1/5], Step [4412/5873], Loss: 4.8385\n",
      "Epoch [1/5], Step [4413/5873], Loss: 4.5340\n",
      "Epoch [1/5], Step [4414/5873], Loss: 4.3445\n",
      "Epoch [1/5], Step [4415/5873], Loss: 1.6565\n",
      "Epoch [1/5], Step [4416/5873], Loss: 5.9407\n",
      "Epoch [1/5], Step [4417/5873], Loss: 5.6937\n",
      "Epoch [1/5], Step [4418/5873], Loss: 1.6975\n",
      "Epoch [1/5], Step [4419/5873], Loss: 4.5691\n",
      "Epoch [1/5], Step [4420/5873], Loss: 4.7417\n",
      "Epoch [1/5], Step [4421/5873], Loss: 2.0743\n",
      "Epoch [1/5], Step [4422/5873], Loss: 3.5119\n",
      "Epoch [1/5], Step [4423/5873], Loss: 3.4297\n",
      "Epoch [1/5], Step [4424/5873], Loss: 3.3903\n",
      "Epoch [1/5], Step [4425/5873], Loss: 4.1672\n",
      "Epoch [1/5], Step [4426/5873], Loss: 5.0427\n",
      "Epoch [1/5], Step [4427/5873], Loss: 3.6854\n",
      "Epoch [1/5], Step [4428/5873], Loss: 3.7119\n",
      "Epoch [1/5], Step [4429/5873], Loss: 4.6613\n",
      "Epoch [1/5], Step [4430/5873], Loss: 4.9148\n",
      "Epoch [1/5], Step [4431/5873], Loss: 3.7277\n",
      "Epoch [1/5], Step [4432/5873], Loss: 3.2234\n",
      "Epoch [1/5], Step [4433/5873], Loss: 5.1757\n",
      "Epoch [1/5], Step [4434/5873], Loss: 5.4258\n",
      "Epoch [1/5], Step [4435/5873], Loss: 2.3419\n",
      "Epoch [1/5], Step [4436/5873], Loss: 3.7273\n",
      "Epoch [1/5], Step [4437/5873], Loss: 4.2948\n",
      "Epoch [1/5], Step [4438/5873], Loss: 2.4703\n",
      "Epoch [1/5], Step [4439/5873], Loss: 2.2107\n",
      "Epoch [1/5], Step [4440/5873], Loss: 3.7339\n",
      "Epoch [1/5], Step [4441/5873], Loss: 4.8545\n",
      "Epoch [1/5], Step [4442/5873], Loss: 3.2017\n",
      "Epoch [1/5], Step [4443/5873], Loss: 3.9203\n",
      "Epoch [1/5], Step [4444/5873], Loss: 4.1162\n",
      "Epoch [1/5], Step [4445/5873], Loss: 3.7855\n",
      "Epoch [1/5], Step [4446/5873], Loss: 3.3238\n",
      "Epoch [1/5], Step [4447/5873], Loss: 3.2740\n",
      "Epoch [1/5], Step [4448/5873], Loss: 3.6114\n",
      "Epoch [1/5], Step [4449/5873], Loss: 3.1927\n",
      "Epoch [1/5], Step [4450/5873], Loss: 4.5969\n",
      "Epoch [1/5], Step [4451/5873], Loss: 5.8551\n",
      "Epoch [1/5], Step [4452/5873], Loss: 5.5953\n",
      "Epoch [1/5], Step [4453/5873], Loss: 2.5956\n",
      "Epoch [1/5], Step [4454/5873], Loss: 4.7285\n",
      "Epoch [1/5], Step [4455/5873], Loss: 2.8025\n",
      "Epoch [1/5], Step [4456/5873], Loss: 3.8231\n",
      "Epoch [1/5], Step [4457/5873], Loss: 4.3271\n",
      "Epoch [1/5], Step [4458/5873], Loss: 3.7861\n",
      "Epoch [1/5], Step [4459/5873], Loss: 4.6959\n",
      "Epoch [1/5], Step [4460/5873], Loss: 5.5696\n",
      "Epoch [1/5], Step [4461/5873], Loss: 2.5087\n",
      "Epoch [1/5], Step [4462/5873], Loss: 2.8716\n",
      "Epoch [1/5], Step [4463/5873], Loss: 3.4081\n",
      "Epoch [1/5], Step [4464/5873], Loss: 2.9988\n",
      "Epoch [1/5], Step [4465/5873], Loss: 4.3326\n",
      "Epoch [1/5], Step [4466/5873], Loss: 3.0525\n",
      "Epoch [1/5], Step [4467/5873], Loss: 4.0645\n",
      "Epoch [1/5], Step [4468/5873], Loss: 4.1297\n",
      "Epoch [1/5], Step [4469/5873], Loss: 4.1697\n",
      "Epoch [1/5], Step [4470/5873], Loss: 5.2534\n",
      "Epoch [1/5], Step [4471/5873], Loss: 3.6518\n",
      "Epoch [1/5], Step [4472/5873], Loss: 4.4591\n",
      "Epoch [1/5], Step [4473/5873], Loss: 1.6648\n",
      "Epoch [1/5], Step [4474/5873], Loss: 2.6569\n",
      "Epoch [1/5], Step [4475/5873], Loss: 2.8563\n",
      "Epoch [1/5], Step [4476/5873], Loss: 4.8729\n",
      "Epoch [1/5], Step [4477/5873], Loss: 4.7255\n",
      "Epoch [1/5], Step [4478/5873], Loss: 2.7785\n",
      "Epoch [1/5], Step [4479/5873], Loss: 3.0619\n",
      "Epoch [1/5], Step [4480/5873], Loss: 5.4146\n",
      "Epoch [1/5], Step [4481/5873], Loss: 3.8025\n",
      "Epoch [1/5], Step [4482/5873], Loss: 5.0662\n",
      "Epoch [1/5], Step [4483/5873], Loss: 4.6909\n",
      "Epoch [1/5], Step [4484/5873], Loss: 3.9830\n",
      "Epoch [1/5], Step [4485/5873], Loss: 5.6452\n",
      "Epoch [1/5], Step [4486/5873], Loss: 4.4821\n",
      "Epoch [1/5], Step [4487/5873], Loss: 4.0510\n",
      "Epoch [1/5], Step [4488/5873], Loss: 4.0159\n",
      "Epoch [1/5], Step [4489/5873], Loss: 2.2292\n",
      "Epoch [1/5], Step [4490/5873], Loss: 1.4068\n",
      "Epoch [1/5], Step [4491/5873], Loss: 5.2557\n",
      "Epoch [1/5], Step [4492/5873], Loss: 2.9534\n",
      "Epoch [1/5], Step [4493/5873], Loss: 2.8924\n",
      "Epoch [1/5], Step [4494/5873], Loss: 1.8467\n",
      "Epoch [1/5], Step [4495/5873], Loss: 3.8506\n",
      "Epoch [1/5], Step [4496/5873], Loss: 2.5544\n",
      "Epoch [1/5], Step [4497/5873], Loss: 2.7595\n",
      "Epoch [1/5], Step [4498/5873], Loss: 4.4944\n",
      "Epoch [1/5], Step [4499/5873], Loss: 1.8314\n",
      "Epoch [1/5], Step [4500/5873], Loss: 4.3879\n",
      "Epoch [1/5], Step [4501/5873], Loss: 4.4647\n",
      "Epoch [1/5], Step [4502/5873], Loss: 3.2936\n",
      "Epoch [1/5], Step [4503/5873], Loss: 2.3679\n",
      "Epoch [1/5], Step [4504/5873], Loss: 6.4946\n",
      "Epoch [1/5], Step [4505/5873], Loss: 2.0007\n",
      "Epoch [1/5], Step [4506/5873], Loss: 5.7041\n",
      "Epoch [1/5], Step [4507/5873], Loss: 3.7907\n",
      "Epoch [1/5], Step [4508/5873], Loss: 3.0100\n",
      "Epoch [1/5], Step [4509/5873], Loss: 1.8715\n",
      "Epoch [1/5], Step [4510/5873], Loss: 3.6153\n",
      "Epoch [1/5], Step [4511/5873], Loss: 5.8047\n",
      "Epoch [1/5], Step [4512/5873], Loss: 4.4326\n",
      "Epoch [1/5], Step [4513/5873], Loss: 3.6554\n",
      "Epoch [1/5], Step [4514/5873], Loss: 4.2521\n",
      "Epoch [1/5], Step [4515/5873], Loss: 3.0127\n",
      "Epoch [1/5], Step [4516/5873], Loss: 2.7707\n",
      "Epoch [1/5], Step [4517/5873], Loss: 3.9100\n",
      "Epoch [1/5], Step [4518/5873], Loss: 6.1573\n",
      "Epoch [1/5], Step [4519/5873], Loss: 3.8036\n",
      "Epoch [1/5], Step [4520/5873], Loss: 3.5509\n",
      "Epoch [1/5], Step [4521/5873], Loss: 4.5061\n",
      "Epoch [1/5], Step [4522/5873], Loss: 4.4247\n",
      "Epoch [1/5], Step [4523/5873], Loss: 5.2431\n",
      "Epoch [1/5], Step [4524/5873], Loss: 5.0702\n",
      "Epoch [1/5], Step [4525/5873], Loss: 2.6400\n",
      "Epoch [1/5], Step [4526/5873], Loss: 4.4019\n",
      "Epoch [1/5], Step [4527/5873], Loss: 4.5482\n",
      "Epoch [1/5], Step [4528/5873], Loss: 5.8404\n",
      "Epoch [1/5], Step [4529/5873], Loss: 5.0243\n",
      "Epoch [1/5], Step [4530/5873], Loss: 4.3434\n",
      "Epoch [1/5], Step [4531/5873], Loss: 4.4901\n",
      "Epoch [1/5], Step [4532/5873], Loss: 4.0529\n",
      "Epoch [1/5], Step [4533/5873], Loss: 4.1167\n",
      "Epoch [1/5], Step [4534/5873], Loss: 4.0622\n",
      "Epoch [1/5], Step [4535/5873], Loss: 2.5794\n",
      "Epoch [1/5], Step [4536/5873], Loss: 3.3919\n",
      "Epoch [1/5], Step [4537/5873], Loss: 2.6328\n",
      "Epoch [1/5], Step [4538/5873], Loss: 1.9430\n",
      "Epoch [1/5], Step [4539/5873], Loss: 3.1061\n",
      "Epoch [1/5], Step [4540/5873], Loss: 5.2445\n",
      "Epoch [1/5], Step [4541/5873], Loss: 5.8153\n",
      "Epoch [1/5], Step [4542/5873], Loss: 4.9807\n",
      "Epoch [1/5], Step [4543/5873], Loss: 3.7972\n",
      "Epoch [1/5], Step [4544/5873], Loss: 4.2145\n",
      "Epoch [1/5], Step [4545/5873], Loss: 5.0321\n",
      "Epoch [1/5], Step [4546/5873], Loss: 6.0584\n",
      "Epoch [1/5], Step [4547/5873], Loss: 5.2616\n",
      "Epoch [1/5], Step [4548/5873], Loss: 2.8955\n",
      "Epoch [1/5], Step [4549/5873], Loss: 5.2894\n",
      "Epoch [1/5], Step [4550/5873], Loss: 4.3105\n",
      "Epoch [1/5], Step [4551/5873], Loss: 4.9775\n",
      "Epoch [1/5], Step [4552/5873], Loss: 2.6616\n",
      "Epoch [1/5], Step [4553/5873], Loss: 4.8622\n",
      "Epoch [1/5], Step [4554/5873], Loss: 3.0654\n",
      "Epoch [1/5], Step [4555/5873], Loss: 3.9607\n",
      "Epoch [1/5], Step [4556/5873], Loss: 3.4753\n",
      "Epoch [1/5], Step [4557/5873], Loss: 4.0415\n",
      "Epoch [1/5], Step [4558/5873], Loss: 3.3080\n",
      "Epoch [1/5], Step [4559/5873], Loss: 4.9421\n",
      "Epoch [1/5], Step [4560/5873], Loss: 4.5525\n",
      "Epoch [1/5], Step [4561/5873], Loss: 3.5710\n",
      "Epoch [1/5], Step [4562/5873], Loss: 1.6140\n",
      "Epoch [1/5], Step [4563/5873], Loss: 4.5761\n",
      "Epoch [1/5], Step [4564/5873], Loss: 4.3037\n",
      "Epoch [1/5], Step [4565/5873], Loss: 3.9804\n",
      "Epoch [1/5], Step [4566/5873], Loss: 5.0080\n",
      "Epoch [1/5], Step [4567/5873], Loss: 2.4102\n",
      "Epoch [1/5], Step [4568/5873], Loss: 3.7652\n",
      "Epoch [1/5], Step [4569/5873], Loss: 4.0484\n",
      "Epoch [1/5], Step [4570/5873], Loss: 4.2843\n",
      "Epoch [1/5], Step [4571/5873], Loss: 2.8665\n",
      "Epoch [1/5], Step [4572/5873], Loss: 3.6448\n",
      "Epoch [1/5], Step [4573/5873], Loss: 5.2863\n",
      "Epoch [1/5], Step [4574/5873], Loss: 4.5634\n",
      "Epoch [1/5], Step [4575/5873], Loss: 1.4146\n",
      "Epoch [1/5], Step [4576/5873], Loss: 5.1190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4577/5873], Loss: 5.8054\n",
      "Epoch [1/5], Step [4578/5873], Loss: 4.8864\n",
      "Epoch [1/5], Step [4579/5873], Loss: 5.2575\n",
      "Epoch [1/5], Step [4580/5873], Loss: 4.8670\n",
      "Epoch [1/5], Step [4581/5873], Loss: 5.1426\n",
      "Epoch [1/5], Step [4582/5873], Loss: 3.4644\n",
      "Epoch [1/5], Step [4583/5873], Loss: 4.9582\n",
      "Epoch [1/5], Step [4584/5873], Loss: 3.7854\n",
      "Epoch [1/5], Step [4585/5873], Loss: 3.6591\n",
      "Epoch [1/5], Step [4586/5873], Loss: 3.4447\n",
      "Epoch [1/5], Step [4587/5873], Loss: 3.7213\n",
      "Epoch [1/5], Step [4588/5873], Loss: 4.1718\n",
      "Epoch [1/5], Step [4589/5873], Loss: 3.8368\n",
      "Epoch [1/5], Step [4590/5873], Loss: 3.0945\n",
      "Epoch [1/5], Step [4591/5873], Loss: 2.0834\n",
      "Epoch [1/5], Step [4592/5873], Loss: 2.8884\n",
      "Epoch [1/5], Step [4593/5873], Loss: 5.5359\n",
      "Epoch [1/5], Step [4594/5873], Loss: 5.7856\n",
      "Epoch [1/5], Step [4595/5873], Loss: 3.5641\n",
      "Epoch [1/5], Step [4596/5873], Loss: 3.6099\n",
      "Epoch [1/5], Step [4597/5873], Loss: 3.9267\n",
      "Epoch [1/5], Step [4598/5873], Loss: 4.7682\n",
      "Epoch [1/5], Step [4599/5873], Loss: 4.2980\n",
      "Epoch [1/5], Step [4600/5873], Loss: 6.3370\n",
      "Epoch [1/5], Step [4601/5873], Loss: 5.4484\n",
      "Epoch [1/5], Step [4602/5873], Loss: 5.5036\n",
      "Epoch [1/5], Step [4603/5873], Loss: 5.7363\n",
      "Epoch [1/5], Step [4604/5873], Loss: 3.5996\n",
      "Epoch [1/5], Step [4605/5873], Loss: 4.9020\n",
      "Epoch [1/5], Step [4606/5873], Loss: 4.9563\n",
      "Epoch [1/5], Step [4607/5873], Loss: 4.6888\n",
      "Epoch [1/5], Step [4608/5873], Loss: 3.4833\n",
      "Epoch [1/5], Step [4609/5873], Loss: 6.5290\n",
      "Epoch [1/5], Step [4610/5873], Loss: 4.1677\n",
      "Epoch [1/5], Step [4611/5873], Loss: 2.9055\n",
      "Epoch [1/5], Step [4612/5873], Loss: 3.7064\n",
      "Epoch [1/5], Step [4613/5873], Loss: 4.8350\n",
      "Epoch [1/5], Step [4614/5873], Loss: 5.1850\n",
      "Epoch [1/5], Step [4615/5873], Loss: 3.8850\n",
      "Epoch [1/5], Step [4616/5873], Loss: 4.6318\n",
      "Epoch [1/5], Step [4617/5873], Loss: 3.2068\n",
      "Epoch [1/5], Step [4618/5873], Loss: 4.8501\n",
      "Epoch [1/5], Step [4619/5873], Loss: 4.1887\n",
      "Epoch [1/5], Step [4620/5873], Loss: 4.8618\n",
      "Epoch [1/5], Step [4621/5873], Loss: 3.8065\n",
      "Epoch [1/5], Step [4622/5873], Loss: 2.6380\n",
      "Epoch [1/5], Step [4623/5873], Loss: 4.0803\n",
      "Epoch [1/5], Step [4624/5873], Loss: 6.1781\n",
      "Epoch [1/5], Step [4625/5873], Loss: 5.6929\n",
      "Epoch [1/5], Step [4626/5873], Loss: 4.5514\n",
      "Epoch [1/5], Step [4627/5873], Loss: 5.7409\n",
      "Epoch [1/5], Step [4628/5873], Loss: 4.7645\n",
      "Epoch [1/5], Step [4629/5873], Loss: 5.3382\n",
      "Epoch [1/5], Step [4630/5873], Loss: 5.7956\n",
      "Epoch [1/5], Step [4631/5873], Loss: 5.1296\n",
      "Epoch [1/5], Step [4632/5873], Loss: 4.8824\n",
      "Epoch [1/5], Step [4633/5873], Loss: 5.3749\n",
      "Epoch [1/5], Step [4634/5873], Loss: 3.8956\n",
      "Epoch [1/5], Step [4635/5873], Loss: 4.0938\n",
      "Epoch [1/5], Step [4636/5873], Loss: 5.6029\n",
      "Epoch [1/5], Step [4637/5873], Loss: 5.3538\n",
      "Epoch [1/5], Step [4638/5873], Loss: 1.8737\n",
      "Epoch [1/5], Step [4639/5873], Loss: 3.7002\n",
      "Epoch [1/5], Step [4640/5873], Loss: 4.4696\n",
      "Epoch [1/5], Step [4641/5873], Loss: 2.7466\n",
      "Epoch [1/5], Step [4642/5873], Loss: 4.7121\n",
      "Epoch [1/5], Step [4643/5873], Loss: 3.4113\n",
      "Epoch [1/5], Step [4644/5873], Loss: 4.2705\n",
      "Epoch [1/5], Step [4645/5873], Loss: 0.9398\n",
      "Epoch [1/5], Step [4646/5873], Loss: 4.5411\n",
      "Epoch [1/5], Step [4647/5873], Loss: 3.8454\n",
      "Epoch [1/5], Step [4648/5873], Loss: 5.2288\n",
      "Epoch [1/5], Step [4649/5873], Loss: 3.7866\n",
      "Epoch [1/5], Step [4650/5873], Loss: 4.6255\n",
      "Epoch [1/5], Step [4651/5873], Loss: 4.7982\n",
      "Epoch [1/5], Step [4652/5873], Loss: 3.9262\n",
      "Epoch [1/5], Step [4653/5873], Loss: 4.9788\n",
      "Epoch [1/5], Step [4654/5873], Loss: 4.8827\n",
      "Epoch [1/5], Step [4655/5873], Loss: 4.3079\n",
      "Epoch [1/5], Step [4656/5873], Loss: 4.1532\n",
      "Epoch [1/5], Step [4657/5873], Loss: 4.6688\n",
      "Epoch [1/5], Step [4658/5873], Loss: 6.0130\n",
      "Epoch [1/5], Step [4659/5873], Loss: 3.8216\n",
      "Epoch [1/5], Step [4660/5873], Loss: 4.1601\n",
      "Epoch [1/5], Step [4661/5873], Loss: 5.1992\n",
      "Epoch [1/5], Step [4662/5873], Loss: 5.2802\n",
      "Epoch [1/5], Step [4663/5873], Loss: 2.5584\n",
      "Epoch [1/5], Step [4664/5873], Loss: 4.3238\n",
      "Epoch [1/5], Step [4665/5873], Loss: 4.2074\n",
      "Epoch [1/5], Step [4666/5873], Loss: 3.4002\n",
      "Epoch [1/5], Step [4667/5873], Loss: 4.0103\n",
      "Epoch [1/5], Step [4668/5873], Loss: 4.0565\n",
      "Epoch [1/5], Step [4669/5873], Loss: 4.8492\n",
      "Epoch [1/5], Step [4670/5873], Loss: 4.8405\n",
      "Epoch [1/5], Step [4671/5873], Loss: 4.6461\n",
      "Epoch [1/5], Step [4672/5873], Loss: 4.4576\n",
      "Epoch [1/5], Step [4673/5873], Loss: 4.4458\n",
      "Epoch [1/5], Step [4674/5873], Loss: 3.3351\n",
      "Epoch [1/5], Step [4675/5873], Loss: 3.3347\n",
      "Epoch [1/5], Step [4676/5873], Loss: 5.4538\n",
      "Epoch [1/5], Step [4677/5873], Loss: 4.7138\n",
      "Epoch [1/5], Step [4678/5873], Loss: 4.3566\n",
      "Epoch [1/5], Step [4679/5873], Loss: 5.6433\n",
      "Epoch [1/5], Step [4680/5873], Loss: 3.6649\n",
      "Epoch [1/5], Step [4681/5873], Loss: 4.2672\n",
      "Epoch [1/5], Step [4682/5873], Loss: 3.3085\n",
      "Epoch [1/5], Step [4683/5873], Loss: 3.7070\n",
      "Epoch [1/5], Step [4684/5873], Loss: 5.1367\n",
      "Epoch [1/5], Step [4685/5873], Loss: 3.1368\n",
      "Epoch [1/5], Step [4686/5873], Loss: 4.5746\n",
      "Epoch [1/5], Step [4687/5873], Loss: 3.3724\n",
      "Epoch [1/5], Step [4688/5873], Loss: 4.6318\n",
      "Epoch [1/5], Step [4689/5873], Loss: 4.0099\n",
      "Epoch [1/5], Step [4690/5873], Loss: 5.2063\n",
      "Epoch [1/5], Step [4691/5873], Loss: 2.4528\n",
      "Epoch [1/5], Step [4692/5873], Loss: 2.4434\n",
      "Epoch [1/5], Step [4693/5873], Loss: 4.6796\n",
      "Epoch [1/5], Step [4694/5873], Loss: 4.2132\n",
      "Epoch [1/5], Step [4695/5873], Loss: 5.1898\n",
      "Epoch [1/5], Step [4696/5873], Loss: 5.6708\n",
      "Epoch [1/5], Step [4697/5873], Loss: 4.1802\n",
      "Epoch [1/5], Step [4698/5873], Loss: 3.3432\n",
      "Epoch [1/5], Step [4699/5873], Loss: 4.9970\n",
      "Epoch [1/5], Step [4700/5873], Loss: 3.1452\n",
      "Epoch [1/5], Step [4701/5873], Loss: 4.2156\n",
      "Epoch [1/5], Step [4702/5873], Loss: 4.6105\n",
      "Epoch [1/5], Step [4703/5873], Loss: 4.4103\n",
      "Epoch [1/5], Step [4704/5873], Loss: 4.2024\n",
      "Epoch [1/5], Step [4705/5873], Loss: 5.5229\n",
      "Epoch [1/5], Step [4706/5873], Loss: 3.6526\n",
      "Epoch [1/5], Step [4707/5873], Loss: 3.8596\n",
      "Epoch [1/5], Step [4708/5873], Loss: 4.7648\n",
      "Epoch [1/5], Step [4709/5873], Loss: 4.4407\n",
      "Epoch [1/5], Step [4710/5873], Loss: 2.9137\n",
      "Epoch [1/5], Step [4711/5873], Loss: 2.9562\n",
      "Epoch [1/5], Step [4712/5873], Loss: 3.7468\n",
      "Epoch [1/5], Step [4713/5873], Loss: 4.9748\n",
      "Epoch [1/5], Step [4714/5873], Loss: 2.9117\n",
      "Epoch [1/5], Step [4715/5873], Loss: 3.5745\n",
      "Epoch [1/5], Step [4716/5873], Loss: 3.3631\n",
      "Epoch [1/5], Step [4717/5873], Loss: 3.5632\n",
      "Epoch [1/5], Step [4718/5873], Loss: 5.4093\n",
      "Epoch [1/5], Step [4719/5873], Loss: 5.2429\n",
      "Epoch [1/5], Step [4720/5873], Loss: 2.7239\n",
      "Epoch [1/5], Step [4721/5873], Loss: 4.3508\n",
      "Epoch [1/5], Step [4722/5873], Loss: 4.7668\n",
      "Epoch [1/5], Step [4723/5873], Loss: 4.8311\n",
      "Epoch [1/5], Step [4724/5873], Loss: 4.8953\n",
      "Epoch [1/5], Step [4725/5873], Loss: 4.2478\n",
      "Epoch [1/5], Step [4726/5873], Loss: 4.4611\n",
      "Epoch [1/5], Step [4727/5873], Loss: 4.4433\n",
      "Epoch [1/5], Step [4728/5873], Loss: 3.9778\n",
      "Epoch [1/5], Step [4729/5873], Loss: 4.8067\n",
      "Epoch [1/5], Step [4730/5873], Loss: 4.6171\n",
      "Epoch [1/5], Step [4731/5873], Loss: 1.2811\n",
      "Epoch [1/5], Step [4732/5873], Loss: 0.7151\n",
      "Epoch [1/5], Step [4733/5873], Loss: 3.3212\n",
      "Epoch [1/5], Step [4734/5873], Loss: 4.4149\n",
      "Epoch [1/5], Step [4735/5873], Loss: 4.8319\n",
      "Epoch [1/5], Step [4736/5873], Loss: 4.0288\n",
      "Epoch [1/5], Step [4737/5873], Loss: 4.5241\n",
      "Epoch [1/5], Step [4738/5873], Loss: 4.4926\n",
      "Epoch [1/5], Step [4739/5873], Loss: 4.7999\n",
      "Epoch [1/5], Step [4740/5873], Loss: 5.0907\n",
      "Epoch [1/5], Step [4741/5873], Loss: 4.0718\n",
      "Epoch [1/5], Step [4742/5873], Loss: 4.7357\n",
      "Epoch [1/5], Step [4743/5873], Loss: 3.5444\n",
      "Epoch [1/5], Step [4744/5873], Loss: 4.1073\n",
      "Epoch [1/5], Step [4745/5873], Loss: 4.9510\n",
      "Epoch [1/5], Step [4746/5873], Loss: 5.0059\n",
      "Epoch [1/5], Step [4747/5873], Loss: 3.6880\n",
      "Epoch [1/5], Step [4748/5873], Loss: 2.7116\n",
      "Epoch [1/5], Step [4749/5873], Loss: 4.6143\n",
      "Epoch [1/5], Step [4750/5873], Loss: 4.6265\n",
      "Epoch [1/5], Step [4751/5873], Loss: 4.4627\n",
      "Epoch [1/5], Step [4752/5873], Loss: 3.1590\n",
      "Epoch [1/5], Step [4753/5873], Loss: 4.9167\n",
      "Epoch [1/5], Step [4754/5873], Loss: 4.0147\n",
      "Epoch [1/5], Step [4755/5873], Loss: 4.0968\n",
      "Epoch [1/5], Step [4756/5873], Loss: 4.8084\n",
      "Epoch [1/5], Step [4757/5873], Loss: 5.2990\n",
      "Epoch [1/5], Step [4758/5873], Loss: 4.8982\n",
      "Epoch [1/5], Step [4759/5873], Loss: 3.0040\n",
      "Epoch [1/5], Step [4760/5873], Loss: 4.3430\n",
      "Epoch [1/5], Step [4761/5873], Loss: 3.2174\n",
      "Epoch [1/5], Step [4762/5873], Loss: 4.3233\n",
      "Epoch [1/5], Step [4763/5873], Loss: 3.6996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4764/5873], Loss: 3.3869\n",
      "Epoch [1/5], Step [4765/5873], Loss: 4.7922\n",
      "Epoch [1/5], Step [4766/5873], Loss: 4.2344\n",
      "Epoch [1/5], Step [4767/5873], Loss: 3.0269\n",
      "Epoch [1/5], Step [4768/5873], Loss: 3.3667\n",
      "Epoch [1/5], Step [4769/5873], Loss: 3.8193\n",
      "Epoch [1/5], Step [4770/5873], Loss: 3.5248\n",
      "Epoch [1/5], Step [4771/5873], Loss: 3.7622\n",
      "Epoch [1/5], Step [4772/5873], Loss: 4.8376\n",
      "Epoch [1/5], Step [4773/5873], Loss: 3.7749\n",
      "Epoch [1/5], Step [4774/5873], Loss: 4.7777\n",
      "Epoch [1/5], Step [4775/5873], Loss: 4.0859\n",
      "Epoch [1/5], Step [4776/5873], Loss: 4.0448\n",
      "Epoch [1/5], Step [4777/5873], Loss: 3.1270\n",
      "Epoch [1/5], Step [4778/5873], Loss: 2.1603\n",
      "Epoch [1/5], Step [4779/5873], Loss: 3.7713\n",
      "Epoch [1/5], Step [4780/5873], Loss: 4.3995\n",
      "Epoch [1/5], Step [4781/5873], Loss: 3.4494\n",
      "Epoch [1/5], Step [4782/5873], Loss: 2.4078\n",
      "Epoch [1/5], Step [4783/5873], Loss: 2.9884\n",
      "Epoch [1/5], Step [4784/5873], Loss: 4.4539\n",
      "Epoch [1/5], Step [4785/5873], Loss: 4.0373\n",
      "Epoch [1/5], Step [4786/5873], Loss: 3.9933\n",
      "Epoch [1/5], Step [4787/5873], Loss: 2.2411\n",
      "Epoch [1/5], Step [4788/5873], Loss: 5.7285\n",
      "Epoch [1/5], Step [4789/5873], Loss: 4.8908\n",
      "Epoch [1/5], Step [4790/5873], Loss: 3.7658\n",
      "Epoch [1/5], Step [4791/5873], Loss: 3.6135\n",
      "Epoch [1/5], Step [4792/5873], Loss: 3.4123\n",
      "Epoch [1/5], Step [4793/5873], Loss: 2.3041\n",
      "Epoch [1/5], Step [4794/5873], Loss: 5.3754\n",
      "Epoch [1/5], Step [4795/5873], Loss: 5.1671\n",
      "Epoch [1/5], Step [4796/5873], Loss: 5.1596\n",
      "Epoch [1/5], Step [4797/5873], Loss: 4.9073\n",
      "Epoch [1/5], Step [4798/5873], Loss: 3.8291\n",
      "Epoch [1/5], Step [4799/5873], Loss: 4.6100\n",
      "Epoch [1/5], Step [4800/5873], Loss: 3.4672\n",
      "Epoch [1/5], Step [4801/5873], Loss: 4.7172\n",
      "Epoch [1/5], Step [4802/5873], Loss: 2.7482\n",
      "Epoch [1/5], Step [4803/5873], Loss: 3.3974\n",
      "Epoch [1/5], Step [4804/5873], Loss: 3.2139\n",
      "Epoch [1/5], Step [4805/5873], Loss: 4.9626\n",
      "Epoch [1/5], Step [4806/5873], Loss: 4.5618\n",
      "Epoch [1/5], Step [4807/5873], Loss: 2.2296\n",
      "Epoch [1/5], Step [4808/5873], Loss: 2.8369\n",
      "Epoch [1/5], Step [4809/5873], Loss: 2.2400\n",
      "Epoch [1/5], Step [4810/5873], Loss: 3.4092\n",
      "Epoch [1/5], Step [4811/5873], Loss: 1.5589\n",
      "Epoch [1/5], Step [4812/5873], Loss: 3.6336\n",
      "Epoch [1/5], Step [4813/5873], Loss: 1.5347\n",
      "Epoch [1/5], Step [4814/5873], Loss: 5.2522\n",
      "Epoch [1/5], Step [4815/5873], Loss: 3.5329\n",
      "Epoch [1/5], Step [4816/5873], Loss: 3.2753\n",
      "Epoch [1/5], Step [4817/5873], Loss: 3.7126\n",
      "Epoch [1/5], Step [4818/5873], Loss: 2.7053\n",
      "Epoch [1/5], Step [4819/5873], Loss: 4.7263\n",
      "Epoch [1/5], Step [4820/5873], Loss: 2.4939\n",
      "Epoch [1/5], Step [4821/5873], Loss: 4.3587\n",
      "Epoch [1/5], Step [4822/5873], Loss: 3.9805\n",
      "Epoch [1/5], Step [4823/5873], Loss: 1.4857\n",
      "Epoch [1/5], Step [4824/5873], Loss: 4.3985\n",
      "Epoch [1/5], Step [4825/5873], Loss: 3.2945\n",
      "Epoch [1/5], Step [4826/5873], Loss: 4.9004\n",
      "Epoch [1/5], Step [4827/5873], Loss: 5.5641\n",
      "Epoch [1/5], Step [4828/5873], Loss: 2.3118\n",
      "Epoch [1/5], Step [4829/5873], Loss: 4.1171\n",
      "Epoch [1/5], Step [4830/5873], Loss: 3.0550\n",
      "Epoch [1/5], Step [4831/5873], Loss: 5.2050\n",
      "Epoch [1/5], Step [4832/5873], Loss: 3.6576\n",
      "Epoch [1/5], Step [4833/5873], Loss: 1.5803\n",
      "Epoch [1/5], Step [4834/5873], Loss: 1.5545\n",
      "Epoch [1/5], Step [4835/5873], Loss: 1.7518\n",
      "Epoch [1/5], Step [4836/5873], Loss: 3.2314\n",
      "Epoch [1/5], Step [4837/5873], Loss: 5.6778\n",
      "Epoch [1/5], Step [4838/5873], Loss: 4.8355\n",
      "Epoch [1/5], Step [4839/5873], Loss: 3.6739\n",
      "Epoch [1/5], Step [4840/5873], Loss: 5.0089\n",
      "Epoch [1/5], Step [4841/5873], Loss: 4.3571\n",
      "Epoch [1/5], Step [4842/5873], Loss: 2.5045\n",
      "Epoch [1/5], Step [4843/5873], Loss: 1.0758\n",
      "Epoch [1/5], Step [4844/5873], Loss: 4.5169\n",
      "Epoch [1/5], Step [4845/5873], Loss: 2.5674\n",
      "Epoch [1/5], Step [4846/5873], Loss: 3.5941\n",
      "Epoch [1/5], Step [4847/5873], Loss: 3.6265\n",
      "Epoch [1/5], Step [4848/5873], Loss: 3.9718\n",
      "Epoch [1/5], Step [4849/5873], Loss: 2.4228\n",
      "Epoch [1/5], Step [4850/5873], Loss: 3.0815\n",
      "Epoch [1/5], Step [4851/5873], Loss: 5.0758\n",
      "Epoch [1/5], Step [4852/5873], Loss: 4.8570\n",
      "Epoch [1/5], Step [4853/5873], Loss: 3.7558\n",
      "Epoch [1/5], Step [4854/5873], Loss: 1.2662\n",
      "Epoch [1/5], Step [4855/5873], Loss: 2.1700\n",
      "Epoch [1/5], Step [4856/5873], Loss: 3.4719\n",
      "Epoch [1/5], Step [4857/5873], Loss: 3.5009\n",
      "Epoch [1/5], Step [4858/5873], Loss: 3.7406\n",
      "Epoch [1/5], Step [4859/5873], Loss: 4.2456\n",
      "Epoch [1/5], Step [4860/5873], Loss: 2.9539\n",
      "Epoch [1/5], Step [4861/5873], Loss: 1.9101\n",
      "Epoch [1/5], Step [4862/5873], Loss: 6.2004\n",
      "Epoch [1/5], Step [4863/5873], Loss: 1.2486\n",
      "Epoch [1/5], Step [4864/5873], Loss: 1.6175\n",
      "Epoch [1/5], Step [4865/5873], Loss: 3.4989\n",
      "Epoch [1/5], Step [4866/5873], Loss: 3.3811\n",
      "Epoch [1/5], Step [4867/5873], Loss: 2.8880\n",
      "Epoch [1/5], Step [4868/5873], Loss: 3.8069\n",
      "Epoch [1/5], Step [4869/5873], Loss: 4.6077\n",
      "Epoch [1/5], Step [4870/5873], Loss: 4.2338\n",
      "Epoch [1/5], Step [4871/5873], Loss: 3.4596\n",
      "Epoch [1/5], Step [4872/5873], Loss: 6.0824\n",
      "Epoch [1/5], Step [4873/5873], Loss: 4.5641\n",
      "Epoch [1/5], Step [4874/5873], Loss: 5.0951\n",
      "Epoch [1/5], Step [4875/5873], Loss: 4.9360\n",
      "Epoch [1/5], Step [4876/5873], Loss: 2.3608\n",
      "Epoch [1/5], Step [4877/5873], Loss: 2.0899\n",
      "Epoch [1/5], Step [4878/5873], Loss: 1.4317\n",
      "Epoch [1/5], Step [4879/5873], Loss: 4.3663\n",
      "Epoch [1/5], Step [4880/5873], Loss: 5.4446\n",
      "Epoch [1/5], Step [4881/5873], Loss: 3.6599\n",
      "Epoch [1/5], Step [4882/5873], Loss: 4.4293\n",
      "Epoch [1/5], Step [4883/5873], Loss: 3.4273\n",
      "Epoch [1/5], Step [4884/5873], Loss: 2.9931\n",
      "Epoch [1/5], Step [4885/5873], Loss: 3.1231\n",
      "Epoch [1/5], Step [4886/5873], Loss: 5.4801\n",
      "Epoch [1/5], Step [4887/5873], Loss: 4.6765\n",
      "Epoch [1/5], Step [4888/5873], Loss: 3.5682\n",
      "Epoch [1/5], Step [4889/5873], Loss: 3.8927\n",
      "Epoch [1/5], Step [4890/5873], Loss: 6.9096\n",
      "Epoch [1/5], Step [4891/5873], Loss: 2.7944\n",
      "Epoch [1/5], Step [4892/5873], Loss: 4.7451\n",
      "Epoch [1/5], Step [4893/5873], Loss: 1.8615\n",
      "Epoch [1/5], Step [4894/5873], Loss: 1.3955\n",
      "Epoch [1/5], Step [4895/5873], Loss: 3.5970\n",
      "Epoch [1/5], Step [4896/5873], Loss: 5.3754\n",
      "Epoch [1/5], Step [4897/5873], Loss: 2.9196\n",
      "Epoch [1/5], Step [4898/5873], Loss: 5.1164\n",
      "Epoch [1/5], Step [4899/5873], Loss: 4.8856\n",
      "Epoch [1/5], Step [4900/5873], Loss: 3.3900\n",
      "Epoch [1/5], Step [4901/5873], Loss: 5.6219\n",
      "Epoch [1/5], Step [4902/5873], Loss: 4.0287\n",
      "Epoch [1/5], Step [4903/5873], Loss: 4.0436\n",
      "Epoch [1/5], Step [4904/5873], Loss: 2.7864\n",
      "Epoch [1/5], Step [4905/5873], Loss: 4.3301\n",
      "Epoch [1/5], Step [4906/5873], Loss: 3.3771\n",
      "Epoch [1/5], Step [4907/5873], Loss: 1.5070\n",
      "Epoch [1/5], Step [4908/5873], Loss: 6.2730\n",
      "Epoch [1/5], Step [4909/5873], Loss: 5.7405\n",
      "Epoch [1/5], Step [4910/5873], Loss: 6.0367\n",
      "Epoch [1/5], Step [4911/5873], Loss: 1.7641\n",
      "Epoch [1/5], Step [4912/5873], Loss: 5.6111\n",
      "Epoch [1/5], Step [4913/5873], Loss: 5.0318\n",
      "Epoch [1/5], Step [4914/5873], Loss: 2.3013\n",
      "Epoch [1/5], Step [4915/5873], Loss: 3.8385\n",
      "Epoch [1/5], Step [4916/5873], Loss: 2.4477\n",
      "Epoch [1/5], Step [4917/5873], Loss: 6.2871\n",
      "Epoch [1/5], Step [4918/5873], Loss: 1.6283\n",
      "Epoch [1/5], Step [4919/5873], Loss: 5.6131\n",
      "Epoch [1/5], Step [4920/5873], Loss: 4.4971\n",
      "Epoch [1/5], Step [4921/5873], Loss: 4.6351\n",
      "Epoch [1/5], Step [4922/5873], Loss: 4.7492\n",
      "Epoch [1/5], Step [4923/5873], Loss: 4.9252\n",
      "Epoch [1/5], Step [4924/5873], Loss: 5.1796\n",
      "Epoch [1/5], Step [4925/5873], Loss: 4.3258\n",
      "Epoch [1/5], Step [4926/5873], Loss: 2.2935\n",
      "Epoch [1/5], Step [4927/5873], Loss: 3.2877\n",
      "Epoch [1/5], Step [4928/5873], Loss: 2.2185\n",
      "Epoch [1/5], Step [4929/5873], Loss: 2.8193\n",
      "Epoch [1/5], Step [4930/5873], Loss: 3.9184\n",
      "Epoch [1/5], Step [4931/5873], Loss: 4.0123\n",
      "Epoch [1/5], Step [4932/5873], Loss: 5.8663\n",
      "Epoch [1/5], Step [4933/5873], Loss: 3.9988\n",
      "Epoch [1/5], Step [4934/5873], Loss: 2.7218\n",
      "Epoch [1/5], Step [4935/5873], Loss: 4.4123\n",
      "Epoch [1/5], Step [4936/5873], Loss: 4.3075\n",
      "Epoch [1/5], Step [4937/5873], Loss: 1.4076\n",
      "Epoch [1/5], Step [4938/5873], Loss: 5.3783\n",
      "Epoch [1/5], Step [4939/5873], Loss: 3.0970\n",
      "Epoch [1/5], Step [4940/5873], Loss: 4.5182\n",
      "Epoch [1/5], Step [4941/5873], Loss: 0.9103\n",
      "Epoch [1/5], Step [4942/5873], Loss: 3.6087\n",
      "Epoch [1/5], Step [4943/5873], Loss: 1.7702\n",
      "Epoch [1/5], Step [4944/5873], Loss: 4.7002\n",
      "Epoch [1/5], Step [4945/5873], Loss: 3.2410\n",
      "Epoch [1/5], Step [4946/5873], Loss: 2.9525\n",
      "Epoch [1/5], Step [4947/5873], Loss: 4.5024\n",
      "Epoch [1/5], Step [4948/5873], Loss: 4.9758\n",
      "Epoch [1/5], Step [4949/5873], Loss: 2.1948\n",
      "Epoch [1/5], Step [4950/5873], Loss: 5.0293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4951/5873], Loss: 3.5991\n",
      "Epoch [1/5], Step [4952/5873], Loss: 2.8763\n",
      "Epoch [1/5], Step [4953/5873], Loss: 4.2715\n",
      "Epoch [1/5], Step [4954/5873], Loss: 6.2972\n",
      "Epoch [1/5], Step [4955/5873], Loss: 4.4869\n",
      "Epoch [1/5], Step [4956/5873], Loss: 3.3968\n",
      "Epoch [1/5], Step [4957/5873], Loss: 4.4247\n",
      "Epoch [1/5], Step [4958/5873], Loss: 3.3321\n",
      "Epoch [1/5], Step [4959/5873], Loss: 3.0209\n",
      "Epoch [1/5], Step [4960/5873], Loss: 3.6909\n",
      "Epoch [1/5], Step [4961/5873], Loss: 3.5693\n",
      "Epoch [1/5], Step [4962/5873], Loss: 3.6768\n",
      "Epoch [1/5], Step [4963/5873], Loss: 4.0327\n",
      "Epoch [1/5], Step [4964/5873], Loss: 3.1381\n",
      "Epoch [1/5], Step [4965/5873], Loss: 2.2151\n",
      "Epoch [1/5], Step [4966/5873], Loss: 2.1658\n",
      "Epoch [1/5], Step [4967/5873], Loss: 4.2995\n",
      "Epoch [1/5], Step [4968/5873], Loss: 3.6164\n",
      "Epoch [1/5], Step [4969/5873], Loss: 6.5499\n",
      "Epoch [1/5], Step [4970/5873], Loss: 4.8583\n",
      "Epoch [1/5], Step [4971/5873], Loss: 4.3274\n",
      "Epoch [1/5], Step [4972/5873], Loss: 2.2827\n",
      "Epoch [1/5], Step [4973/5873], Loss: 5.9051\n",
      "Epoch [1/5], Step [4974/5873], Loss: 2.1229\n",
      "Epoch [1/5], Step [4975/5873], Loss: 3.7804\n",
      "Epoch [1/5], Step [4976/5873], Loss: 2.6618\n",
      "Epoch [1/5], Step [4977/5873], Loss: 3.1225\n",
      "Epoch [1/5], Step [4978/5873], Loss: 5.0066\n",
      "Epoch [1/5], Step [4979/5873], Loss: 5.5121\n",
      "Epoch [1/5], Step [4980/5873], Loss: 5.2428\n",
      "Epoch [1/5], Step [4981/5873], Loss: 3.3131\n",
      "Epoch [1/5], Step [4982/5873], Loss: 0.9995\n",
      "Epoch [1/5], Step [4983/5873], Loss: 4.2046\n",
      "Epoch [1/5], Step [4984/5873], Loss: 3.7295\n",
      "Epoch [1/5], Step [4985/5873], Loss: 4.0986\n",
      "Epoch [1/5], Step [4986/5873], Loss: 4.9295\n",
      "Epoch [1/5], Step [4987/5873], Loss: 3.8143\n",
      "Epoch [1/5], Step [4988/5873], Loss: 3.7987\n",
      "Epoch [1/5], Step [4989/5873], Loss: 2.9327\n",
      "Epoch [1/5], Step [4990/5873], Loss: 4.0816\n",
      "Epoch [1/5], Step [4991/5873], Loss: 5.3285\n",
      "Epoch [1/5], Step [4992/5873], Loss: 3.1583\n",
      "Epoch [1/5], Step [4993/5873], Loss: 4.5802\n",
      "Epoch [1/5], Step [4994/5873], Loss: 5.1537\n",
      "Epoch [1/5], Step [4995/5873], Loss: 5.7272\n",
      "Epoch [1/5], Step [4996/5873], Loss: 3.5531\n",
      "Epoch [1/5], Step [4997/5873], Loss: 4.0718\n",
      "Epoch [1/5], Step [4998/5873], Loss: 4.3474\n",
      "Epoch [1/5], Step [4999/5873], Loss: 4.4046\n",
      "Epoch [1/5], Step [5000/5873], Loss: 4.2981\n",
      "Epoch [1/5], Step [5001/5873], Loss: 3.0359\n",
      "Epoch [1/5], Step [5002/5873], Loss: 3.8076\n",
      "Epoch [1/5], Step [5003/5873], Loss: 4.4771\n",
      "Epoch [1/5], Step [5004/5873], Loss: 4.5640\n",
      "Epoch [1/5], Step [5005/5873], Loss: 2.7223\n",
      "Epoch [1/5], Step [5006/5873], Loss: 3.0026\n",
      "Epoch [1/5], Step [5007/5873], Loss: 4.2742\n",
      "Epoch [1/5], Step [5008/5873], Loss: 4.4333\n",
      "Epoch [1/5], Step [5009/5873], Loss: 4.0379\n",
      "Epoch [1/5], Step [5010/5873], Loss: 2.6572\n",
      "Epoch [1/5], Step [5011/5873], Loss: 2.7259\n",
      "Epoch [1/5], Step [5012/5873], Loss: 1.4124\n",
      "Epoch [1/5], Step [5013/5873], Loss: 3.7987\n",
      "Epoch [1/5], Step [5014/5873], Loss: 4.7026\n",
      "Epoch [1/5], Step [5015/5873], Loss: 4.2484\n",
      "Epoch [1/5], Step [5016/5873], Loss: 3.0143\n",
      "Epoch [1/5], Step [5017/5873], Loss: 2.3717\n",
      "Epoch [1/5], Step [5018/5873], Loss: 5.1319\n",
      "Epoch [1/5], Step [5019/5873], Loss: 3.6143\n",
      "Epoch [1/5], Step [5020/5873], Loss: 5.5757\n",
      "Epoch [1/5], Step [5021/5873], Loss: 3.5756\n",
      "Epoch [1/5], Step [5022/5873], Loss: 4.1424\n",
      "Epoch [1/5], Step [5023/5873], Loss: 5.6523\n",
      "Epoch [1/5], Step [5024/5873], Loss: 5.0016\n",
      "Epoch [1/5], Step [5025/5873], Loss: 4.3879\n",
      "Epoch [1/5], Step [5026/5873], Loss: 1.7989\n",
      "Epoch [1/5], Step [5027/5873], Loss: 5.5014\n",
      "Epoch [1/5], Step [5028/5873], Loss: 6.0008\n",
      "Epoch [1/5], Step [5029/5873], Loss: 3.8766\n",
      "Epoch [1/5], Step [5030/5873], Loss: 4.2801\n",
      "Epoch [1/5], Step [5031/5873], Loss: 4.0779\n",
      "Epoch [1/5], Step [5032/5873], Loss: 3.4820\n",
      "Epoch [1/5], Step [5033/5873], Loss: 5.1259\n",
      "Epoch [1/5], Step [5034/5873], Loss: 4.3631\n",
      "Epoch [1/5], Step [5035/5873], Loss: 5.3322\n",
      "Epoch [1/5], Step [5036/5873], Loss: 3.4645\n",
      "Epoch [1/5], Step [5037/5873], Loss: 3.4774\n",
      "Epoch [1/5], Step [5038/5873], Loss: 3.7361\n",
      "Epoch [1/5], Step [5039/5873], Loss: 4.2561\n",
      "Epoch [1/5], Step [5040/5873], Loss: 3.1602\n",
      "Epoch [1/5], Step [5041/5873], Loss: 4.1229\n",
      "Epoch [1/5], Step [5042/5873], Loss: 3.6655\n",
      "Epoch [1/5], Step [5043/5873], Loss: 4.2220\n",
      "Epoch [1/5], Step [5044/5873], Loss: 4.0015\n",
      "Epoch [1/5], Step [5045/5873], Loss: 4.6388\n",
      "Epoch [1/5], Step [5046/5873], Loss: 3.3140\n",
      "Epoch [1/5], Step [5047/5873], Loss: 1.1240\n",
      "Epoch [1/5], Step [5048/5873], Loss: 4.3022\n",
      "Epoch [1/5], Step [5049/5873], Loss: 4.6672\n",
      "Epoch [1/5], Step [5050/5873], Loss: 3.5097\n",
      "Epoch [1/5], Step [5051/5873], Loss: 5.1909\n",
      "Epoch [1/5], Step [5052/5873], Loss: 1.4714\n",
      "Epoch [1/5], Step [5053/5873], Loss: 4.8780\n",
      "Epoch [1/5], Step [5054/5873], Loss: 2.2854\n",
      "Epoch [1/5], Step [5055/5873], Loss: 3.2442\n",
      "Epoch [1/5], Step [5056/5873], Loss: 3.9763\n",
      "Epoch [1/5], Step [5057/5873], Loss: 4.2574\n",
      "Epoch [1/5], Step [5058/5873], Loss: 3.5844\n",
      "Epoch [1/5], Step [5059/5873], Loss: 1.1339\n",
      "Epoch [1/5], Step [5060/5873], Loss: 3.2491\n",
      "Epoch [1/5], Step [5061/5873], Loss: 2.9204\n",
      "Epoch [1/5], Step [5062/5873], Loss: 6.4984\n",
      "Epoch [1/5], Step [5063/5873], Loss: 5.0727\n",
      "Epoch [1/5], Step [5064/5873], Loss: 2.4055\n",
      "Epoch [1/5], Step [5065/5873], Loss: 3.4961\n",
      "Epoch [1/5], Step [5066/5873], Loss: 2.3533\n",
      "Epoch [1/5], Step [5067/5873], Loss: 4.1977\n",
      "Epoch [1/5], Step [5068/5873], Loss: 5.3522\n",
      "Epoch [1/5], Step [5069/5873], Loss: 2.3832\n",
      "Epoch [1/5], Step [5070/5873], Loss: 4.4528\n",
      "Epoch [1/5], Step [5071/5873], Loss: 3.7598\n",
      "Epoch [1/5], Step [5072/5873], Loss: 4.0678\n",
      "Epoch [1/5], Step [5073/5873], Loss: 4.5350\n",
      "Epoch [1/5], Step [5074/5873], Loss: 4.2068\n",
      "Epoch [1/5], Step [5075/5873], Loss: 4.2914\n",
      "Epoch [1/5], Step [5076/5873], Loss: 4.7324\n",
      "Epoch [1/5], Step [5077/5873], Loss: 5.7261\n",
      "Epoch [1/5], Step [5078/5873], Loss: 4.9357\n",
      "Epoch [1/5], Step [5079/5873], Loss: 3.0402\n",
      "Epoch [1/5], Step [5080/5873], Loss: 1.6893\n",
      "Epoch [1/5], Step [5081/5873], Loss: 3.2063\n",
      "Epoch [1/5], Step [5082/5873], Loss: 5.6994\n",
      "Epoch [1/5], Step [5083/5873], Loss: 3.1995\n",
      "Epoch [1/5], Step [5084/5873], Loss: 6.2822\n",
      "Epoch [1/5], Step [5085/5873], Loss: 4.9200\n",
      "Epoch [1/5], Step [5086/5873], Loss: 5.1158\n",
      "Epoch [1/5], Step [5087/5873], Loss: 3.8024\n",
      "Epoch [1/5], Step [5088/5873], Loss: 3.7379\n",
      "Epoch [1/5], Step [5089/5873], Loss: 4.2057\n",
      "Epoch [1/5], Step [5090/5873], Loss: 2.7093\n",
      "Epoch [1/5], Step [5091/5873], Loss: 3.9136\n",
      "Epoch [1/5], Step [5092/5873], Loss: 3.2543\n",
      "Epoch [1/5], Step [5093/5873], Loss: 5.3279\n",
      "Epoch [1/5], Step [5094/5873], Loss: 3.7412\n",
      "Epoch [1/5], Step [5095/5873], Loss: 2.6667\n",
      "Epoch [1/5], Step [5096/5873], Loss: 4.9087\n",
      "Epoch [1/5], Step [5097/5873], Loss: 1.8252\n",
      "Epoch [1/5], Step [5098/5873], Loss: 4.5654\n",
      "Epoch [1/5], Step [5099/5873], Loss: 3.1051\n",
      "Epoch [1/5], Step [5100/5873], Loss: 2.3629\n",
      "Epoch [1/5], Step [5101/5873], Loss: 3.6795\n",
      "Epoch [1/5], Step [5102/5873], Loss: 4.1192\n",
      "Epoch [1/5], Step [5103/5873], Loss: 2.7976\n",
      "Epoch [1/5], Step [5104/5873], Loss: 2.6045\n",
      "Epoch [1/5], Step [5105/5873], Loss: 3.1949\n",
      "Epoch [1/5], Step [5106/5873], Loss: 3.7971\n",
      "Epoch [1/5], Step [5107/5873], Loss: 3.5100\n",
      "Epoch [1/5], Step [5108/5873], Loss: 4.1137\n",
      "Epoch [1/5], Step [5109/5873], Loss: 4.6714\n",
      "Epoch [1/5], Step [5110/5873], Loss: 3.1477\n",
      "Epoch [1/5], Step [5111/5873], Loss: 2.8451\n",
      "Epoch [1/5], Step [5112/5873], Loss: 5.1833\n",
      "Epoch [1/5], Step [5113/5873], Loss: 3.9915\n",
      "Epoch [1/5], Step [5114/5873], Loss: 1.5961\n",
      "Epoch [1/5], Step [5115/5873], Loss: 3.3489\n",
      "Epoch [1/5], Step [5116/5873], Loss: 2.6105\n",
      "Epoch [1/5], Step [5117/5873], Loss: 5.6809\n",
      "Epoch [1/5], Step [5118/5873], Loss: 3.3909\n",
      "Epoch [1/5], Step [5119/5873], Loss: 3.7037\n",
      "Epoch [1/5], Step [5120/5873], Loss: 4.0005\n",
      "Epoch [1/5], Step [5121/5873], Loss: 4.6049\n",
      "Epoch [1/5], Step [5122/5873], Loss: 4.4425\n",
      "Epoch [1/5], Step [5123/5873], Loss: 2.0266\n",
      "Epoch [1/5], Step [5124/5873], Loss: 4.4662\n",
      "Epoch [1/5], Step [5125/5873], Loss: 3.1808\n",
      "Epoch [1/5], Step [5126/5873], Loss: 3.0847\n",
      "Epoch [1/5], Step [5127/5873], Loss: 3.0600\n",
      "Epoch [1/5], Step [5128/5873], Loss: 3.2611\n",
      "Epoch [1/5], Step [5129/5873], Loss: 2.5215\n",
      "Epoch [1/5], Step [5130/5873], Loss: 3.5418\n",
      "Epoch [1/5], Step [5131/5873], Loss: 5.8693\n",
      "Epoch [1/5], Step [5132/5873], Loss: 4.7473\n",
      "Epoch [1/5], Step [5133/5873], Loss: 4.5049\n",
      "Epoch [1/5], Step [5134/5873], Loss: 4.4675\n",
      "Epoch [1/5], Step [5135/5873], Loss: 2.9551\n",
      "Epoch [1/5], Step [5136/5873], Loss: 4.7967\n",
      "Epoch [1/5], Step [5137/5873], Loss: 5.5854\n",
      "Epoch [1/5], Step [5138/5873], Loss: 5.0941\n",
      "Epoch [1/5], Step [5139/5873], Loss: 4.0102\n",
      "Epoch [1/5], Step [5140/5873], Loss: 2.7795\n",
      "Epoch [1/5], Step [5141/5873], Loss: 3.5461\n",
      "Epoch [1/5], Step [5142/5873], Loss: 3.2589\n",
      "Epoch [1/5], Step [5143/5873], Loss: 3.7638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5144/5873], Loss: 3.5931\n",
      "Epoch [1/5], Step [5145/5873], Loss: 5.1470\n",
      "Epoch [1/5], Step [5146/5873], Loss: 4.3116\n",
      "Epoch [1/5], Step [5147/5873], Loss: 4.1304\n",
      "Epoch [1/5], Step [5148/5873], Loss: 5.3373\n",
      "Epoch [1/5], Step [5149/5873], Loss: 2.7321\n",
      "Epoch [1/5], Step [5150/5873], Loss: 1.9206\n",
      "Epoch [1/5], Step [5151/5873], Loss: 5.7218\n",
      "Epoch [1/5], Step [5152/5873], Loss: 3.6941\n",
      "Epoch [1/5], Step [5153/5873], Loss: 4.5829\n",
      "Epoch [1/5], Step [5154/5873], Loss: 3.9107\n",
      "Epoch [1/5], Step [5155/5873], Loss: 4.2911\n",
      "Epoch [1/5], Step [5156/5873], Loss: 4.3861\n",
      "Epoch [1/5], Step [5157/5873], Loss: 3.3745\n",
      "Epoch [1/5], Step [5158/5873], Loss: 2.8983\n",
      "Epoch [1/5], Step [5159/5873], Loss: 4.7025\n",
      "Epoch [1/5], Step [5160/5873], Loss: 1.3852\n",
      "Epoch [1/5], Step [5161/5873], Loss: 3.4800\n",
      "Epoch [1/5], Step [5162/5873], Loss: 3.2031\n",
      "Epoch [1/5], Step [5163/5873], Loss: 3.8391\n",
      "Epoch [1/5], Step [5164/5873], Loss: 2.9723\n",
      "Epoch [1/5], Step [5165/5873], Loss: 4.2670\n",
      "Epoch [1/5], Step [5166/5873], Loss: 2.0256\n",
      "Epoch [1/5], Step [5167/5873], Loss: 4.1155\n",
      "Epoch [1/5], Step [5168/5873], Loss: 5.2811\n",
      "Epoch [1/5], Step [5169/5873], Loss: 2.7628\n",
      "Epoch [1/5], Step [5170/5873], Loss: 6.8990\n",
      "Epoch [1/5], Step [5171/5873], Loss: 3.7402\n",
      "Epoch [1/5], Step [5172/5873], Loss: 2.8737\n",
      "Epoch [1/5], Step [5173/5873], Loss: 4.4795\n",
      "Epoch [1/5], Step [5174/5873], Loss: 5.1776\n",
      "Epoch [1/5], Step [5175/5873], Loss: 3.8760\n",
      "Epoch [1/5], Step [5176/5873], Loss: 4.2637\n",
      "Epoch [1/5], Step [5177/5873], Loss: 3.5591\n",
      "Epoch [1/5], Step [5178/5873], Loss: 2.4291\n",
      "Epoch [1/5], Step [5179/5873], Loss: 1.3231\n",
      "Epoch [1/5], Step [5180/5873], Loss: 1.6582\n",
      "Epoch [1/5], Step [5181/5873], Loss: 1.6916\n",
      "Epoch [1/5], Step [5182/5873], Loss: 3.0186\n",
      "Epoch [1/5], Step [5183/5873], Loss: 5.1585\n",
      "Epoch [1/5], Step [5184/5873], Loss: 4.5509\n",
      "Epoch [1/5], Step [5185/5873], Loss: 3.1033\n",
      "Epoch [1/5], Step [5186/5873], Loss: 5.2799\n",
      "Epoch [1/5], Step [5187/5873], Loss: 4.3291\n",
      "Epoch [1/5], Step [5188/5873], Loss: 2.8486\n",
      "Epoch [1/5], Step [5189/5873], Loss: 4.5857\n",
      "Epoch [1/5], Step [5190/5873], Loss: 5.2503\n",
      "Epoch [1/5], Step [5191/5873], Loss: 2.4520\n",
      "Epoch [1/5], Step [5192/5873], Loss: 5.4873\n",
      "Epoch [1/5], Step [5193/5873], Loss: 3.0769\n",
      "Epoch [1/5], Step [5194/5873], Loss: 5.9276\n",
      "Epoch [1/5], Step [5195/5873], Loss: 3.3829\n",
      "Epoch [1/5], Step [5196/5873], Loss: 3.1661\n",
      "Epoch [1/5], Step [5197/5873], Loss: 5.2977\n",
      "Epoch [1/5], Step [5198/5873], Loss: 5.5747\n",
      "Epoch [1/5], Step [5199/5873], Loss: 3.3771\n",
      "Epoch [1/5], Step [5200/5873], Loss: 4.2507\n",
      "Epoch [1/5], Step [5201/5873], Loss: 4.7075\n",
      "Epoch [1/5], Step [5202/5873], Loss: 3.7986\n",
      "Epoch [1/5], Step [5203/5873], Loss: 2.5135\n",
      "Epoch [1/5], Step [5204/5873], Loss: 3.7285\n",
      "Epoch [1/5], Step [5205/5873], Loss: 5.1489\n",
      "Epoch [1/5], Step [5206/5873], Loss: 5.2409\n",
      "Epoch [1/5], Step [5207/5873], Loss: 4.7202\n",
      "Epoch [1/5], Step [5208/5873], Loss: 4.1112\n",
      "Epoch [1/5], Step [5209/5873], Loss: 4.5987\n",
      "Epoch [1/5], Step [5210/5873], Loss: 1.9587\n",
      "Epoch [1/5], Step [5211/5873], Loss: 3.5853\n",
      "Epoch [1/5], Step [5212/5873], Loss: 3.2177\n",
      "Epoch [1/5], Step [5213/5873], Loss: 4.2179\n",
      "Epoch [1/5], Step [5214/5873], Loss: 2.0429\n",
      "Epoch [1/5], Step [5215/5873], Loss: 4.1202\n",
      "Epoch [1/5], Step [5216/5873], Loss: 3.3274\n",
      "Epoch [1/5], Step [5217/5873], Loss: 4.5096\n",
      "Epoch [1/5], Step [5218/5873], Loss: 2.7409\n",
      "Epoch [1/5], Step [5219/5873], Loss: 2.6305\n",
      "Epoch [1/5], Step [5220/5873], Loss: 4.3186\n",
      "Epoch [1/5], Step [5221/5873], Loss: 4.8016\n",
      "Epoch [1/5], Step [5222/5873], Loss: 2.2589\n",
      "Epoch [1/5], Step [5223/5873], Loss: 4.8671\n",
      "Epoch [1/5], Step [5224/5873], Loss: 4.1796\n",
      "Epoch [1/5], Step [5225/5873], Loss: 4.2570\n",
      "Epoch [1/5], Step [5226/5873], Loss: 5.1182\n",
      "Epoch [1/5], Step [5227/5873], Loss: 2.9251\n",
      "Epoch [1/5], Step [5228/5873], Loss: 4.1070\n",
      "Epoch [1/5], Step [5229/5873], Loss: 4.7747\n",
      "Epoch [1/5], Step [5230/5873], Loss: 3.9190\n",
      "Epoch [1/5], Step [5231/5873], Loss: 4.2004\n",
      "Epoch [1/5], Step [5232/5873], Loss: 5.0365\n",
      "Epoch [1/5], Step [5233/5873], Loss: 5.8481\n",
      "Epoch [1/5], Step [5234/5873], Loss: 4.1515\n",
      "Epoch [1/5], Step [5235/5873], Loss: 4.7780\n",
      "Epoch [1/5], Step [5236/5873], Loss: 4.6419\n",
      "Epoch [1/5], Step [5237/5873], Loss: 3.6933\n",
      "Epoch [1/5], Step [5238/5873], Loss: 4.8859\n",
      "Epoch [1/5], Step [5239/5873], Loss: 4.1060\n",
      "Epoch [1/5], Step [5240/5873], Loss: 3.4180\n",
      "Epoch [1/5], Step [5241/5873], Loss: 4.6669\n",
      "Epoch [1/5], Step [5242/5873], Loss: 3.3119\n",
      "Epoch [1/5], Step [5243/5873], Loss: 4.7721\n",
      "Epoch [1/5], Step [5244/5873], Loss: 4.1774\n",
      "Epoch [1/5], Step [5245/5873], Loss: 3.3631\n",
      "Epoch [1/5], Step [5246/5873], Loss: 2.9572\n",
      "Epoch [1/5], Step [5247/5873], Loss: 1.1316\n",
      "Epoch [1/5], Step [5248/5873], Loss: 4.0596\n",
      "Epoch [1/5], Step [5249/5873], Loss: 2.7039\n",
      "Epoch [1/5], Step [5250/5873], Loss: 2.7359\n",
      "Epoch [1/5], Step [5251/5873], Loss: 4.4030\n",
      "Epoch [1/5], Step [5252/5873], Loss: 2.1488\n",
      "Epoch [1/5], Step [5253/5873], Loss: 3.2881\n",
      "Epoch [1/5], Step [5254/5873], Loss: 3.3951\n",
      "Epoch [1/5], Step [5255/5873], Loss: 2.9362\n",
      "Epoch [1/5], Step [5256/5873], Loss: 1.9891\n",
      "Epoch [1/5], Step [5257/5873], Loss: 4.5304\n",
      "Epoch [1/5], Step [5258/5873], Loss: 4.5532\n",
      "Epoch [1/5], Step [5259/5873], Loss: 2.7215\n",
      "Epoch [1/5], Step [5260/5873], Loss: 4.8388\n",
      "Epoch [1/5], Step [5261/5873], Loss: 5.0529\n",
      "Epoch [1/5], Step [5262/5873], Loss: 4.1905\n",
      "Epoch [1/5], Step [5263/5873], Loss: 1.0975\n",
      "Epoch [1/5], Step [5264/5873], Loss: 4.8088\n",
      "Epoch [1/5], Step [5265/5873], Loss: 3.6709\n",
      "Epoch [1/5], Step [5266/5873], Loss: 5.8910\n",
      "Epoch [1/5], Step [5267/5873], Loss: 3.7875\n",
      "Epoch [1/5], Step [5268/5873], Loss: 2.4601\n",
      "Epoch [1/5], Step [5269/5873], Loss: 5.0243\n",
      "Epoch [1/5], Step [5270/5873], Loss: 2.5944\n",
      "Epoch [1/5], Step [5271/5873], Loss: 2.4020\n",
      "Epoch [1/5], Step [5272/5873], Loss: 3.4107\n",
      "Epoch [1/5], Step [5273/5873], Loss: 4.0590\n",
      "Epoch [1/5], Step [5274/5873], Loss: 3.9090\n",
      "Epoch [1/5], Step [5275/5873], Loss: 3.0959\n",
      "Epoch [1/5], Step [5276/5873], Loss: 1.6190\n",
      "Epoch [1/5], Step [5277/5873], Loss: 4.6730\n",
      "Epoch [1/5], Step [5278/5873], Loss: 4.4488\n",
      "Epoch [1/5], Step [5279/5873], Loss: 3.0688\n",
      "Epoch [1/5], Step [5280/5873], Loss: 3.7679\n",
      "Epoch [1/5], Step [5281/5873], Loss: 1.4566\n",
      "Epoch [1/5], Step [5282/5873], Loss: 4.3858\n",
      "Epoch [1/5], Step [5283/5873], Loss: 5.2374\n",
      "Epoch [1/5], Step [5284/5873], Loss: 4.1378\n",
      "Epoch [1/5], Step [5285/5873], Loss: 3.2584\n",
      "Epoch [1/5], Step [5286/5873], Loss: 4.4103\n",
      "Epoch [1/5], Step [5287/5873], Loss: 3.5531\n",
      "Epoch [1/5], Step [5288/5873], Loss: 3.7575\n",
      "Epoch [1/5], Step [5289/5873], Loss: 5.2460\n",
      "Epoch [1/5], Step [5290/5873], Loss: 2.3856\n",
      "Epoch [1/5], Step [5291/5873], Loss: 4.1057\n",
      "Epoch [1/5], Step [5292/5873], Loss: 4.6328\n",
      "Epoch [1/5], Step [5293/5873], Loss: 2.9307\n",
      "Epoch [1/5], Step [5294/5873], Loss: 2.1017\n",
      "Epoch [1/5], Step [5295/5873], Loss: 5.2588\n",
      "Epoch [1/5], Step [5296/5873], Loss: 2.7744\n",
      "Epoch [1/5], Step [5297/5873], Loss: 3.4442\n",
      "Epoch [1/5], Step [5298/5873], Loss: 5.7043\n",
      "Epoch [1/5], Step [5299/5873], Loss: 2.8594\n",
      "Epoch [1/5], Step [5300/5873], Loss: 2.1717\n",
      "Epoch [1/5], Step [5301/5873], Loss: 4.6526\n",
      "Epoch [1/5], Step [5302/5873], Loss: 4.3325\n",
      "Epoch [1/5], Step [5303/5873], Loss: 3.8633\n",
      "Epoch [1/5], Step [5304/5873], Loss: 4.3663\n",
      "Epoch [1/5], Step [5305/5873], Loss: 3.9556\n",
      "Epoch [1/5], Step [5306/5873], Loss: 3.6619\n",
      "Epoch [1/5], Step [5307/5873], Loss: 2.3663\n",
      "Epoch [1/5], Step [5308/5873], Loss: 3.3363\n",
      "Epoch [1/5], Step [5309/5873], Loss: 4.7449\n",
      "Epoch [1/5], Step [5310/5873], Loss: 4.6880\n",
      "Epoch [1/5], Step [5311/5873], Loss: 4.7906\n",
      "Epoch [1/5], Step [5312/5873], Loss: 5.5885\n",
      "Epoch [1/5], Step [5313/5873], Loss: 3.9152\n",
      "Epoch [1/5], Step [5314/5873], Loss: 3.0343\n",
      "Epoch [1/5], Step [5315/5873], Loss: 3.7483\n",
      "Epoch [1/5], Step [5316/5873], Loss: 3.4269\n",
      "Epoch [1/5], Step [5317/5873], Loss: 3.2906\n",
      "Epoch [1/5], Step [5318/5873], Loss: 5.5338\n",
      "Epoch [1/5], Step [5319/5873], Loss: 4.7286\n",
      "Epoch [1/5], Step [5320/5873], Loss: 2.5642\n",
      "Epoch [1/5], Step [5321/5873], Loss: 1.0521\n",
      "Epoch [1/5], Step [5322/5873], Loss: 3.6078\n",
      "Epoch [1/5], Step [5323/5873], Loss: 4.9859\n",
      "Epoch [1/5], Step [5324/5873], Loss: 3.1932\n",
      "Epoch [1/5], Step [5325/5873], Loss: 3.7423\n",
      "Epoch [1/5], Step [5326/5873], Loss: 2.4227\n",
      "Epoch [1/5], Step [5327/5873], Loss: 5.8765\n",
      "Epoch [1/5], Step [5328/5873], Loss: 4.0572\n",
      "Epoch [1/5], Step [5329/5873], Loss: 3.9234\n",
      "Epoch [1/5], Step [5330/5873], Loss: 4.8270\n",
      "Epoch [1/5], Step [5331/5873], Loss: 3.4653\n",
      "Epoch [1/5], Step [5332/5873], Loss: 4.0751\n",
      "Epoch [1/5], Step [5333/5873], Loss: 4.2380\n",
      "Epoch [1/5], Step [5334/5873], Loss: 3.8990\n",
      "Epoch [1/5], Step [5335/5873], Loss: 1.4160\n",
      "Epoch [1/5], Step [5336/5873], Loss: 3.0902\n",
      "Epoch [1/5], Step [5337/5873], Loss: 4.8621\n",
      "Epoch [1/5], Step [5338/5873], Loss: 3.6008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5339/5873], Loss: 3.8748\n",
      "Epoch [1/5], Step [5340/5873], Loss: 4.3252\n",
      "Epoch [1/5], Step [5341/5873], Loss: 3.9608\n",
      "Epoch [1/5], Step [5342/5873], Loss: 3.3559\n",
      "Epoch [1/5], Step [5343/5873], Loss: 3.5949\n",
      "Epoch [1/5], Step [5344/5873], Loss: 3.8413\n",
      "Epoch [1/5], Step [5345/5873], Loss: 2.9386\n",
      "Epoch [1/5], Step [5346/5873], Loss: 4.9091\n",
      "Epoch [1/5], Step [5347/5873], Loss: 5.2334\n",
      "Epoch [1/5], Step [5348/5873], Loss: 4.8487\n",
      "Epoch [1/5], Step [5349/5873], Loss: 4.5994\n",
      "Epoch [1/5], Step [5350/5873], Loss: 2.1446\n",
      "Epoch [1/5], Step [5351/5873], Loss: 2.6963\n",
      "Epoch [1/5], Step [5352/5873], Loss: 2.6005\n",
      "Epoch [1/5], Step [5353/5873], Loss: 2.2863\n",
      "Epoch [1/5], Step [5354/5873], Loss: 3.5004\n",
      "Epoch [1/5], Step [5355/5873], Loss: 3.3129\n",
      "Epoch [1/5], Step [5356/5873], Loss: 6.6807\n",
      "Epoch [1/5], Step [5357/5873], Loss: 3.1607\n",
      "Epoch [1/5], Step [5358/5873], Loss: 3.7619\n",
      "Epoch [1/5], Step [5359/5873], Loss: 3.9338\n",
      "Epoch [1/5], Step [5360/5873], Loss: 2.7754\n",
      "Epoch [1/5], Step [5361/5873], Loss: 4.1676\n",
      "Epoch [1/5], Step [5362/5873], Loss: 3.6908\n",
      "Epoch [1/5], Step [5363/5873], Loss: 3.5470\n",
      "Epoch [1/5], Step [5364/5873], Loss: 4.4343\n",
      "Epoch [1/5], Step [5365/5873], Loss: 4.0250\n",
      "Epoch [1/5], Step [5366/5873], Loss: 4.0445\n",
      "Epoch [1/5], Step [5367/5873], Loss: 4.4648\n",
      "Epoch [1/5], Step [5368/5873], Loss: 2.8757\n",
      "Epoch [1/5], Step [5369/5873], Loss: 4.6604\n",
      "Epoch [1/5], Step [5370/5873], Loss: 3.9150\n",
      "Epoch [1/5], Step [5371/5873], Loss: 2.4538\n",
      "Epoch [1/5], Step [5372/5873], Loss: 2.8933\n",
      "Epoch [1/5], Step [5373/5873], Loss: 1.2897\n",
      "Epoch [1/5], Step [5374/5873], Loss: 1.6680\n",
      "Epoch [1/5], Step [5375/5873], Loss: 4.9426\n",
      "Epoch [1/5], Step [5376/5873], Loss: 1.1540\n",
      "Epoch [1/5], Step [5377/5873], Loss: 4.9922\n",
      "Epoch [1/5], Step [5378/5873], Loss: 4.8694\n",
      "Epoch [1/5], Step [5379/5873], Loss: 2.6199\n",
      "Epoch [1/5], Step [5380/5873], Loss: 0.8155\n",
      "Epoch [1/5], Step [5381/5873], Loss: 2.6780\n",
      "Epoch [1/5], Step [5382/5873], Loss: 4.0705\n",
      "Epoch [1/5], Step [5383/5873], Loss: 4.1458\n",
      "Epoch [1/5], Step [5384/5873], Loss: 3.1175\n",
      "Epoch [1/5], Step [5385/5873], Loss: 3.8402\n",
      "Epoch [1/5], Step [5386/5873], Loss: 6.5991\n",
      "Epoch [1/5], Step [5387/5873], Loss: 1.9891\n",
      "Epoch [1/5], Step [5388/5873], Loss: 3.1125\n",
      "Epoch [1/5], Step [5389/5873], Loss: 6.1560\n",
      "Epoch [1/5], Step [5390/5873], Loss: 4.8961\n",
      "Epoch [1/5], Step [5391/5873], Loss: 3.4055\n",
      "Epoch [1/5], Step [5392/5873], Loss: 5.9067\n",
      "Epoch [1/5], Step [5393/5873], Loss: 5.2143\n",
      "Epoch [1/5], Step [5394/5873], Loss: 3.4067\n",
      "Epoch [1/5], Step [5395/5873], Loss: 3.0643\n",
      "Epoch [1/5], Step [5396/5873], Loss: 3.9136\n",
      "Epoch [1/5], Step [5397/5873], Loss: 2.2091\n",
      "Epoch [1/5], Step [5398/5873], Loss: 2.9233\n",
      "Epoch [1/5], Step [5399/5873], Loss: 4.3582\n",
      "Epoch [1/5], Step [5400/5873], Loss: 4.1131\n",
      "Epoch [1/5], Step [5401/5873], Loss: 4.3356\n",
      "Epoch [1/5], Step [5402/5873], Loss: 3.6290\n",
      "Epoch [1/5], Step [5403/5873], Loss: 4.6089\n",
      "Epoch [1/5], Step [5404/5873], Loss: 6.5550\n",
      "Epoch [1/5], Step [5405/5873], Loss: 3.8444\n",
      "Epoch [1/5], Step [5406/5873], Loss: 3.0899\n",
      "Epoch [1/5], Step [5407/5873], Loss: 4.7174\n",
      "Epoch [1/5], Step [5408/5873], Loss: 2.3909\n",
      "Epoch [1/5], Step [5409/5873], Loss: 3.8906\n",
      "Epoch [1/5], Step [5410/5873], Loss: 2.7704\n",
      "Epoch [1/5], Step [5411/5873], Loss: 4.5602\n",
      "Epoch [1/5], Step [5412/5873], Loss: 3.3181\n",
      "Epoch [1/5], Step [5413/5873], Loss: 4.8769\n",
      "Epoch [1/5], Step [5414/5873], Loss: 3.7404\n",
      "Epoch [1/5], Step [5415/5873], Loss: 2.6021\n",
      "Epoch [1/5], Step [5416/5873], Loss: 3.7383\n",
      "Epoch [1/5], Step [5417/5873], Loss: 2.6356\n",
      "Epoch [1/5], Step [5418/5873], Loss: 3.6394\n",
      "Epoch [1/5], Step [5419/5873], Loss: 4.0178\n",
      "Epoch [1/5], Step [5420/5873], Loss: 6.0449\n",
      "Epoch [1/5], Step [5421/5873], Loss: 3.0737\n",
      "Epoch [1/5], Step [5422/5873], Loss: 3.9332\n",
      "Epoch [1/5], Step [5423/5873], Loss: 4.6021\n",
      "Epoch [1/5], Step [5424/5873], Loss: 3.9531\n",
      "Epoch [1/5], Step [5425/5873], Loss: 3.0834\n",
      "Epoch [1/5], Step [5426/5873], Loss: 4.6833\n",
      "Epoch [1/5], Step [5427/5873], Loss: 3.8552\n",
      "Epoch [1/5], Step [5428/5873], Loss: 5.0870\n",
      "Epoch [1/5], Step [5429/5873], Loss: 5.0635\n",
      "Epoch [1/5], Step [5430/5873], Loss: 1.8246\n",
      "Epoch [1/5], Step [5431/5873], Loss: 5.5536\n",
      "Epoch [1/5], Step [5432/5873], Loss: 2.6847\n",
      "Epoch [1/5], Step [5433/5873], Loss: 3.6058\n",
      "Epoch [1/5], Step [5434/5873], Loss: 3.5021\n",
      "Epoch [1/5], Step [5435/5873], Loss: 5.4355\n",
      "Epoch [1/5], Step [5436/5873], Loss: 2.8107\n",
      "Epoch [1/5], Step [5437/5873], Loss: 3.5379\n",
      "Epoch [1/5], Step [5438/5873], Loss: 4.6761\n",
      "Epoch [1/5], Step [5439/5873], Loss: 3.6525\n",
      "Epoch [1/5], Step [5440/5873], Loss: 2.7617\n",
      "Epoch [1/5], Step [5441/5873], Loss: 4.9753\n",
      "Epoch [1/5], Step [5442/5873], Loss: 5.2896\n",
      "Epoch [1/5], Step [5443/5873], Loss: 5.1045\n",
      "Epoch [1/5], Step [5444/5873], Loss: 3.9878\n",
      "Epoch [1/5], Step [5445/5873], Loss: 4.2944\n",
      "Epoch [1/5], Step [5446/5873], Loss: 1.6430\n",
      "Epoch [1/5], Step [5447/5873], Loss: 1.9212\n",
      "Epoch [1/5], Step [5448/5873], Loss: 4.1170\n",
      "Epoch [1/5], Step [5449/5873], Loss: 4.4801\n",
      "Epoch [1/5], Step [5450/5873], Loss: 4.4255\n",
      "Epoch [1/5], Step [5451/5873], Loss: 2.8489\n",
      "Epoch [1/5], Step [5452/5873], Loss: 2.7170\n",
      "Epoch [1/5], Step [5453/5873], Loss: 2.5299\n",
      "Epoch [1/5], Step [5454/5873], Loss: 6.0389\n",
      "Epoch [1/5], Step [5455/5873], Loss: 4.3950\n",
      "Epoch [1/5], Step [5456/5873], Loss: 3.7406\n",
      "Epoch [1/5], Step [5457/5873], Loss: 4.6026\n",
      "Epoch [1/5], Step [5458/5873], Loss: 3.6971\n",
      "Epoch [1/5], Step [5459/5873], Loss: 4.3469\n",
      "Epoch [1/5], Step [5460/5873], Loss: 3.6840\n",
      "Epoch [1/5], Step [5461/5873], Loss: 5.3511\n",
      "Epoch [1/5], Step [5462/5873], Loss: 3.5181\n",
      "Epoch [1/5], Step [5463/5873], Loss: 3.9455\n",
      "Epoch [1/5], Step [5464/5873], Loss: 3.4030\n",
      "Epoch [1/5], Step [5465/5873], Loss: 4.6716\n",
      "Epoch [1/5], Step [5466/5873], Loss: 3.8065\n",
      "Epoch [1/5], Step [5467/5873], Loss: 4.2714\n",
      "Epoch [1/5], Step [5468/5873], Loss: 4.3640\n",
      "Epoch [1/5], Step [5469/5873], Loss: 2.5127\n",
      "Epoch [1/5], Step [5470/5873], Loss: 3.8035\n",
      "Epoch [1/5], Step [5471/5873], Loss: 4.1700\n",
      "Epoch [1/5], Step [5472/5873], Loss: 4.9307\n",
      "Epoch [1/5], Step [5473/5873], Loss: 4.4545\n",
      "Epoch [1/5], Step [5474/5873], Loss: 4.7007\n",
      "Epoch [1/5], Step [5475/5873], Loss: 3.8191\n",
      "Epoch [1/5], Step [5476/5873], Loss: 3.8106\n",
      "Epoch [1/5], Step [5477/5873], Loss: 2.7354\n",
      "Epoch [1/5], Step [5478/5873], Loss: 3.6947\n",
      "Epoch [1/5], Step [5479/5873], Loss: 4.7317\n",
      "Epoch [1/5], Step [5480/5873], Loss: 5.1045\n",
      "Epoch [1/5], Step [5481/5873], Loss: 2.9281\n",
      "Epoch [1/5], Step [5482/5873], Loss: 4.9047\n",
      "Epoch [1/5], Step [5483/5873], Loss: 1.3463\n",
      "Epoch [1/5], Step [5484/5873], Loss: 3.1798\n",
      "Epoch [1/5], Step [5485/5873], Loss: 5.7792\n",
      "Epoch [1/5], Step [5486/5873], Loss: 4.0107\n",
      "Epoch [1/5], Step [5487/5873], Loss: 5.7421\n",
      "Epoch [1/5], Step [5488/5873], Loss: 3.4811\n",
      "Epoch [1/5], Step [5489/5873], Loss: 5.6903\n",
      "Epoch [1/5], Step [5490/5873], Loss: 4.5494\n",
      "Epoch [1/5], Step [5491/5873], Loss: 3.6276\n",
      "Epoch [1/5], Step [5492/5873], Loss: 5.2183\n",
      "Epoch [1/5], Step [5493/5873], Loss: 2.0962\n",
      "Epoch [1/5], Step [5494/5873], Loss: 5.5523\n",
      "Epoch [1/5], Step [5495/5873], Loss: 4.9997\n",
      "Epoch [1/5], Step [5496/5873], Loss: 4.1273\n",
      "Epoch [1/5], Step [5497/5873], Loss: 2.9627\n",
      "Epoch [1/5], Step [5498/5873], Loss: 3.3200\n",
      "Epoch [1/5], Step [5499/5873], Loss: 4.0354\n",
      "Epoch [1/5], Step [5500/5873], Loss: 2.9905\n",
      "Epoch [1/5], Step [5501/5873], Loss: 2.9347\n",
      "Epoch [1/5], Step [5502/5873], Loss: 4.6831\n",
      "Epoch [1/5], Step [5503/5873], Loss: 2.8008\n",
      "Epoch [1/5], Step [5504/5873], Loss: 2.5187\n",
      "Epoch [1/5], Step [5505/5873], Loss: 4.1342\n",
      "Epoch [1/5], Step [5506/5873], Loss: 4.0469\n",
      "Epoch [1/5], Step [5507/5873], Loss: 4.2752\n",
      "Epoch [1/5], Step [5508/5873], Loss: 4.6643\n",
      "Epoch [1/5], Step [5509/5873], Loss: 3.0763\n",
      "Epoch [1/5], Step [5510/5873], Loss: 4.3954\n",
      "Epoch [1/5], Step [5511/5873], Loss: 3.8259\n",
      "Epoch [1/5], Step [5512/5873], Loss: 5.3778\n",
      "Epoch [1/5], Step [5513/5873], Loss: 4.9550\n",
      "Epoch [1/5], Step [5514/5873], Loss: 5.3951\n",
      "Epoch [1/5], Step [5515/5873], Loss: 3.5224\n",
      "Epoch [1/5], Step [5516/5873], Loss: 2.1573\n",
      "Epoch [1/5], Step [5517/5873], Loss: 1.6285\n",
      "Epoch [1/5], Step [5518/5873], Loss: 5.1047\n",
      "Epoch [1/5], Step [5519/5873], Loss: 3.7131\n",
      "Epoch [1/5], Step [5520/5873], Loss: 6.1555\n",
      "Epoch [1/5], Step [5521/5873], Loss: 2.5855\n",
      "Epoch [1/5], Step [5522/5873], Loss: 4.7382\n",
      "Epoch [1/5], Step [5523/5873], Loss: 2.8004\n",
      "Epoch [1/5], Step [5524/5873], Loss: 2.3315\n",
      "Epoch [1/5], Step [5525/5873], Loss: 2.9141\n",
      "Epoch [1/5], Step [5526/5873], Loss: 2.8424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5527/5873], Loss: 4.1307\n",
      "Epoch [1/5], Step [5528/5873], Loss: 4.1778\n",
      "Epoch [1/5], Step [5529/5873], Loss: 5.2667\n",
      "Epoch [1/5], Step [5530/5873], Loss: 5.6405\n",
      "Epoch [1/5], Step [5531/5873], Loss: 2.6804\n",
      "Epoch [1/5], Step [5532/5873], Loss: 2.3353\n",
      "Epoch [1/5], Step [5533/5873], Loss: 4.4115\n",
      "Epoch [1/5], Step [5534/5873], Loss: 3.0846\n",
      "Epoch [1/5], Step [5535/5873], Loss: 3.7709\n",
      "Epoch [1/5], Step [5536/5873], Loss: 3.5342\n",
      "Epoch [1/5], Step [5537/5873], Loss: 3.1906\n",
      "Epoch [1/5], Step [5538/5873], Loss: 4.5236\n",
      "Epoch [1/5], Step [5539/5873], Loss: 3.9120\n",
      "Epoch [1/5], Step [5540/5873], Loss: 4.4096\n",
      "Epoch [1/5], Step [5541/5873], Loss: 1.2179\n",
      "Epoch [1/5], Step [5542/5873], Loss: 5.1771\n",
      "Epoch [1/5], Step [5543/5873], Loss: 5.0115\n",
      "Epoch [1/5], Step [5544/5873], Loss: 6.3902\n",
      "Epoch [1/5], Step [5545/5873], Loss: 3.0542\n",
      "Epoch [1/5], Step [5546/5873], Loss: 5.3144\n",
      "Epoch [1/5], Step [5547/5873], Loss: 3.8130\n",
      "Epoch [1/5], Step [5548/5873], Loss: 4.1936\n",
      "Epoch [1/5], Step [5549/5873], Loss: 1.1970\n",
      "Epoch [1/5], Step [5550/5873], Loss: 3.9202\n",
      "Epoch [1/5], Step [5551/5873], Loss: 4.0856\n",
      "Epoch [1/5], Step [5552/5873], Loss: 4.3014\n",
      "Epoch [1/5], Step [5553/5873], Loss: 5.1605\n",
      "Epoch [1/5], Step [5554/5873], Loss: 4.3301\n",
      "Epoch [1/5], Step [5555/5873], Loss: 4.5783\n",
      "Epoch [1/5], Step [5556/5873], Loss: 4.3236\n",
      "Epoch [1/5], Step [5557/5873], Loss: 4.7660\n",
      "Epoch [1/5], Step [5558/5873], Loss: 3.9368\n",
      "Epoch [1/5], Step [5559/5873], Loss: 5.7988\n",
      "Epoch [1/5], Step [5560/5873], Loss: 2.4304\n",
      "Epoch [1/5], Step [5561/5873], Loss: 3.0934\n",
      "Epoch [1/5], Step [5562/5873], Loss: 2.4152\n",
      "Epoch [1/5], Step [5563/5873], Loss: 4.2814\n",
      "Epoch [1/5], Step [5564/5873], Loss: 1.4091\n",
      "Epoch [1/5], Step [5565/5873], Loss: 3.2829\n",
      "Epoch [1/5], Step [5566/5873], Loss: 4.8522\n",
      "Epoch [1/5], Step [5567/5873], Loss: 3.5529\n",
      "Epoch [1/5], Step [5568/5873], Loss: 4.5489\n",
      "Epoch [1/5], Step [5569/5873], Loss: 2.9625\n",
      "Epoch [1/5], Step [5570/5873], Loss: 2.3288\n",
      "Epoch [1/5], Step [5571/5873], Loss: 4.5113\n",
      "Epoch [1/5], Step [5572/5873], Loss: 1.9418\n",
      "Epoch [1/5], Step [5573/5873], Loss: 4.9542\n",
      "Epoch [1/5], Step [5574/5873], Loss: 4.1162\n",
      "Epoch [1/5], Step [5575/5873], Loss: 2.2001\n",
      "Epoch [1/5], Step [5576/5873], Loss: 3.6172\n",
      "Epoch [1/5], Step [5577/5873], Loss: 4.3917\n",
      "Epoch [1/5], Step [5578/5873], Loss: 1.9451\n",
      "Epoch [1/5], Step [5579/5873], Loss: 3.8867\n",
      "Epoch [1/5], Step [5580/5873], Loss: 2.7182\n",
      "Epoch [1/5], Step [5581/5873], Loss: 3.4778\n",
      "Epoch [1/5], Step [5582/5873], Loss: 1.7181\n",
      "Epoch [1/5], Step [5583/5873], Loss: 2.7804\n",
      "Epoch [1/5], Step [5584/5873], Loss: 3.3319\n",
      "Epoch [1/5], Step [5585/5873], Loss: 3.9379\n",
      "Epoch [1/5], Step [5586/5873], Loss: 4.4611\n",
      "Epoch [1/5], Step [5587/5873], Loss: 4.3368\n",
      "Epoch [1/5], Step [5588/5873], Loss: 3.8151\n",
      "Epoch [1/5], Step [5589/5873], Loss: 4.7076\n",
      "Epoch [1/5], Step [5590/5873], Loss: 5.6614\n",
      "Epoch [1/5], Step [5591/5873], Loss: 1.9525\n",
      "Epoch [1/5], Step [5592/5873], Loss: 2.3319\n",
      "Epoch [1/5], Step [5593/5873], Loss: 4.0462\n",
      "Epoch [1/5], Step [5594/5873], Loss: 5.2651\n",
      "Epoch [1/5], Step [5595/5873], Loss: 2.0067\n",
      "Epoch [1/5], Step [5596/5873], Loss: 4.6417\n",
      "Epoch [1/5], Step [5597/5873], Loss: 3.6527\n",
      "Epoch [1/5], Step [5598/5873], Loss: 4.7498\n",
      "Epoch [1/5], Step [5599/5873], Loss: 4.0147\n",
      "Epoch [1/5], Step [5600/5873], Loss: 5.5779\n",
      "Epoch [1/5], Step [5601/5873], Loss: 4.0553\n",
      "Epoch [1/5], Step [5602/5873], Loss: 2.7503\n",
      "Epoch [1/5], Step [5603/5873], Loss: 2.8982\n",
      "Epoch [1/5], Step [5604/5873], Loss: 1.8139\n",
      "Epoch [1/5], Step [5605/5873], Loss: 3.5992\n",
      "Epoch [1/5], Step [5606/5873], Loss: 5.1509\n",
      "Epoch [1/5], Step [5607/5873], Loss: 4.3392\n",
      "Epoch [1/5], Step [5608/5873], Loss: 2.3326\n",
      "Epoch [1/5], Step [5609/5873], Loss: 3.5796\n",
      "Epoch [1/5], Step [5610/5873], Loss: 2.1561\n",
      "Epoch [1/5], Step [5611/5873], Loss: 4.2473\n",
      "Epoch [1/5], Step [5612/5873], Loss: 4.0910\n",
      "Epoch [1/5], Step [5613/5873], Loss: 4.8777\n",
      "Epoch [1/5], Step [5614/5873], Loss: 3.1862\n",
      "Epoch [1/5], Step [5615/5873], Loss: 3.5377\n",
      "Epoch [1/5], Step [5616/5873], Loss: 3.9281\n",
      "Epoch [1/5], Step [5617/5873], Loss: 4.4666\n",
      "Epoch [1/5], Step [5618/5873], Loss: 1.8459\n",
      "Epoch [1/5], Step [5619/5873], Loss: 3.8252\n",
      "Epoch [1/5], Step [5620/5873], Loss: 3.4702\n",
      "Epoch [1/5], Step [5621/5873], Loss: 1.6665\n",
      "Epoch [1/5], Step [5622/5873], Loss: 2.7415\n",
      "Epoch [1/5], Step [5623/5873], Loss: 3.2311\n",
      "Epoch [1/5], Step [5624/5873], Loss: 4.6479\n",
      "Epoch [1/5], Step [5625/5873], Loss: 2.8135\n",
      "Epoch [1/5], Step [5626/5873], Loss: 3.7769\n",
      "Epoch [1/5], Step [5627/5873], Loss: 4.3736\n",
      "Epoch [1/5], Step [5628/5873], Loss: 4.9243\n",
      "Epoch [1/5], Step [5629/5873], Loss: 3.6323\n",
      "Epoch [1/5], Step [5630/5873], Loss: 4.2597\n",
      "Epoch [1/5], Step [5631/5873], Loss: 4.5038\n",
      "Epoch [1/5], Step [5632/5873], Loss: 3.7918\n",
      "Epoch [1/5], Step [5633/5873], Loss: 4.6443\n",
      "Epoch [1/5], Step [5634/5873], Loss: 3.8538\n",
      "Epoch [1/5], Step [5635/5873], Loss: 2.1059\n",
      "Epoch [1/5], Step [5636/5873], Loss: 4.8823\n",
      "Epoch [1/5], Step [5637/5873], Loss: 4.5685\n",
      "Epoch [1/5], Step [5638/5873], Loss: 4.3019\n",
      "Epoch [1/5], Step [5639/5873], Loss: 3.9290\n",
      "Epoch [1/5], Step [5640/5873], Loss: 2.7241\n",
      "Epoch [1/5], Step [5641/5873], Loss: 1.4380\n",
      "Epoch [1/5], Step [5642/5873], Loss: 4.7483\n",
      "Epoch [1/5], Step [5643/5873], Loss: 2.4529\n",
      "Epoch [1/5], Step [5644/5873], Loss: 3.2231\n",
      "Epoch [1/5], Step [5645/5873], Loss: 1.7969\n",
      "Epoch [1/5], Step [5646/5873], Loss: 1.1507\n",
      "Epoch [1/5], Step [5647/5873], Loss: 3.2970\n",
      "Epoch [1/5], Step [5648/5873], Loss: 3.8511\n",
      "Epoch [1/5], Step [5649/5873], Loss: 4.8221\n",
      "Epoch [1/5], Step [5650/5873], Loss: 4.1783\n",
      "Epoch [1/5], Step [5651/5873], Loss: 3.8426\n",
      "Epoch [1/5], Step [5652/5873], Loss: 3.8357\n",
      "Epoch [1/5], Step [5653/5873], Loss: 5.4506\n",
      "Epoch [1/5], Step [5654/5873], Loss: 3.0228\n",
      "Epoch [1/5], Step [5655/5873], Loss: 2.9806\n",
      "Epoch [1/5], Step [5656/5873], Loss: 5.5495\n",
      "Epoch [1/5], Step [5657/5873], Loss: 2.7796\n",
      "Epoch [1/5], Step [5658/5873], Loss: 4.2379\n",
      "Epoch [1/5], Step [5659/5873], Loss: 4.1538\n",
      "Epoch [1/5], Step [5660/5873], Loss: 1.7057\n",
      "Epoch [1/5], Step [5661/5873], Loss: 4.8174\n",
      "Epoch [1/5], Step [5662/5873], Loss: 3.5157\n",
      "Epoch [1/5], Step [5663/5873], Loss: 5.5991\n",
      "Epoch [1/5], Step [5664/5873], Loss: 4.2477\n",
      "Epoch [1/5], Step [5665/5873], Loss: 2.4803\n",
      "Epoch [1/5], Step [5666/5873], Loss: 4.5882\n",
      "Epoch [1/5], Step [5667/5873], Loss: 3.9948\n",
      "Epoch [1/5], Step [5668/5873], Loss: 0.7939\n",
      "Epoch [1/5], Step [5669/5873], Loss: 3.7029\n",
      "Epoch [1/5], Step [5670/5873], Loss: 3.5498\n",
      "Epoch [1/5], Step [5671/5873], Loss: 5.2595\n",
      "Epoch [1/5], Step [5672/5873], Loss: 1.3197\n",
      "Epoch [1/5], Step [5673/5873], Loss: 3.2921\n",
      "Epoch [1/5], Step [5674/5873], Loss: 3.1549\n",
      "Epoch [1/5], Step [5675/5873], Loss: 2.4838\n",
      "Epoch [1/5], Step [5676/5873], Loss: 4.0471\n",
      "Epoch [1/5], Step [5677/5873], Loss: 4.6188\n",
      "Epoch [1/5], Step [5678/5873], Loss: 4.8729\n",
      "Epoch [1/5], Step [5679/5873], Loss: 1.7326\n",
      "Epoch [1/5], Step [5680/5873], Loss: 5.1909\n",
      "Epoch [1/5], Step [5681/5873], Loss: 5.1936\n",
      "Epoch [1/5], Step [5682/5873], Loss: 3.3931\n",
      "Epoch [1/5], Step [5683/5873], Loss: 4.8840\n",
      "Epoch [1/5], Step [5684/5873], Loss: 5.6643\n",
      "Epoch [1/5], Step [5685/5873], Loss: 5.8382\n",
      "Epoch [1/5], Step [5686/5873], Loss: 1.4551\n",
      "Epoch [1/5], Step [5687/5873], Loss: 1.9601\n",
      "Epoch [1/5], Step [5688/5873], Loss: 3.8496\n",
      "Epoch [1/5], Step [5689/5873], Loss: 4.0152\n",
      "Epoch [1/5], Step [5690/5873], Loss: 3.0709\n",
      "Epoch [1/5], Step [5691/5873], Loss: 2.5584\n",
      "Epoch [1/5], Step [5692/5873], Loss: 1.7375\n",
      "Epoch [1/5], Step [5693/5873], Loss: 3.4983\n",
      "Epoch [1/5], Step [5694/5873], Loss: 6.9337\n",
      "Epoch [1/5], Step [5695/5873], Loss: 4.4842\n",
      "Epoch [1/5], Step [5696/5873], Loss: 5.1787\n",
      "Epoch [1/5], Step [5697/5873], Loss: 2.5627\n",
      "Epoch [1/5], Step [5698/5873], Loss: 4.4734\n",
      "Epoch [1/5], Step [5699/5873], Loss: 3.1255\n",
      "Epoch [1/5], Step [5700/5873], Loss: 3.2370\n",
      "Epoch [1/5], Step [5701/5873], Loss: 4.7355\n",
      "Epoch [1/5], Step [5702/5873], Loss: 3.2335\n",
      "Epoch [1/5], Step [5703/5873], Loss: 2.4010\n",
      "Epoch [1/5], Step [5704/5873], Loss: 3.6836\n",
      "Epoch [1/5], Step [5705/5873], Loss: 3.3535\n",
      "Epoch [1/5], Step [5706/5873], Loss: 3.1080\n",
      "Epoch [1/5], Step [5707/5873], Loss: 3.3947\n",
      "Epoch [1/5], Step [5708/5873], Loss: 5.0237\n",
      "Epoch [1/5], Step [5709/5873], Loss: 1.7018\n",
      "Epoch [1/5], Step [5710/5873], Loss: 3.9944\n",
      "Epoch [1/5], Step [5711/5873], Loss: 4.1335\n",
      "Epoch [1/5], Step [5712/5873], Loss: 6.3547\n",
      "Epoch [1/5], Step [5713/5873], Loss: 5.1343\n",
      "Epoch [1/5], Step [5714/5873], Loss: 3.9657\n",
      "Epoch [1/5], Step [5715/5873], Loss: 4.2593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5716/5873], Loss: 3.5579\n",
      "Epoch [1/5], Step [5717/5873], Loss: 5.0966\n",
      "Epoch [1/5], Step [5718/5873], Loss: 2.6267\n",
      "Epoch [1/5], Step [5719/5873], Loss: 3.3463\n",
      "Epoch [1/5], Step [5720/5873], Loss: 2.2856\n",
      "Epoch [1/5], Step [5721/5873], Loss: 3.4236\n",
      "Epoch [1/5], Step [5722/5873], Loss: 4.3669\n",
      "Epoch [1/5], Step [5723/5873], Loss: 4.7801\n",
      "Epoch [1/5], Step [5724/5873], Loss: 3.4438\n",
      "Epoch [1/5], Step [5725/5873], Loss: 4.9850\n",
      "Epoch [1/5], Step [5726/5873], Loss: 3.0831\n",
      "Epoch [1/5], Step [5727/5873], Loss: 4.2125\n",
      "Epoch [1/5], Step [5728/5873], Loss: 1.9567\n",
      "Epoch [1/5], Step [5729/5873], Loss: 3.7752\n",
      "Epoch [1/5], Step [5730/5873], Loss: 2.8635\n",
      "Epoch [1/5], Step [5731/5873], Loss: 1.8707\n",
      "Epoch [1/5], Step [5732/5873], Loss: 4.5623\n",
      "Epoch [1/5], Step [5733/5873], Loss: 3.2887\n",
      "Epoch [1/5], Step [5734/5873], Loss: 2.7552\n",
      "Epoch [1/5], Step [5735/5873], Loss: 4.8043\n",
      "Epoch [1/5], Step [5736/5873], Loss: 1.4101\n",
      "Epoch [1/5], Step [5737/5873], Loss: 3.5591\n",
      "Epoch [1/5], Step [5738/5873], Loss: 4.3438\n",
      "Epoch [1/5], Step [5739/5873], Loss: 1.4269\n",
      "Epoch [1/5], Step [5740/5873], Loss: 3.1803\n",
      "Epoch [1/5], Step [5741/5873], Loss: 5.7752\n",
      "Epoch [1/5], Step [5742/5873], Loss: 4.3573\n",
      "Epoch [1/5], Step [5743/5873], Loss: 5.1068\n",
      "Epoch [1/5], Step [5744/5873], Loss: 4.0071\n",
      "Epoch [1/5], Step [5745/5873], Loss: 2.4487\n",
      "Epoch [1/5], Step [5746/5873], Loss: 3.8977\n",
      "Epoch [1/5], Step [5747/5873], Loss: 4.3991\n",
      "Epoch [1/5], Step [5748/5873], Loss: 4.4115\n",
      "Epoch [1/5], Step [5749/5873], Loss: 5.2508\n",
      "Epoch [1/5], Step [5750/5873], Loss: 4.5090\n",
      "Epoch [1/5], Step [5751/5873], Loss: 3.6505\n",
      "Epoch [1/5], Step [5752/5873], Loss: 5.4567\n",
      "Epoch [1/5], Step [5753/5873], Loss: 4.6984\n",
      "Epoch [1/5], Step [5754/5873], Loss: 3.9224\n",
      "Epoch [1/5], Step [5755/5873], Loss: 3.3644\n",
      "Epoch [1/5], Step [5756/5873], Loss: 5.4032\n",
      "Epoch [1/5], Step [5757/5873], Loss: 5.1269\n",
      "Epoch [1/5], Step [5758/5873], Loss: 2.0604\n",
      "Epoch [1/5], Step [5759/5873], Loss: 3.7246\n",
      "Epoch [1/5], Step [5760/5873], Loss: 4.9522\n",
      "Epoch [1/5], Step [5761/5873], Loss: 3.7534\n",
      "Epoch [1/5], Step [5762/5873], Loss: 3.4003\n",
      "Epoch [1/5], Step [5763/5873], Loss: 1.6523\n",
      "Epoch [1/5], Step [5764/5873], Loss: 4.4169\n",
      "Epoch [1/5], Step [5765/5873], Loss: 4.4010\n",
      "Epoch [1/5], Step [5766/5873], Loss: 5.5063\n",
      "Epoch [1/5], Step [5767/5873], Loss: 2.6358\n",
      "Epoch [1/5], Step [5768/5873], Loss: 3.9746\n",
      "Epoch [1/5], Step [5769/5873], Loss: 3.0577\n",
      "Epoch [1/5], Step [5770/5873], Loss: 3.0575\n",
      "Epoch [1/5], Step [5771/5873], Loss: 1.5810\n",
      "Epoch [1/5], Step [5772/5873], Loss: 2.0860\n",
      "Epoch [1/5], Step [5773/5873], Loss: 3.8412\n",
      "Epoch [1/5], Step [5774/5873], Loss: 3.7715\n",
      "Epoch [1/5], Step [5775/5873], Loss: 2.0042\n",
      "Epoch [1/5], Step [5776/5873], Loss: 4.2779\n",
      "Epoch [1/5], Step [5777/5873], Loss: 5.5997\n",
      "Epoch [1/5], Step [5778/5873], Loss: 1.9612\n",
      "Epoch [1/5], Step [5779/5873], Loss: 4.9362\n",
      "Epoch [1/5], Step [5780/5873], Loss: 4.3222\n",
      "Epoch [1/5], Step [5781/5873], Loss: 4.7019\n",
      "Epoch [1/5], Step [5782/5873], Loss: 2.6360\n",
      "Epoch [1/5], Step [5783/5873], Loss: 3.5731\n",
      "Epoch [1/5], Step [5784/5873], Loss: 1.9444\n",
      "Epoch [1/5], Step [5785/5873], Loss: 2.2810\n",
      "Epoch [1/5], Step [5786/5873], Loss: 1.6741\n",
      "Epoch [1/5], Step [5787/5873], Loss: 3.8881\n",
      "Epoch [1/5], Step [5788/5873], Loss: 2.4550\n",
      "Epoch [1/5], Step [5789/5873], Loss: 1.4781\n",
      "Epoch [1/5], Step [5790/5873], Loss: 4.2234\n",
      "Epoch [1/5], Step [5791/5873], Loss: 4.4010\n",
      "Epoch [1/5], Step [5792/5873], Loss: 4.9685\n",
      "Epoch [1/5], Step [5793/5873], Loss: 4.6403\n",
      "Epoch [1/5], Step [5794/5873], Loss: 1.6039\n",
      "Epoch [1/5], Step [5795/5873], Loss: 4.7541\n",
      "Epoch [1/5], Step [5796/5873], Loss: 3.5318\n",
      "Epoch [1/5], Step [5797/5873], Loss: 4.2265\n",
      "Epoch [1/5], Step [5798/5873], Loss: 2.6898\n",
      "Epoch [1/5], Step [5799/5873], Loss: 3.3874\n",
      "Epoch [1/5], Step [5800/5873], Loss: 4.0965\n",
      "Epoch [1/5], Step [5801/5873], Loss: 4.2789\n",
      "Epoch [1/5], Step [5802/5873], Loss: 5.2368\n",
      "Epoch [1/5], Step [5803/5873], Loss: 4.1548\n",
      "Epoch [1/5], Step [5804/5873], Loss: 3.8185\n",
      "Epoch [1/5], Step [5805/5873], Loss: 4.7805\n",
      "Epoch [1/5], Step [5806/5873], Loss: 5.1153\n",
      "Epoch [1/5], Step [5807/5873], Loss: 3.3077\n",
      "Epoch [1/5], Step [5808/5873], Loss: 2.6717\n",
      "Epoch [1/5], Step [5809/5873], Loss: 5.4196\n",
      "Epoch [1/5], Step [5810/5873], Loss: 5.4482\n",
      "Epoch [1/5], Step [5811/5873], Loss: 3.5945\n",
      "Epoch [1/5], Step [5812/5873], Loss: 3.2856\n",
      "Epoch [1/5], Step [5813/5873], Loss: 5.4232\n",
      "Epoch [1/5], Step [5814/5873], Loss: 5.5497\n",
      "Epoch [1/5], Step [5815/5873], Loss: 5.0385\n",
      "Epoch [1/5], Step [5816/5873], Loss: 5.6852\n",
      "Epoch [1/5], Step [5817/5873], Loss: 3.0767\n",
      "Epoch [1/5], Step [5818/5873], Loss: 3.7202\n",
      "Epoch [1/5], Step [5819/5873], Loss: 6.5956\n",
      "Epoch [1/5], Step [5820/5873], Loss: 1.5855\n",
      "Epoch [1/5], Step [5821/5873], Loss: 4.5573\n",
      "Epoch [1/5], Step [5822/5873], Loss: 4.6723\n",
      "Epoch [1/5], Step [5823/5873], Loss: 4.3901\n",
      "Epoch [1/5], Step [5824/5873], Loss: 3.9748\n",
      "Epoch [1/5], Step [5825/5873], Loss: 2.2402\n",
      "Epoch [1/5], Step [5826/5873], Loss: 5.2544\n",
      "Epoch [1/5], Step [5827/5873], Loss: 5.4220\n",
      "Epoch [1/5], Step [5828/5873], Loss: 3.8173\n",
      "Epoch [1/5], Step [5829/5873], Loss: 3.7397\n",
      "Epoch [1/5], Step [5830/5873], Loss: 3.7649\n",
      "Epoch [1/5], Step [5831/5873], Loss: 4.9126\n",
      "Epoch [1/5], Step [5832/5873], Loss: 3.8871\n",
      "Epoch [1/5], Step [5833/5873], Loss: 3.8138\n",
      "Epoch [1/5], Step [5834/5873], Loss: 3.9655\n",
      "Epoch [1/5], Step [5835/5873], Loss: 3.4518\n",
      "Epoch [1/5], Step [5836/5873], Loss: 2.0796\n",
      "Epoch [1/5], Step [5837/5873], Loss: 4.6913\n",
      "Epoch [1/5], Step [5838/5873], Loss: 4.3744\n",
      "Epoch [1/5], Step [5839/5873], Loss: 3.3772\n",
      "Epoch [1/5], Step [5840/5873], Loss: 4.3185\n",
      "Epoch [1/5], Step [5841/5873], Loss: 3.9480\n",
      "Epoch [1/5], Step [5842/5873], Loss: 3.9053\n",
      "Epoch [1/5], Step [5843/5873], Loss: 4.5418\n",
      "Epoch [1/5], Step [5844/5873], Loss: 4.7070\n",
      "Epoch [1/5], Step [5845/5873], Loss: 4.5438\n",
      "Epoch [1/5], Step [5846/5873], Loss: 3.3295\n",
      "Epoch [1/5], Step [5847/5873], Loss: 4.8695\n",
      "Epoch [1/5], Step [5848/5873], Loss: 4.7170\n",
      "Epoch [1/5], Step [5849/5873], Loss: 2.3629\n",
      "Epoch [1/5], Step [5850/5873], Loss: 1.3998\n",
      "Epoch [1/5], Step [5851/5873], Loss: 5.7303\n",
      "Epoch [1/5], Step [5852/5873], Loss: 3.9793\n",
      "Epoch [1/5], Step [5853/5873], Loss: 2.6900\n",
      "Epoch [1/5], Step [5854/5873], Loss: 2.5822\n",
      "Epoch [1/5], Step [5855/5873], Loss: 5.2996\n",
      "Epoch [1/5], Step [5856/5873], Loss: 3.7835\n",
      "Epoch [1/5], Step [5857/5873], Loss: 1.4974\n",
      "Epoch [1/5], Step [5858/5873], Loss: 4.8801\n",
      "Epoch [1/5], Step [5859/5873], Loss: 5.4479\n",
      "Epoch [1/5], Step [5860/5873], Loss: 4.9267\n",
      "Epoch [1/5], Step [5861/5873], Loss: 2.9156\n",
      "Epoch [1/5], Step [5862/5873], Loss: 4.9328\n",
      "Epoch [1/5], Step [5863/5873], Loss: 4.0513\n",
      "Epoch [1/5], Step [5864/5873], Loss: 4.4731\n",
      "Epoch [1/5], Step [5865/5873], Loss: 1.7288\n",
      "Epoch [1/5], Step [5866/5873], Loss: 3.3746\n",
      "Epoch [1/5], Step [5867/5873], Loss: 3.6197\n",
      "Epoch [1/5], Step [5868/5873], Loss: 2.4980\n",
      "Epoch [1/5], Step [5869/5873], Loss: 3.0808\n",
      "Epoch [1/5], Step [5870/5873], Loss: 4.5883\n",
      "Epoch [1/5], Step [5871/5873], Loss: 4.5114\n",
      "Epoch [1/5], Step [5872/5873], Loss: 4.3403\n",
      "Epoch [1/5], Step [5873/5873], Loss: 3.7392\n",
      "extracting feature\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b589ee7ca7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 1 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': net.state_dict()},\n",
    "               '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'mytraining.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('mytraining.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TESTING##\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting feature\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nandwani_vaibhav/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n"
     ]
    }
   ],
   "source": [
    "print('extracting feature')\n",
    "embeds = []\n",
    "labels = []\n",
    "preds = np.zeros((1289,1))\n",
    "j = 0\n",
    "for data, target in test_loader:\n",
    "    if cuda:\n",
    "        data, target = data.cuda(), target.cuda(async=True)  \n",
    "    data_var = Variable(data, volatile=True)\n",
    "    # compute output\n",
    "    output = net(data_var)\n",
    "    print(j)\n",
    "    cls = net.forward_classifier(data_var)\n",
    "    values, indices = torch.max(cls, 1)\n",
    "    indices = indices + 1\n",
    "#   print(target.size())\n",
    "    embeds.append( output.data.cpu().numpy() )\n",
    "    labels.append( target.cpu().numpy() )\n",
    "    indices = np.array(indices)\n",
    "    for i in range(indices.shape[0]):\n",
    "            preds[i + j] = indices[i]\n",
    "    j = j + indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1289, 1)\n",
      "[[59.]\n",
      " [59.]\n",
      " [32.]\n",
      " ...\n",
      " [74.]\n",
      " [45.]\n",
      " [81.]]\n",
      "[44 43  3 ... 42  2 78]\n",
      "embeds shape is  (1289, 256)\n",
      "labels shape is  (1289,)\n",
      "Test Accuracy:  0.008533747090768037\n"
     ]
    }
   ],
   "source": [
    "embeds = np.vstack(embeds)\n",
    "labels = np.hstack(labels)\n",
    "print(preds.shape)\n",
    "print(preds)\n",
    "print(labels)\n",
    "print('embeds shape is ', embeds.shape)\n",
    "print('labels shape is ', labels.shape)\n",
    "print('Test Accuracy: ',accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('extracting feature train')\n",
    "embeds_t = []\n",
    "labels_t = []\n",
    "preds_t = np.zeros((5873,1))\n",
    "j = 0\n",
    "for data, target in train_iterator:\n",
    "    if cuda:\n",
    "        data, target = data.cuda(), target.cuda(async=True)\n",
    "        \n",
    "    data_var = Variable(data, volatile=True)\n",
    "    # compute output\n",
    "    output = net(data_var)\n",
    "    cls = net.forward_classifier(data_var)\n",
    "    values, indices = torch.max(cls, 1)\n",
    "    indices = indices\n",
    "#         print(target.size())\n",
    "    embeds_t.append( output.data.cpu().numpy() )\n",
    "    labels_t.append( target.cpu().numpy() )\n",
    "    indices = np.array(indices)\n",
    "    for i in range(indices.shape[0]):\n",
    "           preds_t[i + j] = indices[i]\n",
    "    j = j + indices.shape[0]\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = np.hstack(preds)\n",
    "embeds_t = np.vstack(embeds_t)\n",
    "labels_t = np.hstack(labels_t)\n",
    "#     print(preds_t.shape)\n",
    "#     print(preds_t)\n",
    "#     print(labels_t)\n",
    "print('embeds shape is ', embeds_t.shape)\n",
    "print('labels shape is ', labels_t.shape)\n",
    "print('Train accyracy: ',accuracy_score(labels_t, preds_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
